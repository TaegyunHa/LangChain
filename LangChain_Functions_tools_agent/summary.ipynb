{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "784c5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Tavily API key\n",
    "def setup_openai_api_key():\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "setup_openai_api_key()\n",
    "\n",
    "LLM_MODEL:str = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eba92f",
   "metadata": {},
   "source": [
    "# OpenAI Function calling\n",
    "\n",
    "> https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "| Image | Description |\n",
    "| ----- | ----------- |\n",
    "<img src=\"./images/openai-function-calling.png\" alt=\"sequence\" height=\"600\"/> | With the function calling of OpenAI, the LLM will return:<br/><ul><li>which function to call</li><li>arguments of the function</li></ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e91dbd",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "**To leverage the function calling of OpenAI:**\n",
    "\n",
    "1. define tools: [example](https://platform.openai.com/docs/guides/function-calling#defining-functions)\n",
    "2. pass list of tools along with prompt into LLM\n",
    "3. check `Response.output.type` is `\"function_call\"`\n",
    "4. If it is, excute code on the application side with input from the tool call\n",
    "5. Make a second request to the model with the tool output\n",
    "6. Receive a final response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "f7581b9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final input:\n",
      "[{'role': 'user', 'content': \"What's the weather like in London?\"}, ResponseFunctionToolCall(arguments='{\"location\":\"London, UK\"}', call_id='call_NxTqO3VoWMyxYRhYgliYojx0', name='get_current_weather', type='function_call', id='fc_68b7f07906988196ad6b75f21b34245f00a016e59abdfec5', status='completed'), {'type': 'function_call_output', 'call_id': 'call_NxTqO3VoWMyxYRhYgliYojx0', 'output': '{\"weather\": \"{\\\\\"location\\\\\": {\\\\\"location\\\\\": \\\\\"London, UK\\\\\"}, \\\\\"temperature\\\\\": \\\\\"26\\\\\", \\\\\"unit\\\\\": \\\\\"celsius\\\\\", \\\\\"forecast\\\\\": [\\\\\"sunny\\\\\", \\\\\"windy\\\\\"]}\"}'}]\n",
      "Final output:\n",
      "{\n",
      "  \"id\": \"resp_68b7f07990108196a9830a88d52896dd00a016e59abdfec5\",\n",
      "  \"created_at\": 1756885113.0,\n",
      "  \"error\": null,\n",
      "  \"incomplete_details\": null,\n",
      "  \"instructions\": null,\n",
      "  \"metadata\": {},\n",
      "  \"model\": \"gpt-4o-mini-2024-07-18\",\n",
      "  \"object\": \"response\",\n",
      "  \"output\": [\n",
      "    {\n",
      "      \"id\": \"msg_68b7f07a70008196a3a9fee743927ab500a016e59abdfec5\",\n",
      "      \"content\": [\n",
      "        {\n",
      "          \"annotations\": [],\n",
      "          \"text\": \"The weather in London is currently 26°C, sunny, and windy.\",\n",
      "          \"type\": \"output_text\",\n",
      "          \"logprobs\": []\n",
      "        }\n",
      "      ],\n",
      "      \"role\": \"assistant\",\n",
      "      \"status\": \"completed\",\n",
      "      \"type\": \"message\"\n",
      "    }\n",
      "  ],\n",
      "  \"parallel_tool_calls\": true,\n",
      "  \"temperature\": 1.0,\n",
      "  \"tool_choice\": \"auto\",\n",
      "  \"tools\": [\n",
      "    {\n",
      "      \"name\": \"get_current_weather\",\n",
      "      \"parameters\": {\n",
      "        \"type\": \"object\",\n",
      "        \"properties\": {\n",
      "          \"location\": {\n",
      "            \"type\": \"string\",\n",
      "            \"description\": \"The city and country, e.g. London, UK\"\n",
      "          },\n",
      "          \"unit\": {\n",
      "            \"type\": \"string\",\n",
      "            \"enum\": [\n",
      "              \"celsius\",\n",
      "              \"fahrenheit\"\n",
      "            ]\n",
      "          }\n",
      "        },\n",
      "        \"required\": [\n",
      "          \"location\"\n",
      "        ]\n",
      "      },\n",
      "      \"strict\": false,\n",
      "      \"type\": \"function\",\n",
      "      \"description\": \"Get the current weather in a given location\"\n",
      "    }\n",
      "  ],\n",
      "  \"top_p\": 1.0,\n",
      "  \"background\": false,\n",
      "  \"max_output_tokens\": null,\n",
      "  \"max_tool_calls\": null,\n",
      "  \"previous_response_id\": null,\n",
      "  \"prompt\": null,\n",
      "  \"prompt_cache_key\": null,\n",
      "  \"reasoning\": {\n",
      "    \"effort\": null,\n",
      "    \"generate_summary\": null,\n",
      "    \"summary\": null\n",
      "  },\n",
      "  \"safety_identifier\": null,\n",
      "  \"service_tier\": \"default\",\n",
      "  \"status\": \"completed\",\n",
      "  \"text\": {\n",
      "    \"format\": {\n",
      "      \"type\": \"text\"\n",
      "    },\n",
      "    \"verbosity\": \"medium\"\n",
      "  },\n",
      "  \"top_logprobs\": 0,\n",
      "  \"truncation\": \"disabled\",\n",
      "  \"usage\": {\n",
      "    \"input_tokens\": 142,\n",
      "    \"input_tokens_details\": {\n",
      "      \"cached_tokens\": 0\n",
      "    },\n",
      "    \"output_tokens\": 17,\n",
      "    \"output_tokens_details\": {\n",
      "      \"reasoning_tokens\": 0\n",
      "    },\n",
      "    \"total_tokens\": 159\n",
      "  },\n",
      "  \"user\": null,\n",
      "  \"store\": true\n",
      "}\n",
      "\n",
      " The weather in London is currently 26°C, sunny, and windy.\n"
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.responses.function_tool_param import FunctionToolParam\n",
    "import json\n",
    "\n",
    "# mock of tool call of application\n",
    "def get_current_weather(location: str, unit=\"celsius\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"26\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)\n",
    "\n",
    "# define a list of callable tools for the model\n",
    "tools: list[FunctionToolParam]= [{\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and country, e.g. London, UK\",\n",
    "            },\n",
    "            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "    \"strict\": None,\n",
    "    \"type\": \"function\"\n",
    "}]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in London?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prompt the model with tools defined\n",
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=LLM_MODEL,\n",
    "    tools=tools,\n",
    "    input=messages,\n",
    ")\n",
    "\n",
    "# Save function call outputs for subsequent requests\n",
    "messages += response.output\n",
    "\n",
    "for item in response.output:\n",
    "    if item.type == \"function_call\":\n",
    "        if item.name == \"get_current_weather\":\n",
    "            # 3. Execute the function logic for get_horoscope\n",
    "            weather = get_current_weather(json.loads(item.arguments))\n",
    "            \n",
    "            # 4. Provide function call results to the model\n",
    "            messages.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\n",
    "                  \"weather\": weather\n",
    "                })\n",
    "            })\n",
    "\n",
    "print(\"Final input:\")\n",
    "print(messages)\n",
    "response = client.responses.create(\n",
    "    model=LLM_MODEL,\n",
    "    tools=tools,\n",
    "    input=messages,\n",
    ")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(\"\\n\", response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa07b4",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "LCEL and the runnable protocol defines \"what\" should happen, rather than \"how\". It's important that a \"chain\" is `Runnable`.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[block1] --> B[/allowed set of input types/]\n",
    "    subgraph box2\n",
    "    B --> C[required methods<br/>invoke, stream, batch]\n",
    "    C --> D[/output types/]\n",
    "    end\n",
    "    D --> E[block3]\n",
    "```\n",
    "\n",
    "**Example of composition:**\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | OutputParser\n",
    "```\n",
    "\n",
    "**Interface**\n",
    "\n",
    "- Components implement `Runnable` protocol\n",
    "- Common methods\n",
    "  - `invoke` / `ainvoke`\n",
    "  - `stream` / `astream`\n",
    "  - `batch` / `abatch`\n",
    "- Common properties\n",
    "  - `input_schema`\n",
    "  - `output_schema`\n",
    "- Common I/O\n",
    "    | Component     | Input Type                              | Output Type       |\n",
    "    |---------------|-----------------------------------------|-------------------|\n",
    "    | Prompt        | Dictionary                              | Prompt Value      |\n",
    "    | Retriever     | Single String                           | List of Documents |\n",
    "    | LLM           | String, list of messages or Prompt Value| String            |\n",
    "    | ChatModel     | String, list of messages or Prompt Value| ChatMessage       |\n",
    "    | Tool          | String/Dictionary                       | Tool dependent    |\n",
    "    | Output Parser | Output of LLM or ChatModel              | Parser dependent  |\n",
    "\n",
    "**Supported feature**\n",
    "\n",
    "- async, batch and stream\n",
    "- fallbacks\n",
    "- parallelism\n",
    "- logging\n",
    "\n",
    "### Simple chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "abc8091a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Why did the bear break up with his girlfriend? Because he couldn't bear the relationship any longer!\""
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc812e",
   "metadata": {},
   "source": [
    "### Chain with embedding as context example\n",
    "\n",
    "With `RunnableMap`, one input can be converted into the other format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "7892e2f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "get_relevant_documents\n",
      "-  [Document(metadata={}, page_content='harrison worked at kensho'), Document(metadata={}, page_content='mears like to eat honey')]\n",
      "-  [Document(metadata={}, page_content='mears like to eat honey'), Document(metadata={}, page_content='harrison worked at kensho')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Harrison worked at Kensho.'"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"harrison worked at kensho\",\n",
    "        \"mears like to eat honey\"\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"get_relevant_documents\")\n",
    "print(\"- \", retriever.get_relevant_documents(\"where did harrison work?\"))\n",
    "print(\"- \", retriever.get_relevant_documents(\"what do bears like to eat?\"))\n",
    "\n",
    "template = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "chain= RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479136de",
   "metadata": {},
   "source": [
    "### Chain with function bind\n",
    "\n",
    "With function binding, LLM can extract relavant function name along with required arguments from the user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "3d29aef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"LHR\"}', 'name': 'weather_search'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 98, 'total_tokens': 114, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--bc042942-c01b-4d67-9df8-4f0d1231d80b-0' usage_metadata={'input_tokens': 98, 'output_tokens': 16, 'total_tokens': 114, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='' additional_kwargs={'function_call': {'arguments': '{\"team_name\":\"patriots\"}', 'name': 'sports_search'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 18, 'prompt_tokens': 99, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--f5f373d9-c824-4dd0-ae28-18c95797a85e-0' usage_metadata={'input_tokens': 99, 'output_tokens': 18, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"input\": \"what is the weather in London?\"})\n",
    "print(response)\n",
    "response = chain.invoke({\"input\": \"how did the patriots do yesterday?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbfbae",
   "metadata": {},
   "source": [
    "### Fallbacks\n",
    "\n",
    "With fallbacks, we can handle LLM call failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "37b41a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simple chain failed: Extra data: line 9 column 1 (char 125)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'poem1': {'title': 'The Rose',\n",
       "  'author': 'Emily Dickinson',\n",
       "  'firstLine': 'A rose by any other name would smell as sweet'},\n",
       " 'poem2': {'title': 'The Road Not Taken',\n",
       "  'author': 'Robert Frost',\n",
       "  'firstLine': 'Two roads diverged in a yellow wood'},\n",
       " 'poem3': {'title': 'Hope is the Thing with Feathers',\n",
       "  'author': 'Emily Dickinson',\n",
       "  'firstLine': 'Hope is the thing with feathers that perches in the soul'}}"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "simple_model = OpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=100, \n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads\n",
    "\n",
    "final_chain = simple_chain.with_fallbacks([chain])\n",
    "\n",
    "input = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
    "\n",
    "try:\n",
    "    simple_chain.invoke(input)\n",
    "except Exception as e:\n",
    "    print(f\"Simple chain failed: {e}\")\n",
    "\n",
    "final_chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f5abf",
   "metadata": {},
   "source": [
    "### Interface\n",
    "\n",
    "As mentioned above, `invoke`, `stream`, and `batch` are supported interface along with their async implentation.\n",
    "- `invoke`: normal run\n",
    "- `stream`: async yeild\n",
    "- `batch`: parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "2b71327b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke: Why did the bear break up with his girlfriend? \n",
      "\n",
      "Because he couldn't bear to be with someone who was always so grizzly!\n",
      "batch: ['Why did the bear sit on the stove?\\n\\nBecause he wanted to be a \"hot cross bear!\"', 'Why are frogs so happy? Because they eat whatever bugs them!']\n",
      "stream:\n",
      "\n",
      "Why\n",
      " did\n",
      " the\n",
      " bear\n",
      " break\n",
      " up\n",
      " with\n",
      " the\n",
      " slo\n",
      "th\n",
      "?\n",
      " \n",
      "\n",
      "Because\n",
      " he\n",
      " was\n",
      " too\n",
      " slow\n",
      " for\n",
      " him\n",
      "!\n",
      "\n",
      "ainvoke: Why did the bear bring a flashlight to the party? \n",
      "\n",
      "Because he heard it was going to be a bear-y dark and grizzly night!\n"
     ]
    }
   ],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"bears\"})\n",
    "print(\"invoke:\", response)\n",
    "response = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])\n",
    "print(\"batch:\", response)\n",
    "print(\"stream:\")\n",
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)\n",
    "\n",
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "print(\"ainvoke:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68c717",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling in LangChain\n",
    "\n",
    "pydantic can be used to create a function definition. It's important to note that pydantic object itself is not used, but it's used to create a schema.\n",
    "\n",
    "With function callings, LLM can:\n",
    "1. invoke LLM with optional functions\n",
    "2. bind functions to the LLM model to always refer to available functions\n",
    "3. force LLM to return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3556988f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"WeatherSearch\",\n",
      "  \"description\": \"Call this with an airport code to get the weather at that airport\",\n",
      "  \"parameters\": {\n",
      "    \"properties\": {\n",
      "      \"airport_code\": {\n",
      "        \"description\": \"airport code to get weather for\",\n",
      "        \"type\": \"string\"\n",
      "      }\n",
      "    },\n",
      "    \"required\": [\n",
      "      \"airport_code\"\n",
      "    ],\n",
      "    \"type\": \"object\"\n",
      "  }\n",
      "}\n",
      "ivoke-functions: content='' additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 70, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--ae5a9a81-c56e-418a-9483-2b05b694299c-0' usage_metadata={'input_tokens': 70, 'output_tokens': 17, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "functions: content='' additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 70, 'total_tokens': 87, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--6d9e2c0c-9d60-47da-b7d1-8ace3a46c9ae-0' usage_metadata={'input_tokens': 70, 'output_tokens': 17, 'total_tokens': 87, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "function_call: content='' additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"JFK\"}', 'name': 'WeatherSearch'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 7, 'prompt_tokens': 74, 'total_tokens': 81, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--e691f0ac-a04e-4eef-a1ca-8d52429f8bde-0' usage_metadata={'input_tokens': 74, 'output_tokens': 7, 'total_tokens': 81, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "chain: content='' additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 75, 'total_tokens': 92, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--ffb3452a-82b0-40ba-aa2c-15e39b29d701-0' usage_metadata={'input_tokens': 75, 'output_tokens': 17, 'total_tokens': 92, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "model_with_functions: content='' additional_kwargs={'function_call': {'arguments': '{\"airport_code\":\"SFO\"}', 'name': 'WeatherSearch'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 17, 'prompt_tokens': 116, 'total_tokens': 133, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--e6880336-c8a9-4388-8339-e0880baf9c5d-0' usage_metadata={'input_tokens': 116, 'output_tokens': 17, 'total_tokens': 133, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "model_with_functions: content='' additional_kwargs={'function_call': {'arguments': '{\"artist_name\":\"taylor swift\",\"n\":3}', 'name': 'ArtistSearch'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 22, 'prompt_tokens': 118, 'total_tokens': 140, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--e8db2ac5-91b8-42c2-ae61-1ac114e0b5ed-0' usage_metadata={'input_tokens': 118, 'output_tokens': 22, 'total_tokens': 140, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "model_with_functions: content='Hello! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 111, 'total_tokens': 121, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--e2d88ba7-4519-4636-ab5d-6217f7bd1277-0' usage_metadata={'input_tokens': 111, 'output_tokens': 10, 'total_tokens': 121, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class WeatherSearch(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")\n",
    "\n",
    "weather_function = convert_pydantic_to_openai_function(WeatherSearch)\n",
    "print(json.dumps(weather_function, indent=2))\n",
    "\n",
    "model = ChatOpenAI()\n",
    "# 1. invoke with functions directly\n",
    "print(\"ivoke-functions:\", model.invoke(\"what is the weather in SF today?\", functions=[weather_function]))\n",
    "\n",
    "# 2. bind functions to model first then invoke\n",
    "model_with_function = model.bind(functions=[weather_function])\n",
    "print(\"functions:\", model_with_function.invoke(\"what is the weather in SF today?\"))\n",
    "\n",
    "# 3. force the model to use function\n",
    "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})\n",
    "print(\"function_call:\", model_with_forced_function.invoke(\"hi!\"))\n",
    "\n",
    "# 4. with chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt | model_with_function\n",
    "print(\"chain:\", chain.invoke({\"input\": \"what is the weather in sf?\"}))\n",
    "\n",
    "# 5. multiple functions\n",
    "class ArtistSearch(BaseModel):\n",
    "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
    "    artist_name: str = Field(description=\"name of artist to look up\")\n",
    "    n: int = Field(description=\"number of results\")\n",
    "functions = [\n",
    "    convert_pydantic_to_openai_function(WeatherSearch),\n",
    "    convert_pydantic_to_openai_function(ArtistSearch),\n",
    "]\n",
    "model_with_functions = model.bind(functions=functions)\n",
    "print(\"model_with_functions:\", model_with_functions.invoke(\"what is the weather in sf?\"))\n",
    "print(\"model_with_functions:\", model_with_functions.invoke(\"what are three songs by taylor swift?\"))\n",
    "print(\"model_with_functions:\", model_with_functions.invoke(\"hi!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba815c5f",
   "metadata": {},
   "source": [
    "- `convert_pydantic_to_openai_function`\n",
    "  - description of `BaseModel` is enforced (comment of pydantic class).\n",
    "  - description of parametres are optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1cdf0",
   "metadata": {},
   "source": [
    "# Tagging\n",
    "\n",
    "LLM can evaluate the input text and generate structured output.\n",
    "- LLM has been fine tuned to find and fill in the parameters when JSON schema is provided.\n",
    "\n",
    "**Tagging means labeling a document with classes such as:**\n",
    "- Sentiment\n",
    "- Language\n",
    "- Style (formal, informal etc.)\n",
    "- Covered topics\n",
    "\n",
    "**The core components of taggings are:**\n",
    "- `function`: specify how the model tag an input\n",
    "- `schema`: defines how to tag an input\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph tagging schema\n",
    "    direction TB\n",
    "    A[description] --> B[\"Tagging(BaseModel)\"]\n",
    "    C[label] --> B\n",
    "    D[description] --> C\n",
    "    end\n",
    "    B --> E[convert_pydantic_<br/>to_openai_function]\n",
    "    E --> F[llm]\n",
    "    G[input text] --> F\n",
    "    F --> H[structured output]\n",
    "\n",
    "    style F fill:#bbf,color:#000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9cc5d8c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Tagging', 'description': 'Tag the piece of text with particular info.', 'parameters': {'properties': {'sentiment': {'enum': ['positive', 'negative', 'neutral'], 'type': 'string'}, 'language': {'description': 'language of text (should be ISO 639-1 code)', 'type': 'string'}}, 'required': ['sentiment', 'language'], 'type': 'object'}}]\n",
      "content='' additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"positive\",\"language\":\"en\"}', 'name': 'Tagging'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 100, 'total_tokens': 110, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--7b18d59b-ccfa-4460-acdc-87dff9a6bd17-0' usage_metadata={'input_tokens': 100, 'output_tokens': 10, 'total_tokens': 110, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='' additional_kwargs={'function_call': {'arguments': '{\"sentiment\":\"negative\",\"language\":\"it\"}', 'name': 'Tagging'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 103, 'total_tokens': 113, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None} id='run--bc8bd3c0-333e-4cef-bd7b-d86cdf6fcb53-0' usage_metadata={'input_tokens': 103, 'output_tokens': 10, 'total_tokens': 113, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "{'sentiment': 'positive', 'language': 'en'}\n",
      "{'sentiment': 'negative', 'language': 'it'}\n"
     ]
    }
   ],
   "source": [
    "from enum import StrEnum\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "class Sentiment(StrEnum):\n",
    "    positive = \"positive\"\n",
    "    negative = \"negative\"\n",
    "    neutral = \"neutral\"\n",
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: Sentiment = Field(description=\"sentiment of text\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]\n",
    "print(tagging_functions)\n",
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")\n",
    "# Without parser\n",
    "tagging_chain = prompt | model_with_functions\n",
    "print(tagging_chain.invoke({\"input\": \"I love langchain\"}))\n",
    "print(tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"}))\n",
    "# With parser\n",
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()\n",
    "print(tagging_chain.invoke({\"input\": \"I love langchain\"}))\n",
    "print(tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7a058",
   "metadata": {},
   "source": [
    "# Extraction\n",
    "\n",
    "Similar to tagging, LLM can extract multiple pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "2c5a3e63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'Information', 'description': 'Information to extract.', 'parameters': {'properties': {'people': {'description': 'List of info about people', 'items': {'description': 'Information about a person', 'properties': {'name': {'description': \"person's name\", 'type': 'string'}, 'age': {'anyOf': [{'type': 'integer'}, {'type': 'null'}], 'description': \"person's age\"}}, 'required': ['name', 'age'], 'type': 'object'}, 'type': 'array'}}, 'required': ['people'], 'type': 'object'}}]\n",
      "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]}\n",
      "{'people': [{'name': 'Joe', 'age': 30}, {'name': 'Martha', 'age': None}]}\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: int|None = Field(description=\"person's age\")\n",
    "\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: list[Person] = Field(description=\"List of info about people\")\n",
    "\n",
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "print(extraction_functions)\n",
    "\n",
    "model = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "extraction_model = model.bind(functions=extraction_functions)\n",
    "extraction_chain = extraction_model | JsonOutputFunctionsParser()\n",
    "print(extraction_chain.invoke(\"Joe is 30, his mom is Martha\"))\n",
    "\n",
    "# with prompt to make sure to avoid filling incorrect information\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\n",
    "print(extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775aaaba",
   "metadata": {},
   "source": [
    "## Online document summarise example\n",
    "\n",
    "**Loading document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "87a36555",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[115]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlangchain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebBaseLoader\n\u001b[32m      2\u001b[39m loader = WebBaseLoader(\u001b[33m\"\u001b[39m\u001b[33mhttps://lilianweng.github.io/posts/2023-06-23-agent/\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m documents = \u001b[43mloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m doc = documents[\u001b[32m0\u001b[39m]\n\u001b[32m      5\u001b[39m page_content = doc.page_content[:\u001b[32m10000\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/langchain_core/document_loaders/base.py:32\u001b[39m, in \u001b[36mBaseLoader.load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mload\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[Document]:\n\u001b[32m     31\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Load data into Document objects.\"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlazy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/langchain_community/document_loaders/web_base.py:378\u001b[39m, in \u001b[36mWebBaseLoader.lazy_load\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    376\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Lazy load text from the url(s) in web_path.\"\"\"\u001b[39;00m\n\u001b[32m    377\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.web_paths:\n\u001b[32m--> \u001b[39m\u001b[32m378\u001b[39m     soup = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_scrape\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbs_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    379\u001b[39m     text = soup.get_text(**\u001b[38;5;28mself\u001b[39m.bs_get_text_kwargs)\n\u001b[32m    380\u001b[39m     metadata = _build_metadata(soup, path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/langchain_community/document_loaders/web_base.py:357\u001b[39m, in \u001b[36mWebBaseLoader._scrape\u001b[39m\u001b[34m(self, url, parser, bs_kwargs)\u001b[39m\n\u001b[32m    353\u001b[39m         parser = \u001b[38;5;28mself\u001b[39m.default_parser\n\u001b[32m    355\u001b[39m \u001b[38;5;28mself\u001b[39m._check_parser(parser)\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m html_doc = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequests_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.raise_for_status:\n\u001b[32m    359\u001b[39m     html_doc.raise_for_status()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/requests/sessions.py:602\u001b[39m, in \u001b[36mSession.get\u001b[39m\u001b[34m(self, url, **kwargs)\u001b[39m\n\u001b[32m    594\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Sends a GET request. Returns :class:`Response` object.\u001b[39;00m\n\u001b[32m    595\u001b[39m \n\u001b[32m    596\u001b[39m \u001b[33;03m:param url: URL for the new :class:`Request` object.\u001b[39;00m\n\u001b[32m    597\u001b[39m \u001b[33;03m:param \\*\\*kwargs: Optional arguments that ``request`` takes.\u001b[39;00m\n\u001b[32m    598\u001b[39m \u001b[33;03m:rtype: requests.Response\u001b[39;00m\n\u001b[32m    599\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    601\u001b[39m kwargs.setdefault(\u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m602\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mGET\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/requests/sessions.py:589\u001b[39m, in \u001b[36mSession.request\u001b[39m\u001b[34m(self, method, url, params, data, headers, cookies, files, auth, timeout, allow_redirects, proxies, hooks, stream, verify, cert, json)\u001b[39m\n\u001b[32m    584\u001b[39m send_kwargs = {\n\u001b[32m    585\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtimeout\u001b[39m\u001b[33m\"\u001b[39m: timeout,\n\u001b[32m    586\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mallow_redirects\u001b[39m\u001b[33m\"\u001b[39m: allow_redirects,\n\u001b[32m    587\u001b[39m }\n\u001b[32m    588\u001b[39m send_kwargs.update(settings)\n\u001b[32m--> \u001b[39m\u001b[32m589\u001b[39m resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprep\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msend_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    591\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m resp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/requests/sessions.py:703\u001b[39m, in \u001b[36mSession.send\u001b[39m\u001b[34m(self, request, **kwargs)\u001b[39m\n\u001b[32m    700\u001b[39m start = preferred_clock()\n\u001b[32m    702\u001b[39m \u001b[38;5;66;03m# Send the request\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m r = \u001b[43madapter\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    705\u001b[39m \u001b[38;5;66;03m# Total elapsed time of the request (approximately)\u001b[39;00m\n\u001b[32m    706\u001b[39m elapsed = preferred_clock() - start\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/requests/adapters.py:644\u001b[39m, in \u001b[36mHTTPAdapter.send\u001b[39m\u001b[34m(self, request, stream, timeout, verify, cert, proxies)\u001b[39m\n\u001b[32m    641\u001b[39m     timeout = TimeoutSauce(connect=timeout, read=timeout)\n\u001b[32m    643\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m     resp = \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43murlopen\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    645\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    646\u001b[39m \u001b[43m        \u001b[49m\u001b[43murl\u001b[49m\u001b[43m=\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m        \u001b[49m\u001b[43mredirect\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    650\u001b[39m \u001b[43m        \u001b[49m\u001b[43massert_same_host\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    651\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    652\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    653\u001b[39m \u001b[43m        \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_retries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    654\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    655\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    658\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m (ProtocolError, \u001b[38;5;167;01mOSError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mConnectionError\u001b[39;00m(err, request=request)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:787\u001b[39m, in \u001b[36mHTTPConnectionPool.urlopen\u001b[39m\u001b[34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[39m\n\u001b[32m    784\u001b[39m response_conn = conn \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m release_conn \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    786\u001b[39m \u001b[38;5;66;03m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m787\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    788\u001b[39m \u001b[43m    \u001b[49m\u001b[43mconn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    789\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmethod\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    790\u001b[39m \u001b[43m    \u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    791\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtimeout_obj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    792\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbody\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbody\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    793\u001b[39m \u001b[43m    \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    794\u001b[39m \u001b[43m    \u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunked\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    795\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretries\u001b[49m\u001b[43m=\u001b[49m\u001b[43mretries\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    796\u001b[39m \u001b[43m    \u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresponse_conn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    797\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreload_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    798\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdecode_content\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    799\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mresponse_kw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    800\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    802\u001b[39m \u001b[38;5;66;03m# Everything went great!\u001b[39;00m\n\u001b[32m    803\u001b[39m clean_exit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:464\u001b[39m, in \u001b[36mHTTPConnectionPool._make_request\u001b[39m\u001b[34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    462\u001b[39m     \u001b[38;5;66;03m# Trigger any extra validation we need to do.\u001b[39;00m\n\u001b[32m    463\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m464\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    465\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m (SocketTimeout, BaseSSLError) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    466\u001b[39m         \u001b[38;5;28mself\u001b[39m._raise_timeout(err=e, url=url, timeout_value=conn.timeout)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1093\u001b[39m, in \u001b[36mHTTPSConnectionPool._validate_conn\u001b[39m\u001b[34m(self, conn)\u001b[39m\n\u001b[32m   1091\u001b[39m \u001b[38;5;66;03m# Force connect early to allow us to validate the connection.\u001b[39;00m\n\u001b[32m   1092\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m conn.is_closed:\n\u001b[32m-> \u001b[39m\u001b[32m1093\u001b[39m     \u001b[43mconn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1095\u001b[39m \u001b[38;5;66;03m# TODO revise this, see https://github.com/urllib3/urllib3/issues/2791\u001b[39;00m\n\u001b[32m   1096\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.is_verified \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m conn.proxy_is_verified:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/urllib3/connection.py:753\u001b[39m, in \u001b[36mHTTPSConnection.connect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    751\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    752\u001b[39m     sock: socket.socket | ssl.SSLSocket\n\u001b[32m--> \u001b[39m\u001b[32m753\u001b[39m     \u001b[38;5;28mself\u001b[39m.sock = sock = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_new_conn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    754\u001b[39m     server_hostname: \u001b[38;5;28mstr\u001b[39m = \u001b[38;5;28mself\u001b[39m.host\n\u001b[32m    755\u001b[39m     tls_in_tls = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/urllib3/connection.py:198\u001b[39m, in \u001b[36mHTTPConnection._new_conn\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    193\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Establish a socket connection and set nodelay settings on it.\u001b[39;00m\n\u001b[32m    194\u001b[39m \n\u001b[32m    195\u001b[39m \u001b[33;03m:return: New socket connection.\u001b[39;00m\n\u001b[32m    196\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m198\u001b[39m     sock = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    199\u001b[39m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dns_host\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    200\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    201\u001b[39m \u001b[43m        \u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msource_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    202\u001b[39m \u001b[43m        \u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msocket_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    203\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m socket.gaierror \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NameResolutionError(\u001b[38;5;28mself\u001b[39m.host, \u001b[38;5;28mself\u001b[39m, e) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/.venv/lib/python3.13/site-packages/urllib3/util/connection.py:73\u001b[39m, in \u001b[36mcreate_connection\u001b[39m\u001b[34m(address, timeout, source_address, socket_options)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m source_address:\n\u001b[32m     72\u001b[39m     sock.bind(source_address)\n\u001b[32m---> \u001b[39m\u001b[32m73\u001b[39m \u001b[43msock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n\u001b[32m     75\u001b[39m err = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()\n",
    "doc = documents[0]\n",
    "page_content = doc.page_content[:10000]\n",
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6a382",
   "metadata": {},
   "source": [
    "**Summarise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c35ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser, JsonKeyOutputFunctionsParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# setup model with prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "\n",
    "# extraction schema\n",
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")\n",
    "overview_tagging_function = convert_pydantic_to_openai_function(Overview)\n",
    "print(overview_tagging_function)\n",
    "\n",
    "# chain everything to extract overview\n",
    "tagging_model = model.bind(\n",
    "    functions=[overview_tagging_function],\n",
    "    function_call={\"name\":overview_tagging_function[\"name\"]}\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()\n",
    "print(tagging_chain.invoke({\"input\": page_content}))\n",
    "\n",
    "# chain to extract paper information\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str = Field(description=\"Title of a paper\")\n",
    "    author: str|None = Field(description=\"Author of a paper\")\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: list[Paper] = Field(description=\"List of information about paper\")\n",
    "\n",
    "paper_extraction_function = convert_pydantic_to_openai_function(Info)\n",
    "extraction_model = model.bind(\n",
    "    functions=[paper_extraction_function], \n",
    "    function_call={\"name\":paper_extraction_function[\"name\"]}\n",
    ")\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "print(extraction_chain.invoke({\"input\": page_content}))\n",
    "\n",
    "# improved chain to extract paper information with prompt\n",
    "template = \"\"\"\\\n",
    "A article will be passed to you. Extract from it all papers that are mentioned by this article follow by its author. \n",
    "\n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "print(extraction_chain.invoke({\"input\": page_content}))\n",
    "print(extraction_chain.invoke({\"input\": \"hi\"}))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\n",
    "splits = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "def flatten(matrix):\n",
    "    \"\"\"flatten list of list into a list\"\"\"\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list\n",
    "\n",
    "doc_splits = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")\n",
    "chain = doc_splits | extraction_chain.map() | flatten\n",
    "print(chain.invoke(doc.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3ae96",
   "metadata": {},
   "source": [
    "- `Runnable.map`:\n",
    "    - Return a new `Runnable` that maps a list of inputs to a list of outputs.\n",
    "    - Calls invoke() with each input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca1692",
   "metadata": {},
   "source": [
    "# Tools and Routing\n",
    "\n",
    "> https://python.langchain.com/docs/integrations/tools/\n",
    "\n",
    "Tools are utilities designed to be called by a model. Their inputs are designed to be generated by models, and their outputs are designed to be passed back to models. Functions and services in LLM can be represented as **tools** in LangChain. \n",
    "\n",
    "LangChain offers many tools such as:\n",
    "- Search tools\n",
    "- Math tools\n",
    "- SQL tools\n",
    "\n",
    "**basic tool definition example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f4e4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: search\n",
      "desc: Search for weather online\n",
      "args: {\n",
      "  \"query\": {\n",
      "    \"title\": \"Query\",\n",
      "    \"type\": \"string\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for weather online\"\"\"\n",
    "    return \"42f\"\n",
    "print(\"name:\", search.name)\n",
    "print(\"desc:\", search.description)\n",
    "print(\"args:\", json.dumps(search.args, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ae2f3",
   "metadata": {},
   "source": [
    "- `search(query: str)`: argument of functions marked as tool\n",
    "  - used as key of `dict` returned by `search.args`\n",
    "  - used as `title` after first character is capitalised\n",
    "\n",
    "**tool with pydantic schema example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7327fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42f\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from langchain.agents import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Thing to search for\")\n",
    "\n",
    "@tool(args_schema=SearchInput)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for the weather online.\"\"\"\n",
    "    return \"42f\"\n",
    "\n",
    "print(search.run(\"sf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae5617",
   "metadata": {},
   "source": [
    "- In Pydantic's `Field`, the `...` (Ellipsis) means the field is required.\n",
    "  - It's equivalent to specifying `default=None`\n",
    "- `format_tool_to_openai_function` has been deprecated\n",
    "  - `langchain_core.utils.function_calling.convert_to_openai_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428dd3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name: get_current_temperature\n",
      "desc: Fetch current temperature for given coordinates.\n",
      "args: {'latitude': {'description': 'Latitude of the location to fetch weather data for', 'title': 'Latitude', 'type': 'number'}, 'longitude': {'description': 'Longitude of the location to fetch weather data for', 'title': 'Longitude', 'type': 'number'}}\n",
      "openai_fn: {'name': 'get_current_temperature', 'description': 'Fetch current temperature for given coordinates.', 'parameters': {'properties': {'latitude': {'description': 'Latitude of the location to fetch weather data for', 'type': 'number'}, 'longitude': {'description': 'Longitude of the location to fetch weather data for', 'type': 'number'}}, 'required': ['latitude', 'longitude'], 'type': 'object'}}\n",
      "name: search_wikipedia\n",
      "desc: Run Wikipedia search and get page summaries.\n",
      "args: {'query': {'title': 'Query', 'type': 'string'}}\n",
      "wikipedia_fn: {'name': 'search_wikipedia', 'description': 'Run Wikipedia search and get page summaries.', 'parameters': {'properties': {'query': {'type': 'string'}}, 'required': ['query'], 'type': 'object'}}\n",
      "temp: The current temperature is 25.3°C\n",
      "wiki: Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Vector database\n",
      "Summary: A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\n",
      "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\n",
      "These feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\n",
      "Vector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\n",
      "Vector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\n",
      "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\n",
      "RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
      "The term RAG was first introduced in a 2020 research paper from Meta.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import wikipedia\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str).replace(tzinfo=datetime.timezone.utc) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            wikipedia.exceptions.PageError,\n",
    "            wikipedia.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "openai_fn = convert_to_openai_function(get_current_temperature)\n",
    "print(\"name:\", get_current_temperature.name)\n",
    "print(\"desc:\", get_current_temperature.description)\n",
    "print(\"args:\", get_current_temperature.args)\n",
    "print(\"openai_fn:\", openai_fn)\n",
    "\n",
    "wikipedia_fn = convert_to_openai_function(search_wikipedia)\n",
    "print(\"name:\", search_wikipedia.name)\n",
    "print(\"desc:\", search_wikipedia.description)\n",
    "print(\"args:\", search_wikipedia.args)\n",
    "print(\"wikipedia_fn:\", wikipedia_fn)\n",
    "\n",
    "print(\"temp:\", get_current_temperature.invoke({\"latitude\": 13, \"longitude\": 14}))\n",
    "print(\"wiki:\", search_wikipedia.invoke({\"query\": \"langchain\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d59fd2",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "Selecting a tool from the output of LLM is **routing**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b92fb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "content='' additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 105, 'total_tokens': 130, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--c8b719d0-5b24-4ce1-9d39-45f03ef18532-0' usage_metadata={'input_tokens': 105, 'output_tokens': 25, 'total_tokens': 130, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='' additional_kwargs={'function_call': {'arguments': '{\"query\":\"Langchain\"}', 'name': 'search_wikipedia'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 101, 'total_tokens': 117, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--daa65b4c-76c7-4cd8-9e52-aa19d4db8adf-0' usage_metadata={'input_tokens': 101, 'output_tokens': 16, 'total_tokens': 117, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "type : <class 'langchain_core.agents.AgentActionMessageLog'>\n",
      "tool : get_current_temperature\n",
      "input: {'latitude': 37.7749, 'longitude': -122.4194}\n",
      "type : <class 'langchain_core.agents.AgentFinish'>\n",
      "ret  : {'output': 'Well, hello there! How can I assist you today?'}\n",
      "The current temperature is 14.6°C\n",
      "Page: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "\n",
      "\n",
      "Page: Vector database\n",
      "Summary: A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\n",
      "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\n",
      "These feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\n",
      "Vector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\n",
      "Vector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\n",
      "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\n",
      "RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
      "The term RAG was first introduced in a 2020 research paper from Meta.\n",
      "Well, hello there! How can I assist you today?\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "functions =  [\n",
    "    convert_to_openai_function(f) for f in [\n",
    "        search_wikipedia,\n",
    "        get_current_temperature\n",
    "    ]\n",
    "]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "\n",
    "print(model.invoke(\"what is the weather in sf right now\"))\n",
    "print(model.invoke(\"what is langchain\"))\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model\n",
    "chain.invoke({\"input\": \"what is the weather in sf right now\"})\n",
    "\n",
    "chain_with_parser = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result = chain_with_parser.invoke({\"input\": \"what is the weather in sf right now\"})\n",
    "print(\"type :\", type(result))\n",
    "print(\"tool :\", result.tool)\n",
    "print(\"input:\", result.tool_input)\n",
    "\n",
    "result = chain_with_parser.invoke({\"input\": \"hi!\"})\n",
    "print(\"type :\", type(result))\n",
    "print(\"ret  :\", result.return_values)\n",
    "\n",
    "# With router\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        tools = {\n",
    "            search_wikipedia.name : search_wikipedia,\n",
    "            get_current_temperature.name : get_current_temperature,\n",
    "        }\n",
    "        return tools[result.tool].run(result.tool_input)\n",
    "\n",
    "chain_with_router = prompt | model | OpenAIFunctionsAgentOutputParser() | route\n",
    "\n",
    "print(chain_with_router.invoke({\"input\": \"What is the weather in san francisco right now?\"}))\n",
    "print(chain_with_router.invoke({\"input\": \"What is langchain?\"}))\n",
    "print(chain_with_router.invoke({\"input\": \"hi!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee9ae9",
   "metadata": {},
   "source": [
    "- `OpenAIFunctionsAgentOutputParser`\n",
    "  - parse return message from OpenAI into `AgentActionMessageLog` or `AgentFinish`\n",
    "  - `AgentActionMessageLog` can be routed to desired tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b19ab",
   "metadata": {},
   "source": [
    "# Direct interaction with OpenAPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6dc3465",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functions: [{'name': 'listPets', 'description': 'List all pets', 'parameters': {'type': 'object', 'properties': {'params': {'type': 'object', 'properties': {'limit': {'type': 'integer', 'maximum': 100.0, 'schema_format': 'int32', 'description': 'How many items to return at one time (max 100)'}}, 'required': []}}}}, {'name': 'createPets', 'description': 'Create a pet', 'parameters': {'type': 'object', 'properties': {}}}, {'name': 'showPetById', 'description': 'Info for a specific pet', 'parameters': {'type': 'object', 'properties': {'path_params': {'type': 'object', 'properties': {'petId': {'type': 'string', 'description': 'The id of the pet to retrieve'}}, 'required': ['petId']}}}}]\n",
      "callables: <function openapi_spec_to_openai_fn.<locals>.default_call_api at 0x125b27420>\n",
      "content='' additional_kwargs={'function_call': {'arguments': '{\"params\":{\"limit\":3}}', 'name': 'listPets'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 16, 'prompt_tokens': 123, 'total_tokens': 139, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--a9be950c-bb16-41a6-b94b-508275e05068-0' usage_metadata={'input_tokens': 123, 'output_tokens': 16, 'total_tokens': 139, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "content='' additional_kwargs={'function_call': {'arguments': '{\"path_params\":{\"petId\":\"42\"}}', 'name': 'showPetById'}, 'refusal': None} response_metadata={'token_usage': {'completion_tokens': 19, 'prompt_tokens': 126, 'total_tokens': 145, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None} id='run--dbd3102c-5323-4f04-8c77-160c2a4bcc7f-0' usage_metadata={'input_tokens': 126, 'output_tokens': 19, 'total_tokens': 145, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n"
     ]
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "{\n",
    "  \"openapi\": \"3.1.0\",\n",
    "  \"jsonSchemaDialect\": \"https://spec.openapis.org/oas/3.1/dialect/base\",\n",
    "  \"info\": {\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"title\": \"Swagger Petstore\",\n",
    "    \"license\": {\n",
    "      \"name\": \"MIT\"\n",
    "    }\n",
    "  },\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"url\": \"http://petstore.swagger.io/v1\"\n",
    "    }\n",
    "  ],\n",
    "  \"paths\": {\n",
    "    \"/pets\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"List all pets\",\n",
    "        \"operationId\": \"listPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"limit\",\n",
    "            \"in\": \"query\",\n",
    "            \"description\": \"How many items to return at one time (max 100)\",\n",
    "            \"required\": false,\n",
    "            \"schema\": {\n",
    "              \"type\": \"integer\",\n",
    "              \"maximum\": 100,\n",
    "              \"format\": \"int32\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"A paged array of pets\",\n",
    "            \"headers\": {\n",
    "              \"x-next\": {\n",
    "                \"description\": \"A link to the next page of responses\",\n",
    "                \"schema\": {\n",
    "                  \"type\": \"string\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pets\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"Unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"post\": {\n",
    "        \"summary\": \"Create a pet\",\n",
    "        \"operationId\": \"createPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"201\": {\n",
    "            \"description\": \"Created\"\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"Unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"/pets/{petId}\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"Info for a specific pet\",\n",
    "        \"operationId\": \"showPetById\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"petId\",\n",
    "            \"in\": \"path\",\n",
    "            \"required\": true,\n",
    "            \"description\": \"The id of the pet to retrieve\",\n",
    "            \"schema\": {\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"Expected response to a valid request\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pet\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"Unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"components\": {\n",
    "    \"schemas\": {\n",
    "      \"Pet\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"id\",\n",
    "          \"name\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int64\"\n",
    "          },\n",
    "          \"name\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"tag\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        },\n",
    "        \"additionalProperties\": false\n",
    "      },\n",
    "      \"Pets\": {\n",
    "        \"type\": \"array\",\n",
    "        \"maxItems\": 100,\n",
    "        \"items\": {\n",
    "          \"$ref\": \"#/components/schemas/Pet\"\n",
    "        }\n",
    "      },\n",
    "      \"Error\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"code\",\n",
    "          \"message\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"code\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int32\"\n",
    "          },\n",
    "          \"message\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        },\n",
    "        \"additionalProperties\": false\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chains.openai_functions.openapi import openapi_spec_to_openai_fn\n",
    "from langchain.utilities.openapi import OpenAPISpec\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "spec = OpenAPISpec.from_text(text)\n",
    "pet_openai_functions, pet_callables = openapi_spec_to_openai_fn(spec)\n",
    "print(\"functions:\", pet_openai_functions)\n",
    "print(\"callables:\", pet_callables)\n",
    "\n",
    "model = ChatOpenAI(temperature=0).bind(functions=pet_openai_functions)\n",
    "print(model.invoke(\"what are three pets names\"))\n",
    "print(model.invoke(\"tell me about pet with id 42\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be1172",
   "metadata": {},
   "source": [
    "- OpenAPISpec has an issue, and following change needs to be made to make it work\n",
    "    ```python\n",
    "    # openapi.py\n",
    "    @classmethod\n",
    "    def parse_obj(cls, obj: dict) -> OpenAPISpec:\n",
    "        try:\n",
    "            cls._alert_unsupported_spec(obj)\n",
    "            return super().model_validate(obj)\n",
    "        except ValidationError as e:\n",
    "            # We are handling possibly misconfigured specs and\n",
    "            # want to do a best-effort job to get a reasonable interface out of it.\n",
    "            new_obj = copy.deepcopy(obj)\n",
    "            for error in e.errors():\n",
    "                keys = error[\"loc\"]\n",
    "                item = new_obj\n",
    "                for key in keys[:-1]:\n",
    "                    item = item[key]\n",
    "                item.pop(keys[-1], None)\n",
    "            return cls.parse_obj(new_obj)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a1f8c",
   "metadata": {},
   "source": [
    "# Conversational Agent\n",
    "\n",
    "Agent is a combination of LLMs and code. LLMs reason about what steps to tak and call for actions.\n",
    "- Agent loop\n",
    "  - Choose a tool to use\n",
    "  - Observe the output of the tool\n",
    "  - Repeat until a stopping condition is met\n",
    "- Stopping condition can be\n",
    "  - LLM determined\n",
    "  - Hardcoded rules\n",
    "\n",
    "**defining tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import wikipedia\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str).replace(tzinfo=datetime.timezone.utc) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            wikipedia.exceptions.PageError,\n",
    "            wikipedia.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "tools = [get_current_temperature, search_wikipedia]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625d8af",
   "metadata": {},
   "source": [
    "**chain of function and tool example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20330ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "result: tool='get_current_temperature' tool_input={'latitude': 37.7749, 'longitude': -122.4194} log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--89239625-f9cd-46aa-b160-4fbf19524dd9-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "tool  : get_current_temperature\n",
      "toolin: {'latitude': 37.7749, 'longitude': -122.4194}\n",
      "result : tool='get_current_temperature' tool_input={'latitude': 37.7749, 'longitude': -122.4194} log=\"\\nInvoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\\n\\n\\n\" message_log=[AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--508f448d-42b3-402b-a03e-9ff55edd54f7-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "type.  : <class 'langchain_core.agents.AgentActionMessageLog'>\n",
      "tool   : get_current_temperature\n",
      "msg log: [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--508f448d-42b3-402b-a03e-9ff55edd54f7-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]\n",
      "observ : The current temperature is 14.4°C\n",
      "openfn : [AIMessage(content='', additional_kwargs={'function_call': {'arguments': '{\"latitude\":37.7749,\"longitude\":-122.4194}', 'name': 'get_current_temperature'}, 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 112, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'function_call', 'logprobs': None}, id='run--508f448d-42b3-402b-a03e-9ff55edd54f7-0', usage_metadata={'input_tokens': 112, 'output_tokens': 25, 'total_tokens': 137, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}), FunctionMessage(content='The current temperature is 14.4°C', additional_kwargs={}, response_metadata={}, name='get_current_temperature')]\n",
      "result2: return_values={'output': 'The current temperature in San Francisco is 14.4°C.'} log='The current temperature in San Francisco is 14.4°C.'\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain_core.agents import AgentAction, AgentActionMessageLog\n",
    "\n",
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "# chain basic\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result = chain.invoke({\"input\": \"what is the weather is sf?\"})\n",
    "print(\"result:\", result)\n",
    "if  isinstance(result, AgentActionMessageLog):\n",
    "    print(\"tool  :\", result.tool)\n",
    "    print(\"toolin:\", result.tool_input)\n",
    "\n",
    "# chain1\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result1 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\",\n",
    "    \"agent_scratchpad\": []\n",
    "})\n",
    "\n",
    "print(\"result1:\", result1)\n",
    "print(\"type.  :\", type(result1))\n",
    "if  isinstance(result1, AgentActionMessageLog):\n",
    "    print(\"tool   :\", result1.tool)\n",
    "    print(\"msg log:\", result1.message_log)\n",
    "\n",
    "    observation = get_current_temperature.invoke(result1.tool_input)\n",
    "    openai_fn_format = format_to_openai_functions([(result1, observation), ])\n",
    "    result2 = chain.invoke({\n",
    "        \"input\": \"what is the weather is sf?\", \n",
    "        \"agent_scratchpad\": openai_fn_format,\n",
    "    })\n",
    "    print(\"observ :\", observation)\n",
    "    print(\"openfn :\", openai_fn_format)\n",
    "    print(\"result2:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6e2a6",
   "metadata": {},
   "source": [
    "**agent chain example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b3646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "return_values={'output': 'The current temperature in San Francisco is 14.4°C.'} log='The current temperature in San Francisco is 14.4°C.'\n",
      "return_values={'output': 'I couldn\\'t find specific information about \"Langchain.\" However, I found information about LangChain, which is a software framework that helps integrate large language models into applications. It is used for tasks like document analysis, chatbots, and code analysis. Let me know if you need more details about LangChain or if you were referring to something else.'} log='I couldn\\'t find specific information about \"Langchain.\" However, I found information about LangChain, which is a software framework that helps integrate large language models into applications. It is used for tasks like document analysis, chatbots, and code analysis. Let me know if you need more details about LangChain or if you were referring to something else.'\n",
      "return_values={'output': 'Well, hello there! How can I assist you today?'} log='Well, hello there! How can I assist you today?'\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `search_wikipedia` with `{'query': 'Langchain'}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[33;1m\u001b[1;3mPage: LangChain\n",
      "Summary: LangChain is a software framework that helps facilitate the integration of large language models (LLMs) into applications. As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
      "\n",
      "Page: Vector database\n",
      "Summary: A vector database, vector store or vector search engine is a database that uses the vector space model to store vectors (fixed-length lists of numbers) along with other data items. Vector databases typically implement one or more approximate nearest neighbor algorithms, so that one can search the database with a query vector to retrieve the closest matching database records.\n",
      "Vectors are mathematical representations of data in a high-dimensional space. In this space, each dimension corresponds to a feature of the data, with the number of dimensions ranging from a few hundred to tens of thousands, depending on the complexity of the data being represented. A vector's position in this space represents its characteristics. Words, phrases, or entire documents, as well as images, audio, and other types of data, can all be vectorized.\n",
      "These feature vectors may be computed from the raw data using machine learning methods such as feature extraction algorithms, word embeddings or deep learning networks. The goal is that semantically similar data items receive feature vectors close to each other.\n",
      "Vector databases can be used for similarity search, semantic search, multi-modal search, recommendations engines, large language models (LLMs), object detection,  etc.\n",
      "Vector databases are also often used to implement retrieval-augmented generation (RAG), a method to improve domain-specific responses of large language models. The retrieval component of a RAG can be any search system, but is most often implemented as a vector database. Text documents describing the domain of interest are collected, and for each document or document section, a feature vector (known as an \"embedding\") is computed, typically using a deep learning network, and stored in a vector database. Given a user prompt, the feature vector of the prompt is computed, and the database is queried to retrieve the most relevant documents. These are then automatically added into the context window of the large language model, and the large language model proceeds to create a response to the prompt given this context.\n",
      "\n",
      "Page: Retrieval-augmented generation\n",
      "Summary: Retrieval-augmented generation (RAG) is a technique that enables large language models (LLMs) to retrieve and incorporate new information. With RAG, LLMs do not respond to user queries until they refer to a specified set of documents. These documents supplement information from the LLM's pre-existing training data. This allows LLMs to use domain-specific and/or updated information that is not available in the training data. For example, this helps LLM-based chatbots access internal company data or generate responses based on authoritative sources.\n",
      "RAG improves large language models (LLMs) by incorporating information retrieval before generating responses. Unlike traditional LLMs that rely on static training data, RAG pulls relevant text from databases, uploaded documents, or web sources. According to Ars Technica, \"RAG is a way of improving LLM performance, in essence by blending the LLM process with a web search or other document look-up process to help LLMs stick to the facts.\" This method helps reduce AI hallucinations, which have caused chatbots to describe policies that don't exist, or recommend nonexistent legal cases to lawyers that are looking for citations to support their arguments.\n",
      "RAG also reduces the need to retrain LLMs with new data, saving on computational and financial costs. Beyond efficiency gains, RAG also allows LLMs to include sources in their responses, so users can verify the cited sources. This provides greater transparency, as users can cross-check retrieved content to ensure accuracy and relevance.\n",
      "The term RAG was first introduced in a 2020 research paper from Meta.\u001b[0m\u001b[32;1m\u001b[1;3mI couldn't find specific information about \"Langchain.\" However, I found information about LangChain, which is a software framework that helps integrate large language models into applications. It is used for tasks like document analysis, chatbots, and code analysis. Let me know if you want more details about LangChain or if you were referring to something else.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ae: {'input': 'what is langchain?', 'output': 'I couldn\\'t find specific information about \"Langchain.\" However, I found information about LangChain, which is a software framework that helps integrate large language models into applications. It is used for tasks like document analysis, chatbots, and code analysis. Let me know if you want more details about LangChain or if you were referring to something else.'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNice to meet you, Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ae: {'input': 'my name is bob', 'output': 'Nice to meet you, Bob! How can I assist you today?'}\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mI'm sorry, I don't have access to your personal information. How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "ae: {'input': 'what is my name', 'output': \"I'm sorry, I don't have access to your personal information. How can I assist you today?\"}\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input,                     # initial user input\n",
    "            \"intermediate_steps\": intermediate_steps # keep growing with observation\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))\n",
    "\n",
    "print(run_agent(\"what is the weather is sf?\"))\n",
    "print(run_agent(\"what is langchain?\"))\n",
    "print(run_agent(\"hi!\"))\n",
    "\n",
    "# with wrapper AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"what is langchain?\"})\n",
    "agent_executor.invoke({\"input\": \"my name is bob\"})\n",
    "agent_executor.invoke({\"input\": \"what is my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b88c9",
   "metadata": {},
   "source": [
    "- `RunnablePassthrough`\n",
    "    - Runnable to passthrough inputs unchanged or with additional keys.\n",
    "    - `assign`: unchainged input with additional key-val pair from assigned by `assign`\n",
    "    ```python\n",
    "    runnable = {\n",
    "      'llm1':  lambda: \"llm1_fin\",\n",
    "      'llm2':  lambda: \"llm2_fin\",\n",
    "      } | RunnablePassthrough.assign(\n",
    "          total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    "      )\n",
    "      runnable.invoke('hello')\n",
    "      # {'llm1': 'llm1_fin', 'llm2': 'llm2_fin', 'total_chars': 16}\n",
    "    ```\n",
    "- `format_to_openai_functions`\n",
    "  - Convert `(AgentAction, tool output)` tuples into `FunctionMessages`.\n",
    "  - when input is empty list `[]`, return `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "69dd04af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/8h107ckj52d2fz6fb8wfmfjm0000gn/T/ipykernel_10711/794828385.py:18: LangChainDeprecationWarning: Please see the migration guide at: https://python.langchain.com/docs/versions/migrating_memory/\n",
      "  memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mNice to meet you, Bob! How can I assist you today?\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mYour name is Bob.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Invoking: `get_current_temperature` with `{'latitude': 37.7749, 'longitude': -122.4194}`\n",
      "\n",
      "\n",
      "\u001b[0m\u001b[36;1m\u001b[1;3mThe current temperature is 14.4°C\u001b[0m\u001b[32;1m\u001b[1;3mThe current temperature in San Francisco is 14.4°C.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input': 'whats the weather in sf?',\n",
       " 'chat_history': [HumanMessage(content='my name is bob', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Nice to meet you, Bob! How can I assist you today?', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='whats my name', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='Your name is Bob.', additional_kwargs={}, response_metadata={}),\n",
       "  HumanMessage(content='whats the weather in sf?', additional_kwargs={}, response_metadata={}),\n",
       "  AIMessage(content='The current temperature in San Francisco is 14.4°C.', additional_kwargs={}, response_metadata={})],\n",
       " 'output': 'The current temperature in San Francisco is 14.4°C.'}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# With memory\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"my name is bob\"})\n",
    "agent_executor.invoke({\"input\": \"whats my name\"})\n",
    "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6c3d3",
   "metadata": {},
   "source": [
    "# Custom Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_your_own(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]\n",
    "\n",
    "tools = [get_current_temperature, search_wikipedia, create_your_own]\n",
    "\n",
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    \n",
    "    def __init__(self, tools, **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are helpful but sassy assistant\"),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        self.chain = RunnablePassthrough.assign(\n",
    "            agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "        ) | self.prompt | self.model | OpenAIFunctionsAgentOutputParser()\n",
    "        self.qa = AgentExecutor(agent=self.chain, tools=tools, verbose=False, memory=self.memory)\n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        inp.value = ''\n",
    "        result = self.qa.invoke({\"input\": query})\n",
    "        self.answer = result['output'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=450)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=450, styles={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        return pn.WidgetBox(*self.panels, scroll=True)\n",
    "\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return\n",
    "\n",
    "# Chatbot\n",
    "cb = cbfs(tools)\n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=400),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# QnA_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1))\n",
    ")\n",
    "dashboard"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-study-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
