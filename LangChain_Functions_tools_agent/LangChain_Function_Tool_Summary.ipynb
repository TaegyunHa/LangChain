{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784c5b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Tavily API key\n",
    "def setup_openai_api_key():\n",
    "    from dotenv import load_dotenv, find_dotenv\n",
    "    _ = load_dotenv(find_dotenv())\n",
    "setup_openai_api_key()\n",
    "\n",
    "LLM_MODEL:str = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34eba92f",
   "metadata": {},
   "source": [
    "# OpenAI Function calling\n",
    "\n",
    "> https://platform.openai.com/docs/guides/function-calling\n",
    "\n",
    "| Image | Description |\n",
    "| ----- | ----------- |\n",
    "<img src=\"./images/openai-function-calling.png\" alt=\"sequence\" height=\"600\"/> | With the function calling of OpenAI, the LLM will return:<br/><ul><li>which function to call</li><li>arguments of the function</li></ul>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e91dbd",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "**To leverage the function calling of OpenAI:**\n",
    "\n",
    "1. define tools: [example](https://platform.openai.com/docs/guides/function-calling#defining-functions)\n",
    "2. pass list of tools along with prompt into LLM\n",
    "3. check `Response.output.type` is `\"function_call\"`\n",
    "4. If it is, excute code on the application side with input from the tool call\n",
    "5. Make a second request to the model with the tool output\n",
    "6. Receive a final response from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7581b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from openai.types.responses.function_tool_param import FunctionToolParam\n",
    "import json\n",
    "\n",
    "# mock of tool call of application\n",
    "def get_current_weather(location: str, unit=\"celsius\"):\n",
    "    \"\"\"Get the current weather in a given location\"\"\"\n",
    "    weather_info = {\n",
    "        \"location\": location,\n",
    "        \"temperature\": \"26\",\n",
    "        \"unit\": unit,\n",
    "        \"forecast\": [\"sunny\", \"windy\"],\n",
    "    }\n",
    "    return json.dumps(weather_info)\n",
    "\n",
    "# define a list of callable tools for the model\n",
    "tools: list[FunctionToolParam]= [{\n",
    "    \"name\": \"get_current_weather\",\n",
    "    \"description\": \"Get the current weather in a given location\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"location\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The city and country, e.g. London, UK\",\n",
    "            },\n",
    "            \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]},\n",
    "        },\n",
    "        \"required\": [\"location\"],\n",
    "    },\n",
    "    \"strict\": None,\n",
    "    \"type\": \"function\"\n",
    "}]\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What's the weather like in London?\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Prompt the model with tools defined\n",
    "client = OpenAI()\n",
    "response = client.responses.create(\n",
    "    model=LLM_MODEL,\n",
    "    tools=tools,\n",
    "    input=messages,\n",
    ")\n",
    "\n",
    "# Save function call outputs for subsequent requests\n",
    "messages += response.output\n",
    "\n",
    "for item in response.output:\n",
    "    if item.type == \"function_call\":\n",
    "        if item.name == \"get_current_weather\":\n",
    "            # 3. Execute the function logic for get_horoscope\n",
    "            weather = get_current_weather(json.loads(item.arguments))\n",
    "            \n",
    "            # 4. Provide function call results to the model\n",
    "            messages.append({\n",
    "                \"type\": \"function_call_output\",\n",
    "                \"call_id\": item.call_id,\n",
    "                \"output\": json.dumps({\n",
    "                  \"weather\": weather\n",
    "                })\n",
    "            })\n",
    "\n",
    "print(\"Final input:\")\n",
    "print(messages)\n",
    "response = client.responses.create(\n",
    "    model=LLM_MODEL,\n",
    "    tools=tools,\n",
    "    input=messages,\n",
    ")\n",
    "\n",
    "print(\"Final output:\")\n",
    "print(response.model_dump_json(indent=2))\n",
    "print(\"\\n\", response.output_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaa07b4",
   "metadata": {},
   "source": [
    "# LangChain Expression Language (LCEL)\n",
    "\n",
    "LCEL and the runnable protocol defines \"what\" should happen, rather than \"how\". It's important that a \"chain\" is `Runnable`.\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[block1] --> B[/allowed set of input types/]\n",
    "    subgraph box2\n",
    "    B --> C[required methods<br/>invoke, stream, batch]\n",
    "    C --> D[/output types/]\n",
    "    end\n",
    "    D --> E[block3]\n",
    "```\n",
    "\n",
    "**Example of composition:**\n",
    "\n",
    "```python\n",
    "chain = prompt | llm | OutputParser\n",
    "```\n",
    "\n",
    "**Interface**\n",
    "\n",
    "- Components implement `Runnable` protocol\n",
    "- Common methods\n",
    "  - `invoke` / `ainvoke`\n",
    "  - `stream` / `astream`\n",
    "  - `batch` / `abatch`\n",
    "- Common properties\n",
    "  - `input_schema`\n",
    "  - `output_schema`\n",
    "- Common I/O\n",
    "    | Component     | Input Type                              | Output Type       |\n",
    "    |---------------|-----------------------------------------|-------------------|\n",
    "    | Prompt        | Dictionary                              | Prompt Value      |\n",
    "    | Retriever     | Single String                           | List of Documents |\n",
    "    | LLM           | String, list of messages or Prompt Value| String            |\n",
    "    | ChatModel     | String, list of messages or Prompt Value| ChatMessage       |\n",
    "    | Tool          | String/Dictionary                       | Tool dependent    |\n",
    "    | Output Parser | Output of LLM or ChatModel              | Parser dependent  |\n",
    "\n",
    "**Supported feature**\n",
    "\n",
    "- async, batch and stream\n",
    "- fallbacks\n",
    "- parallelism\n",
    "- logging\n",
    "\n",
    "### Simple chain example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abc8091a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "chain = prompt | model | output_parser\n",
    "chain.invoke({\"topic\": \"bear\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5fc812e",
   "metadata": {},
   "source": [
    "### Chain with embedding as context example\n",
    "\n",
    "With `RunnableMap`, one input can be converted into the other format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7892e2f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import DocArrayInMemorySearch\n",
    "from langchain.schema.runnable import RunnableMap\n",
    "\n",
    "vectorstore = DocArrayInMemorySearch.from_texts(\n",
    "    [\n",
    "        \"harrison worked at kensho\",\n",
    "        \"mears like to eat honey\"\n",
    "    ],\n",
    "    embedding=OpenAIEmbeddings()\n",
    ")\n",
    "retriever = vectorstore.as_retriever()\n",
    "\n",
    "print(\"get_relevant_documents\")\n",
    "print(\"- \", retriever.get_relevant_documents(\"where did harrison work?\"))\n",
    "print(\"- \", retriever.get_relevant_documents(\"what do bears like to eat?\"))\n",
    "\n",
    "template = \"\"\"\\\n",
    "Answer the question based only on the following context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "model = ChatOpenAI()\n",
    "chain= RunnableMap({\n",
    "    \"context\": lambda x: retriever.get_relevant_documents(x[\"question\"]),\n",
    "    \"question\": lambda x: x[\"question\"]\n",
    "}) | prompt | model | output_parser\n",
    "\n",
    "chain.invoke({\"question\": \"where did harrison work?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479136de",
   "metadata": {},
   "source": [
    "### Chain with function bind\n",
    "\n",
    "With function binding, LLM can extract relavant function name along with required arguments from the user message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d29aef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "functions = [\n",
    "    {\n",
    "      \"name\": \"weather_search\",\n",
    "      \"description\": \"Search for weather given an airport code\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"airport_code\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The airport code to get the weather for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"airport_code\"]\n",
    "      }\n",
    "    },\n",
    "    {\n",
    "      \"name\": \"sports_search\",\n",
    "      \"description\": \"Search for news of recent sport events\",\n",
    "      \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "          \"team_name\": {\n",
    "            \"type\": \"string\",\n",
    "            \"description\": \"The sports team to search for\"\n",
    "          },\n",
    "        },\n",
    "        \"required\": [\"team_name\"]\n",
    "      }\n",
    "    }\n",
    "]\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "chain = prompt | model\n",
    "\n",
    "response = chain.invoke({\"input\": \"what is the weather in London?\"})\n",
    "print(response)\n",
    "response = chain.invoke({\"input\": \"how did the patriots do yesterday?\"})\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bbfbae",
   "metadata": {},
   "source": [
    "### Fallbacks\n",
    "\n",
    "With fallbacks, we can handle LLM call failure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b41a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "import json\n",
    "\n",
    "simple_model = OpenAI(\n",
    "    temperature=0, \n",
    "    max_tokens=100, \n",
    "    model=\"gpt-3.5-turbo-instruct\"\n",
    ")\n",
    "simple_chain = simple_model | json.loads\n",
    "\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "chain = model | StrOutputParser() | json.loads\n",
    "\n",
    "final_chain = simple_chain.with_fallbacks([chain])\n",
    "\n",
    "input = \"write three poems in a json blob, where each poem is a json blob of a title, author, and first line\"\n",
    "\n",
    "try:\n",
    "    simple_chain.invoke(input)\n",
    "except Exception as e:\n",
    "    print(f\"Simple chain failed: {e}\")\n",
    "\n",
    "final_chain.invoke(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "968f5abf",
   "metadata": {},
   "source": [
    "### Interface\n",
    "\n",
    "As mentioned above, `invoke`, `stream`, and `batch` are supported interface along with their async implentation.\n",
    "- `invoke`: normal run\n",
    "- `stream`: async yeild\n",
    "- `batch`: parallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b71327b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"Tell me a short joke about {topic}\"\n",
    ")\n",
    "model = ChatOpenAI()\n",
    "output_parser = StrOutputParser()\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "response = chain.invoke({\"topic\": \"bears\"})\n",
    "print(\"invoke:\", response)\n",
    "response = chain.batch([{\"topic\": \"bears\"}, {\"topic\": \"frogs\"}])\n",
    "print(\"batch:\", response)\n",
    "print(\"stream:\")\n",
    "for t in chain.stream({\"topic\": \"bears\"}):\n",
    "    print(t)\n",
    "\n",
    "response = await chain.ainvoke({\"topic\": \"bears\"})\n",
    "print(\"ainvoke:\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68c717",
   "metadata": {},
   "source": [
    "# OpenAI Function Calling in LangChain\n",
    "\n",
    "pydantic can be used to create a function definition. It's important to note that pydantic object itself is not used, but it's used to create a schema.\n",
    "\n",
    "With function callings, LLM can:\n",
    "1. invoke LLM with optional functions\n",
    "2. bind functions to the LLM model to always refer to available functions\n",
    "3. force LLM to return function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3556988f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class WeatherSearch(BaseModel):\n",
    "    \"\"\"Call this with an airport code to get the weather at that airport\"\"\"\n",
    "    airport_code: str = Field(description=\"airport code to get weather for\")\n",
    "\n",
    "weather_function = convert_pydantic_to_openai_function(WeatherSearch)\n",
    "print(json.dumps(weather_function, indent=2))\n",
    "\n",
    "model = ChatOpenAI()\n",
    "# 1. invoke with functions directly\n",
    "print(\"ivoke-functions:\", model.invoke(\"what is the weather in SF today?\", functions=[weather_function]))\n",
    "\n",
    "# 2. bind functions to model first then invoke\n",
    "model_with_function = model.bind(functions=[weather_function])\n",
    "print(\"functions:\", model_with_function.invoke(\"what is the weather in SF today?\"))\n",
    "\n",
    "# 3. force the model to use function\n",
    "model_with_forced_function = model.bind(functions=[weather_function], function_call={\"name\":\"WeatherSearch\"})\n",
    "print(\"function_call:\", model_with_forced_function.invoke(\"hi!\"))\n",
    "\n",
    "# 4. with chain\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "chain = prompt | model_with_function\n",
    "print(\"chain:\", chain.invoke({\"input\": \"what is the weather in sf?\"}))\n",
    "\n",
    "# 5. multiple functions\n",
    "class ArtistSearch(BaseModel):\n",
    "    \"\"\"Call this to get the names of songs by a particular artist\"\"\"\n",
    "    artist_name: str = Field(description=\"name of artist to look up\")\n",
    "    n: int = Field(description=\"number of results\")\n",
    "functions = [\n",
    "    convert_pydantic_to_openai_function(WeatherSearch),\n",
    "    convert_pydantic_to_openai_function(ArtistSearch),\n",
    "]\n",
    "model_with_functions = model.bind(functions=functions)\n",
    "print(\"model_with_functions:\", model_with_functions.invoke(\"what is the weather in sf?\"))\n",
    "print(\"model_with_functions:\", model_with_functions.invoke(\"what are three songs by taylor swift?\"))\n",
    "print(\"model_with_functions:\", model_with_functions.invoke(\"hi!\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba815c5f",
   "metadata": {},
   "source": [
    "- `convert_pydantic_to_openai_function`\n",
    "  - description of `BaseModel` is enforced (comment of pydantic class).\n",
    "  - description of parametres are optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98c1cdf0",
   "metadata": {},
   "source": [
    "# Tagging\n",
    "\n",
    "LLM can evaluate the input text and generate structured output.\n",
    "- LLM has been fine tuned to find and fill in the parameters when JSON schema is provided.\n",
    "\n",
    "**Tagging means labeling a document with classes such as:**\n",
    "- Sentiment\n",
    "- Language\n",
    "- Style (formal, informal etc.)\n",
    "- Covered topics\n",
    "\n",
    "**The core components of taggings are:**\n",
    "- `function`: specify how the model tag an input\n",
    "- `schema`: defines how to tag an input\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    subgraph tagging schema\n",
    "    direction TB\n",
    "    A[description] --> B[\"Tagging(BaseModel)\"]\n",
    "    C[label] --> B\n",
    "    D[description] --> C\n",
    "    end\n",
    "    B --> E[convert_pydantic_<br/>to_openai_function]\n",
    "    E --> F[llm]\n",
    "    G[input text] --> F\n",
    "    F --> H[structured output]\n",
    "\n",
    "    style F fill:#bbf,color:#000\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cc5d8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser\n",
    "\n",
    "class Sentiment(StrEnum):\n",
    "    positive = \"positive\"\n",
    "    negative = \"negative\"\n",
    "    neutral = \"neutral\"\n",
    "\n",
    "class Tagging(BaseModel):\n",
    "    \"\"\"Tag the piece of text with particular info.\"\"\"\n",
    "    sentiment: Sentiment = Field(description=\"sentiment of text\")\n",
    "    language: str = Field(description=\"language of text (should be ISO 639-1 code)\")\n",
    "\n",
    "model = ChatOpenAI(temperature=0)\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Think carefully, and then tag the text as instructed\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])\n",
    "\n",
    "tagging_functions = [convert_pydantic_to_openai_function(Tagging)]\n",
    "print(tagging_functions)\n",
    "model_with_functions = model.bind(\n",
    "    functions=tagging_functions,\n",
    "    function_call={\"name\": \"Tagging\"}\n",
    ")\n",
    "# Without parser\n",
    "tagging_chain = prompt | model_with_functions\n",
    "print(tagging_chain.invoke({\"input\": \"I love langchain\"}))\n",
    "print(tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"}))\n",
    "# With parser\n",
    "tagging_chain = prompt | model_with_functions | JsonOutputFunctionsParser()\n",
    "print(tagging_chain.invoke({\"input\": \"I love langchain\"}))\n",
    "print(tagging_chain.invoke({\"input\": \"non mi piace questo cibo\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4d7a058",
   "metadata": {},
   "source": [
    "# Extraction\n",
    "\n",
    "Similar to tagging, LLM can extract multiple pieces of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c5a3e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "from langchain.utils.openai_functions import convert_pydantic_to_openai_function\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "class Person(BaseModel):\n",
    "    \"\"\"Information about a person\"\"\"\n",
    "    name: str = Field(description=\"person's name\")\n",
    "    age: int|None = Field(description=\"person's age\")\n",
    "\n",
    "class Information(BaseModel):\n",
    "    \"\"\"Information to extract.\"\"\"\n",
    "    people: list[Person] = Field(description=\"List of info about people\")\n",
    "\n",
    "extraction_functions = [convert_pydantic_to_openai_function(Information)]\n",
    "print(extraction_functions)\n",
    "\n",
    "model = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "extraction_model = model.bind(functions=extraction_functions)\n",
    "extraction_chain = extraction_model | JsonOutputFunctionsParser()\n",
    "print(extraction_chain.invoke(\"Joe is 30, his mom is Martha\"))\n",
    "\n",
    "# with prompt to make sure to avoid filling incorrect information\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "extraction_chain = prompt | extraction_model | JsonOutputFunctionsParser()\n",
    "print(extraction_chain.invoke({\"input\": \"Joe is 30, his mom is Martha\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "775aaaba",
   "metadata": {},
   "source": [
    "## Online document summarise example\n",
    "\n",
    "**Loading document**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a36555",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://lilianweng.github.io/posts/2023-06-23-agent/\")\n",
    "documents = loader.load()\n",
    "doc = documents[0]\n",
    "page_content = doc.page_content[:10000]\n",
    "print(page_content[:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fe6a382",
   "metadata": {},
   "source": [
    "**Summarise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c35ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "from langchain.output_parsers.openai_functions import JsonOutputFunctionsParser, JsonKeyOutputFunctionsParser\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.schema.runnable import RunnableLambda\n",
    "\n",
    "# setup model with prompt\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Extract the relevant information, if not explicitly provided do not guess. Extract partial info\"),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "model = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "\n",
    "# extraction schema\n",
    "class Overview(BaseModel):\n",
    "    \"\"\"Overview of a section of text.\"\"\"\n",
    "    summary: str = Field(description=\"Provide a concise summary of the content.\")\n",
    "    language: str = Field(description=\"Provide the language that the content is written in.\")\n",
    "    keywords: str = Field(description=\"Provide keywords related to the content.\")\n",
    "overview_tagging_function = convert_pydantic_to_openai_function(Overview)\n",
    "print(overview_tagging_function)\n",
    "\n",
    "# chain everything to extract overview\n",
    "tagging_model = model.bind(\n",
    "    functions=[overview_tagging_function],\n",
    "    function_call={\"name\":overview_tagging_function[\"name\"]}\n",
    ")\n",
    "tagging_chain = prompt | tagging_model | JsonOutputFunctionsParser()\n",
    "print(tagging_chain.invoke({\"input\": page_content}))\n",
    "\n",
    "# chain to extract paper information\n",
    "class Paper(BaseModel):\n",
    "    \"\"\"Information about papers mentioned.\"\"\"\n",
    "    title: str = Field(description=\"Title of a paper\")\n",
    "    author: str|None = Field(description=\"Author of a paper\")\n",
    "\n",
    "class Info(BaseModel):\n",
    "    \"\"\"Information to extract\"\"\"\n",
    "    papers: list[Paper] = Field(description=\"List of information about paper\")\n",
    "\n",
    "paper_extraction_function = convert_pydantic_to_openai_function(Info)\n",
    "extraction_model = model.bind(\n",
    "    functions=[paper_extraction_function], \n",
    "    function_call={\"name\":paper_extraction_function[\"name\"]}\n",
    ")\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "print(extraction_chain.invoke({\"input\": page_content}))\n",
    "\n",
    "# improved chain to extract paper information with prompt\n",
    "template = \"\"\"\\\n",
    "A article will be passed to you. Extract from it all papers that are mentioned by this article follow by its author. \n",
    "\n",
    "Do not extract the name of the article itself. If no papers are mentioned that's fine - you don't need to extract any! Just return an empty list.\n",
    "\n",
    "Do not make up or guess ANY extra information. Only extract what exactly is in the text.\"\"\"\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", template),\n",
    "    (\"human\", \"{input}\")\n",
    "])\n",
    "extraction_chain = prompt | extraction_model | JsonKeyOutputFunctionsParser(key_name=\"papers\")\n",
    "print(extraction_chain.invoke({\"input\": page_content}))\n",
    "print(extraction_chain.invoke({\"input\": \"hi\"}))\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_overlap=0)\n",
    "splits = text_splitter.split_text(doc.page_content)\n",
    "\n",
    "def flatten(matrix):\n",
    "    \"\"\"flatten list of list into a list\"\"\"\n",
    "    flat_list = []\n",
    "    for row in matrix:\n",
    "        flat_list += row\n",
    "    return flat_list\n",
    "\n",
    "doc_splits = RunnableLambda(\n",
    "    lambda x: [{\"input\": doc} for doc in text_splitter.split_text(x)]\n",
    ")\n",
    "chain = doc_splits | extraction_chain.map() | flatten\n",
    "print(chain.invoke(doc.page_content))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb3ae96",
   "metadata": {},
   "source": [
    "- `Runnable.map`:\n",
    "    - Return a new `Runnable` that maps a list of inputs to a list of outputs.\n",
    "    - Calls invoke() with each input."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ca1692",
   "metadata": {},
   "source": [
    "# Tools and Routing\n",
    "\n",
    "> https://python.langchain.com/docs/integrations/tools/\n",
    "\n",
    "Tools are utilities designed to be called by a model. Their inputs are designed to be generated by models, and their outputs are designed to be passed back to models. Functions and services in LLM can be represented as **tools** in LangChain. \n",
    "\n",
    "LangChain offers many tools such as:\n",
    "- Search tools\n",
    "- Math tools\n",
    "- SQL tools\n",
    "\n",
    "**basic tool definition example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400f4e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.agents import tool\n",
    "\n",
    "@tool\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for weather online\"\"\"\n",
    "    return \"42f\"\n",
    "print(\"name:\", search.name)\n",
    "print(\"desc:\", search.description)\n",
    "print(\"args:\", json.dumps(search.args, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979ae2f3",
   "metadata": {},
   "source": [
    "- `search(query: str)`: argument of functions marked as tool\n",
    "  - used as key of `dict` returned by `search.args`\n",
    "  - used as `title` after first character is capitalised\n",
    "\n",
    "**tool with pydantic schema example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7327fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.agents import tool\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "class SearchInput(BaseModel):\n",
    "    query: str = Field(description=\"Thing to search for\")\n",
    "\n",
    "@tool(args_schema=SearchInput)\n",
    "def search(query: str) -> str:\n",
    "    \"\"\"Search for the weather online.\"\"\"\n",
    "    return \"42f\"\n",
    "\n",
    "print(search.run(\"sf\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40ae5617",
   "metadata": {},
   "source": [
    "- In Pydantic's `Field`, the `...` (Ellipsis) means the field is required.\n",
    "  - It's equivalent to specifying `default=None`\n",
    "- `format_tool_to_openai_function` has been deprecated\n",
    "  - `langchain_core.utils.function_calling.convert_to_openai_function`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0428dd3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import wikipedia\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_core.utils.function_calling import convert_to_openai_function\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    \n",
    "    # Parameters for the request\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str).replace(tzinfo=datetime.timezone.utc) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            wikipedia.exceptions.PageError,\n",
    "            wikipedia.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "openai_fn = convert_to_openai_function(get_current_temperature)\n",
    "print(\"name:\", get_current_temperature.name)\n",
    "print(\"desc:\", get_current_temperature.description)\n",
    "print(\"args:\", get_current_temperature.args)\n",
    "print(\"openai_fn:\", openai_fn)\n",
    "\n",
    "wikipedia_fn = convert_to_openai_function(search_wikipedia)\n",
    "print(\"name:\", search_wikipedia.name)\n",
    "print(\"desc:\", search_wikipedia.description)\n",
    "print(\"args:\", search_wikipedia.args)\n",
    "print(\"wikipedia_fn:\", wikipedia_fn)\n",
    "\n",
    "print(\"temp:\", get_current_temperature.invoke({\"latitude\": 13, \"longitude\": 14}))\n",
    "print(\"wiki:\", search_wikipedia.invoke({\"query\": \"langchain\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8d59fd2",
   "metadata": {},
   "source": [
    "# Routing\n",
    "\n",
    "Selecting a tool from the output of LLM is **routing**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b92fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "functions =  [\n",
    "    convert_to_openai_function(f) for f in [\n",
    "        search_wikipedia,\n",
    "        get_current_temperature\n",
    "    ]\n",
    "]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "\n",
    "print(model.invoke(\"what is the weather in sf right now\"))\n",
    "print(model.invoke(\"what is langchain\"))\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model\n",
    "chain.invoke({\"input\": \"what is the weather in sf right now\"})\n",
    "\n",
    "chain_with_parser = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result = chain_with_parser.invoke({\"input\": \"what is the weather in sf right now\"})\n",
    "print(\"type :\", type(result))\n",
    "print(\"tool :\", result.tool)\n",
    "print(\"input:\", result.tool_input)\n",
    "\n",
    "result = chain_with_parser.invoke({\"input\": \"hi!\"})\n",
    "print(\"type :\", type(result))\n",
    "print(\"ret  :\", result.return_values)\n",
    "\n",
    "# With router\n",
    "def route(result):\n",
    "    if isinstance(result, AgentFinish):\n",
    "        return result.return_values['output']\n",
    "    else:\n",
    "        tools = {\n",
    "            search_wikipedia.name : search_wikipedia,\n",
    "            get_current_temperature.name : get_current_temperature,\n",
    "        }\n",
    "        return tools[result.tool].run(result.tool_input)\n",
    "\n",
    "chain_with_router = prompt | model | OpenAIFunctionsAgentOutputParser() | route\n",
    "\n",
    "print(chain_with_router.invoke({\"input\": \"What is the weather in san francisco right now?\"}))\n",
    "print(chain_with_router.invoke({\"input\": \"What is langchain?\"}))\n",
    "print(chain_with_router.invoke({\"input\": \"hi!\"}))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ee9ae9",
   "metadata": {},
   "source": [
    "- `OpenAIFunctionsAgentOutputParser`\n",
    "  - parse return message from OpenAI into `AgentActionMessageLog` or `AgentFinish`\n",
    "  - `AgentActionMessageLog` can be routed to desired tool"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58b19ab",
   "metadata": {},
   "source": [
    "# Direct interaction with OpenAPI\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6dc3465",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "{\n",
    "  \"openapi\": \"3.1.0\",\n",
    "  \"jsonSchemaDialect\": \"https://spec.openapis.org/oas/3.1/dialect/base\",\n",
    "  \"info\": {\n",
    "    \"version\": \"1.0.0\",\n",
    "    \"title\": \"Swagger Petstore\",\n",
    "    \"license\": {\n",
    "      \"name\": \"MIT\"\n",
    "    }\n",
    "  },\n",
    "  \"servers\": [\n",
    "    {\n",
    "      \"url\": \"http://petstore.swagger.io/v1\"\n",
    "    }\n",
    "  ],\n",
    "  \"paths\": {\n",
    "    \"/pets\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"List all pets\",\n",
    "        \"operationId\": \"listPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"limit\",\n",
    "            \"in\": \"query\",\n",
    "            \"description\": \"How many items to return at one time (max 100)\",\n",
    "            \"required\": false,\n",
    "            \"schema\": {\n",
    "              \"type\": \"integer\",\n",
    "              \"maximum\": 100,\n",
    "              \"format\": \"int32\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"A paged array of pets\",\n",
    "            \"headers\": {\n",
    "              \"x-next\": {\n",
    "                \"description\": \"A link to the next page of responses\",\n",
    "                \"schema\": {\n",
    "                  \"type\": \"string\"\n",
    "                }\n",
    "              }\n",
    "            },\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pets\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"Unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"post\": {\n",
    "        \"summary\": \"Create a pet\",\n",
    "        \"operationId\": \"createPets\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"201\": {\n",
    "            \"description\": \"Created\"\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"Unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    },\n",
    "    \"/pets/{petId}\": {\n",
    "      \"get\": {\n",
    "        \"summary\": \"Info for a specific pet\",\n",
    "        \"operationId\": \"showPetById\",\n",
    "        \"tags\": [\n",
    "          \"pets\"\n",
    "        ],\n",
    "        \"parameters\": [\n",
    "          {\n",
    "            \"name\": \"petId\",\n",
    "            \"in\": \"path\",\n",
    "            \"required\": true,\n",
    "            \"description\": \"The id of the pet to retrieve\",\n",
    "            \"schema\": {\n",
    "              \"type\": \"string\"\n",
    "            }\n",
    "          }\n",
    "        ],\n",
    "        \"responses\": {\n",
    "          \"200\": {\n",
    "            \"description\": \"Expected response to a valid request\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Pet\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          },\n",
    "          \"default\": {\n",
    "            \"description\": \"Unexpected error\",\n",
    "            \"content\": {\n",
    "              \"application/json\": {\n",
    "                \"schema\": {\n",
    "                  \"$ref\": \"#/components/schemas/Error\"\n",
    "                }\n",
    "              }\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  },\n",
    "  \"components\": {\n",
    "    \"schemas\": {\n",
    "      \"Pet\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"id\",\n",
    "          \"name\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"id\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int64\"\n",
    "          },\n",
    "          \"name\": {\n",
    "            \"type\": \"string\"\n",
    "          },\n",
    "          \"tag\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        },\n",
    "        \"additionalProperties\": false\n",
    "      },\n",
    "      \"Pets\": {\n",
    "        \"type\": \"array\",\n",
    "        \"maxItems\": 100,\n",
    "        \"items\": {\n",
    "          \"$ref\": \"#/components/schemas/Pet\"\n",
    "        }\n",
    "      },\n",
    "      \"Error\": {\n",
    "        \"type\": \"object\",\n",
    "        \"required\": [\n",
    "          \"code\",\n",
    "          \"message\"\n",
    "        ],\n",
    "        \"properties\": {\n",
    "          \"code\": {\n",
    "            \"type\": \"integer\",\n",
    "            \"format\": \"int32\"\n",
    "          },\n",
    "          \"message\": {\n",
    "            \"type\": \"string\"\n",
    "          }\n",
    "        },\n",
    "        \"additionalProperties\": false\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "from langchain.chains.openai_functions.openapi import openapi_spec_to_openai_fn\n",
    "from langchain.utilities.openapi import OpenAPISpec\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "spec = OpenAPISpec.from_text(text)\n",
    "pet_openai_functions, pet_callables = openapi_spec_to_openai_fn(spec)\n",
    "print(\"functions:\", pet_openai_functions)\n",
    "print(\"callables:\", pet_callables)\n",
    "\n",
    "model = ChatOpenAI(temperature=0).bind(functions=pet_openai_functions)\n",
    "print(model.invoke(\"what are three pets names\"))\n",
    "print(model.invoke(\"tell me about pet with id 42\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57be1172",
   "metadata": {},
   "source": [
    "- OpenAPISpec has an issue, and following change needs to be made to make it work\n",
    "    ```python\n",
    "    # openapi.py\n",
    "    @classmethod\n",
    "    def parse_obj(cls, obj: dict) -> OpenAPISpec:\n",
    "        try:\n",
    "            cls._alert_unsupported_spec(obj)\n",
    "            return super().model_validate(obj)\n",
    "        except ValidationError as e:\n",
    "            # We are handling possibly misconfigured specs and\n",
    "            # want to do a best-effort job to get a reasonable interface out of it.\n",
    "            new_obj = copy.deepcopy(obj)\n",
    "            for error in e.errors():\n",
    "                keys = error[\"loc\"]\n",
    "                item = new_obj\n",
    "                for key in keys[:-1]:\n",
    "                    item = item[key]\n",
    "                item.pop(keys[-1], None)\n",
    "            return cls.parse_obj(new_obj)\n",
    "    ```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685a1f8c",
   "metadata": {},
   "source": [
    "# Conversational Agent\n",
    "\n",
    "Agent is a combination of LLMs and code. LLMs reason about what steps to tak and call for actions.\n",
    "- Agent loop\n",
    "  - Choose a tool to use\n",
    "  - Observe the output of the tool\n",
    "  - Repeat until a stopping condition is met\n",
    "- Stopping condition can be\n",
    "  - LLM determined\n",
    "  - Hardcoded rules\n",
    "\n",
    "**defining tools**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a22b94b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import datetime\n",
    "import wikipedia\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain.tools import tool\n",
    "\n",
    "# Define the input schema\n",
    "class OpenMeteoInput(BaseModel):\n",
    "    latitude: float = Field(description=\"Latitude of the location to fetch weather data for\")\n",
    "    longitude: float = Field(description=\"Longitude of the location to fetch weather data for\")\n",
    "\n",
    "@tool(args_schema=OpenMeteoInput)\n",
    "def get_current_temperature(latitude: float, longitude: float) -> dict:\n",
    "    \"\"\"Fetch current temperature for given coordinates.\"\"\"\n",
    "    \n",
    "    BASE_URL = \"https://api.open-meteo.com/v1/forecast\"\n",
    "    params = {\n",
    "        'latitude': latitude,\n",
    "        'longitude': longitude,\n",
    "        'hourly': 'temperature_2m',\n",
    "        'forecast_days': 1,\n",
    "    }\n",
    "    # Make the request\n",
    "    response = requests.get(BASE_URL, params=params)\n",
    "    \n",
    "    if response.status_code == 200:\n",
    "        results = response.json()\n",
    "    else:\n",
    "        raise Exception(f\"API Request failed with status code: {response.status_code}\")\n",
    "\n",
    "    current_utc_time = datetime.datetime.now(datetime.timezone.utc)\n",
    "    time_list = [datetime.datetime.fromisoformat(time_str).replace(tzinfo=datetime.timezone.utc) for time_str in results['hourly']['time']]\n",
    "    temperature_list = results['hourly']['temperature_2m']\n",
    "    \n",
    "    closest_time_index = min(range(len(time_list)), key=lambda i: abs(time_list[i] - current_utc_time))\n",
    "    current_temperature = temperature_list[closest_time_index]\n",
    "    \n",
    "    return f'The current temperature is {current_temperature}°C'\n",
    "\n",
    "@tool\n",
    "def search_wikipedia(query: str) -> str:\n",
    "    \"\"\"Run Wikipedia search and get page summaries.\"\"\"\n",
    "    page_titles = wikipedia.search(query)\n",
    "    summaries = []\n",
    "    for page_title in page_titles[: 3]:\n",
    "        try:\n",
    "            wiki_page =  wikipedia.page(title=page_title, auto_suggest=False)\n",
    "            summaries.append(f\"Page: {page_title}\\nSummary: {wiki_page.summary}\")\n",
    "        except (\n",
    "            wikipedia.exceptions.PageError,\n",
    "            wikipedia.exceptions.DisambiguationError,\n",
    "        ):\n",
    "            pass\n",
    "    if not summaries:\n",
    "        return \"No good Wikipedia Search Result was found\"\n",
    "    return \"\\n\\n\".join(summaries)\n",
    "\n",
    "tools = [get_current_temperature, search_wikipedia]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f625d8af",
   "metadata": {},
   "source": [
    "**chain of function and tool example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20330ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.tools.render import format_tool_to_openai_function\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents.format_scratchpad import format_to_openai_functions\n",
    "from langchain_core.agents import AgentAction, AgentActionMessageLog\n",
    "\n",
    "functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "model = ChatOpenAI(temperature=0).bind(functions=functions)\n",
    "# chain basic\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result = chain.invoke({\"input\": \"what is the weather is sf?\"})\n",
    "print(\"result:\", result)\n",
    "if  isinstance(result, AgentActionMessageLog):\n",
    "    print(\"tool  :\", result.tool)\n",
    "    print(\"toolin:\", result.tool_input)\n",
    "\n",
    "# chain1\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "chain = prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "result1 = chain.invoke({\n",
    "    \"input\": \"what is the weather is sf?\",\n",
    "    \"agent_scratchpad\": []\n",
    "})\n",
    "\n",
    "print(\"result1:\", result1)\n",
    "print(\"type.  :\", type(result1))\n",
    "if  isinstance(result1, AgentActionMessageLog):\n",
    "    print(\"tool   :\", result1.tool)\n",
    "    print(\"msg log:\", result1.message_log)\n",
    "\n",
    "    observation = get_current_temperature.invoke(result1.tool_input)\n",
    "    openai_fn_format = format_to_openai_functions([(result1, observation), ])\n",
    "    result2 = chain.invoke({\n",
    "        \"input\": \"what is the weather is sf?\", \n",
    "        \"agent_scratchpad\": openai_fn_format,\n",
    "    })\n",
    "    print(\"observ :\", observation)\n",
    "    print(\"openfn :\", openai_fn_format)\n",
    "    print(\"result2:\", result2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff6e2a6",
   "metadata": {},
   "source": [
    "**agent chain example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180b3646",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.agent import AgentFinish\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents import AgentExecutor\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "\n",
    "def run_agent(user_input):\n",
    "    intermediate_steps = []\n",
    "    while True:\n",
    "        result = agent_chain.invoke({\n",
    "            \"input\": user_input,                     # initial user input\n",
    "            \"intermediate_steps\": intermediate_steps # keep growing with observation\n",
    "        })\n",
    "        if isinstance(result, AgentFinish):\n",
    "            return result\n",
    "        tool = {\n",
    "            \"search_wikipedia\": search_wikipedia, \n",
    "            \"get_current_temperature\": get_current_temperature,\n",
    "        }[result.tool]\n",
    "        observation = tool.run(result.tool_input)\n",
    "        intermediate_steps.append((result, observation))\n",
    "\n",
    "print(run_agent(\"what is the weather is sf?\"))\n",
    "print(run_agent(\"what is langchain?\"))\n",
    "print(run_agent(\"hi!\"))\n",
    "\n",
    "# with wrapper AgentExecutor\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True)\n",
    "agent_executor.invoke({\"input\": \"what is langchain?\"})\n",
    "agent_executor.invoke({\"input\": \"my name is bob\"})\n",
    "agent_executor.invoke({\"input\": \"what is my name\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f66b88c9",
   "metadata": {},
   "source": [
    "- `RunnablePassthrough`\n",
    "    - Runnable to passthrough inputs unchanged or with additional keys.\n",
    "    - `assign`: unchainged input with additional key-val pair from assigned by `assign`\n",
    "    ```python\n",
    "    runnable = {\n",
    "      'llm1':  lambda: \"llm1_fin\",\n",
    "      'llm2':  lambda: \"llm2_fin\",\n",
    "      } | RunnablePassthrough.assign(\n",
    "          total_chars=lambda inputs: len(inputs['llm1'] + inputs['llm2'])\n",
    "      )\n",
    "      runnable.invoke('hello')\n",
    "      # {'llm1': 'llm1_fin', 'llm2': 'llm2_fin', 'total_chars': 16}\n",
    "    ```\n",
    "- `format_to_openai_functions`\n",
    "  - Convert `(AgentAction, tool output)` tuples into `FunctionMessages`.\n",
    "  - when input is empty list `[]`, return `[]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dd04af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.agents.output_parsers import OpenAIFunctionsAgentOutputParser\n",
    "from langchain.agents import AgentExecutor\n",
    "from langchain.prompts import MessagesPlaceholder\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "# With memory\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are helpful but sassy assistant\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "])\n",
    "agent_chain = RunnablePassthrough.assign(\n",
    "    agent_scratchpad= lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    ") | prompt | model | OpenAIFunctionsAgentOutputParser()\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "agent_executor = AgentExecutor(agent=agent_chain, tools=tools, verbose=True, memory=memory)\n",
    "\n",
    "agent_executor.invoke({\"input\": \"my name is bob\"})\n",
    "agent_executor.invoke({\"input\": \"whats my name\"})\n",
    "agent_executor.invoke({\"input\": \"whats the weather in sf?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf6c3d3",
   "metadata": {},
   "source": [
    "# Custom Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8993343f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def create_your_own(query: str) -> str:\n",
    "    \"\"\"This function can do whatever you would like once you fill it in \"\"\"\n",
    "    print(type(query))\n",
    "    return query[::-1]\n",
    "\n",
    "tools = [get_current_temperature, search_wikipedia, create_your_own]\n",
    "\n",
    "import panel as pn  # GUI\n",
    "pn.extension()\n",
    "import panel as pn\n",
    "import param\n",
    "\n",
    "class cbfs(param.Parameterized):\n",
    "    \n",
    "    def __init__(self, tools, **params):\n",
    "        super(cbfs, self).__init__( **params)\n",
    "        self.panels = []\n",
    "        self.functions = [format_tool_to_openai_function(f) for f in tools]\n",
    "        self.model = ChatOpenAI(temperature=0).bind(functions=self.functions)\n",
    "        self.memory = ConversationBufferMemory(return_messages=True,memory_key=\"chat_history\")\n",
    "        self.prompt = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are helpful but sassy assistant\"),\n",
    "            MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "            (\"user\", \"{input}\"),\n",
    "            MessagesPlaceholder(variable_name=\"agent_scratchpad\")\n",
    "        ])\n",
    "        self.chain = RunnablePassthrough.assign(\n",
    "            agent_scratchpad = lambda x: format_to_openai_functions(x[\"intermediate_steps\"])\n",
    "        ) | self.prompt | self.model | OpenAIFunctionsAgentOutputParser()\n",
    "        self.qa = AgentExecutor(agent=self.chain, tools=tools, verbose=False, memory=self.memory)\n",
    "    \n",
    "    def convchain(self, query):\n",
    "        if not query:\n",
    "            return\n",
    "        inp.value = ''\n",
    "        result = self.qa.invoke({\"input\": query})\n",
    "        self.answer = result['output'] \n",
    "        self.panels.extend([\n",
    "            pn.Row('User:', pn.pane.Markdown(query, width=450)),\n",
    "            pn.Row('ChatBot:', pn.pane.Markdown(self.answer, width=450, styles={'background-color': '#F6F6F6'}))\n",
    "        ])\n",
    "        return pn.WidgetBox(*self.panels, scroll=True)\n",
    "\n",
    "\n",
    "    def clr_history(self,count=0):\n",
    "        self.chat_history = []\n",
    "        return\n",
    "\n",
    "# Chatbot\n",
    "cb = cbfs(tools)\n",
    "\n",
    "inp = pn.widgets.TextInput( placeholder='Enter text here…')\n",
    "\n",
    "conversation = pn.bind(cb.convchain, inp) \n",
    "\n",
    "tab1 = pn.Column(\n",
    "    pn.Row(inp),\n",
    "    pn.layout.Divider(),\n",
    "    pn.panel(conversation,  loading_indicator=True, height=400),\n",
    "    pn.layout.Divider(),\n",
    ")\n",
    "\n",
    "dashboard = pn.Column(\n",
    "    pn.Row(pn.pane.Markdown('# QnA_Bot')),\n",
    "    pn.Tabs(('Conversation', tab1))\n",
    ")\n",
    "dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a725dbaf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
