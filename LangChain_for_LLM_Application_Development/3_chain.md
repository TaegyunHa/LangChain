# Chain

> https://python.langchain.com/api_reference/langchain/chains/langchain.chains.llm.LLMChain.html

LLMChain simply combines a prompt with a chat model in a sequential manner.

# LangChain Objects

- `langchain.chains.LLMChain`
    - Deprecated since version `0.1.17`: Use `prompt | llm`() instead.
    - It will not be removed until `langchain==1.0`.
    - This is a component of Chain object (i.e., `SequentialChain`)
- `langchain.chains.SequentialChain`
- `langchain.chains.router.MultiPromptChain`
- `langchain.chains.router.llm_router.LLMRouterChain.RouterOutputParser`
- `langchain.chains.router.multi_prompt.MULTI_PROMPT_ROUTER_TEMPLATE`

**Basic Example**

```python
from langchain.chat_models import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
from langchain.chains import LLMChain

llm = ChatOpenAI(temperature=0.9, model=llm_model)
prompt = ChatPromptTemplate.from_template(
    "What is the best name to describe \
    a company that makes {product}?"
)
chain = LLMChain(llm=llm, prompt=prompt)

product = "Random Size Sheet Set"
chain.run(product)
```

# Sequential Chain

Sequential chain combines multiple chains sequentially where the output of one chain is the input of the next chain.
There are two types of sequential chains:
- SimpleSequentialChain
    - Single input/output
- SequentialChain
    - Multiple input/output

## SimpleSequentialChain

> https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sequential.SimpleSequentialChain.html#simplesequentialchain

SimpleSequentialChain works well when there is a single input and single output.

**SimpleSequentialChain example**
```python
from langchain.chains import SimpleSequentialChain

llm = ChatOpenAI(temperature=0.9, model=llm_model)

# ---------------------------------------------------------
# Chain1
first_prompt = ChatPromptTemplate.from_template(
    "What is the best name to describe \
    a company that makes {product}?")
chain_one = LLMChain(llm=llm, prompt=first_prompt)

# ---------------------------------------------------------
# Chain2
second_prompt = ChatPromptTemplate.from_template(
    "Write a 20 words description for the following \
    company: {company_name}")
chain_two = LLMChain(llm=llm, prompt=second_prompt)

# Simple Chain
overall_simple_chain = SimpleSequentialChain(
    chains=[chain_one, chain_two],
    verbose=True)

overall_simple_chain.run(product)
```

**langchain.chains.SimpleSequentialChain**
- `inputs: dict[str, Any] | Any`
    - Dictionary of inputs. `dict` should contain all inputs specified in `Chain.input_keys` except for inputs that will be set by the chain's memory.
    - Single input if chain expects only one param.
- `return_only_outputs: bool`
    - Whether to return only outputs in the response.
        - If `True`, only new keys generated by this chain will be returned.
        - If `False`, both input keys and new keys generated by this chain will be returned.
        - Defaults to `False`.
- Return: `dict[str, Any]`
    - A dict of named outputs. Should contain all outputs specified in `Chain.output_keys`.

## SequentialChain

> https://python.langchain.com/api_reference/langchain/chains/langchain.chains.sequential.SequentialChain.html

`SequentialChain` handles multiple inputs and multiple outputs.
Input keys and output keys should be precise to make it work.

When input-output keys are not aligned, you get `ValidationError`. I think it's raised before the LLM is run.

```
ValidationError: 1 validation error for SequentialChain
__root__
  Missing required input keys: {'summary'}, only had {'language', 'summary2', 'Review', 'English_Review'} (type=value_error)
```

**SequentialChain example**
```python
from langchain.chains import SequentialChain

llm = ChatOpenAI(temperature=0.9, model=llm_model)

# ---------------------------------------------------------
# chain 1: translate to english
# input= Review, output= English_Review
first_prompt = ChatPromptTemplate.from_template(
    "Translate the following review to english:"
    "\n\n{Review}")
chain_one = LLMChain(llm=llm, prompt=first_prompt, 
                     output_key="English_Review")

# ---------------------------------------------------------
# chain 2: summarize english review
# input= English_Review, output= summary
second_prompt = ChatPromptTemplate.from_template(
    "Can you summarize the following review in 1 sentence:"
    "\n\n{English_Review}")
chain_two = LLMChain(llm=llm, prompt=second_prompt, 
                     output_key="summary")

# ---------------------------------------------------------
# chain 3: identify the language of review
# input= Review and output= language
third_prompt = ChatPromptTemplate.from_template(
    "What language is the following review:\n\n{Review}")
chain_three = LLMChain(llm=llm, prompt=third_prompt,
                       output_key="language")

# ---------------------------------------------------------
# chain 4: write follow up response
# input= summary, language and output= followup_message
fourth_prompt = ChatPromptTemplate.from_template(
    "Write a follow up response to the following "
    "summary in the specified language:"
    "\n\nSummary: {summary}\n\nLanguage: {language}")
chain_four = LLMChain(llm=llm, prompt=fourth_prompt,
                      output_key="followup_message")

# ---------------------------------------------------------
# overall_chain: input= Review 
# and output= English_Review, summary, followup_message
overall_chain = SequentialChain(
    chains=[chain_one, chain_two, chain_three, chain_four],
    input_variables=["Review"],
    output_variables=["English_Review", "summary", "followup_message"],
    verbose=True)

review = df.Review[5]
overall_chain(review)
```

- Why no `overall_chain.run`?

**langchain.chains.SequentialChain**

- `inputs: dict[str, Any] | Any`
    - Dictionary of inputs. `dict` should contain all inputs specified in `Chain.input_keys` except for inputs that will be set by the chain's memory.
    - Single input if chain expects only one param

# LLMRouterChain

The `LLMRouterChain` routes an input query to one of multiple destinations.
Given an input query, it uses an LLM to select from a list of destination chains, and passes its inputs to the selected chain.

`LLMRouterChain` routes a query by instructing the LLM to generate JSON-formatted text, and parsing out the intended destination.

**RouterChain example**

```python
from langchain.chains.router import MultiPromptChain
from langchain.chains.router.llm_router import LLMRouterChain, RouterOutputParser
from langchain.prompts import PromptTemplate
from langchain.chains.router.multi_prompt import MULTI_PROMPT_ROUTER_TEMPLATE

llm = ChatOpenAI(temperature=0, model=llm_model)

math_template = """You are a very good mathematician. \
You are great at answering math questions. \
You are so good because you are able to break down \
hard problems into their component parts, 
answer the component parts, and then put them together\
to answer the broader question.

Here is a question:
{input}"""

computerscience_template = """ You are a successful computer scientist.\
You have a passion for creativity, collaboration,\
forward-thinking, confidence, strong problem-solving capabilities,\
understanding of theories and algorithms, and excellent communication \
skills. You are great at answering coding questions. \
You are so good because you know how to solve a problem by \
describing the solution in imperative steps \
that a machine can easily interpret and you know how to \
choose a solution that has a good balance between \
time complexity and space complexity. 

Here is a question:
{input}"""

prompt_infos = [
    {
        "name": "math", 
        "description": "Good for answering math questions", 
        "prompt_template": math_template
    },
    {
        "name": "computer science", 
        "description": "Good for answering computer science questions", 
        "prompt_template": computerscience_template
    }
]

# name - prompt_template pair
destination_chains = {}
for p_info in prompt_infos:
    prompt = ChatPromptTemplate.from_template(template=p_info["prompt_template"])
    destination_chains[p_info["name"]] = LLMChain(llm=llm, prompt=prompt)
# name - description pair
destinations = [f"{p['name']}: {p['description']}" for p in prompt_infos]
destinations_str = "\n".join(destinations)

# default chain in case of failing routing
default_prompt = ChatPromptTemplate.from_template("{input}")
default_chain = LLMChain(llm=llm, prompt=default_prompt)

router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(
    destinations=destinations_str
)
router_prompt = PromptTemplate(
    template=router_template,
    input_variables=["input"],
    output_parser=RouterOutputParser(),
)
router_chain = LLMRouterChain.from_llm(llm, router_prompt)

chain = MultiPromptChain(router_chain=router_chain, 
                         destination_chains=destination_chains, 
                         default_chain=default_chain,
                         verbose=True)
chain.run("What is python?")
```

- prompt includes parser

**MULTI_PROMPT_ROUTER_TEMPLATE**
```
Given a raw text input to a \
language model select the model prompt best suited for the input. \
You will be given the names of the available prompts and a \
description of what the prompt is best suited for. \
You may also revise the original input if you think that revising\
it will ultimately lead to a better response from the language model.

<< FORMATTING >>
Return a markdown code snippet with a JSON object formatted to look like:
```json
{{{{
    "destination": string \ "DEFAULT" or name of the prompt to use in {destinations}
    "next_inputs": string \ a potentially modified version of the original input
}}}}
```

REMEMBER: The value of “destination” MUST match one of \
the candidate prompts listed below.\
If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”
REMEMBER: "next_inputs" can just be the original input \
if you don't think any modifications are needed.

<< CANDIDATE PROMPTS >>
{destinations}

<< INPUT >>
{{input}}

<< OUTPUT (remember to include the ```json)>>
```