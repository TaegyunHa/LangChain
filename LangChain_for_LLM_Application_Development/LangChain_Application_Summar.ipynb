{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cf3e03a5",
   "metadata": {},
   "source": [
    "# Model, Prompt and Parser\n",
    "\n",
    "## Topics\n",
    "\n",
    "- Models\n",
    "\t- LLMs\n",
    "\t- Chat Models\n",
    "\t- Text Embedding Models\n",
    "- Prompts\n",
    "\t- Prompt Templates\n",
    "\t- Output Parsers\n",
    "\t\t- Retry/fixing\n",
    "\t- Example Selectors\n",
    "- Indexes\n",
    "\t- Document Loaders\n",
    "\t- Text Splitters\n",
    "\t- Vector Stores\n",
    "\t- Retrievers\n",
    "- Chains\n",
    "\t- Prompt + LLM + Output Parsing\n",
    "- Agents\n",
    "\t- Algorithms for getting LLMs to use tools\n",
    "\n",
    "## LangChain Objects\n",
    "\n",
    "- `langchain.chat_models.ChatOpenAI`\n",
    "- `langchain.prompts.ChatPromptTemplate`\n",
    "- `langchain.output_parsers.ResponseSchema`\n",
    "- `langchain.output_parsers.StructuredOutputParser`\n",
    "\n",
    "## Model\n",
    "\n",
    "### OpenAI API\n",
    "\n",
    "> https://github.com/openai/openai-python\n",
    "\n",
    "OpenAI provide a pythonAPI to interact with their LLM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a065b8c2",
   "metadata": {},
   "source": [
    "**Setup OpenAI API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fcb595d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI and API key\n",
    "import openai\n",
    "\n",
    "def setup_openai_api_key():\n",
    "    from dotenv import load_dotenv\n",
    "    _ = load_dotenv()\n",
    "setup_openai_api_key()\n",
    "\n",
    "LLM_MODEL:str = \"gpt-4o-mini\"\n",
    "# LLM_MODEL:str = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f872ebc",
   "metadata": {},
   "source": [
    "**OpenAI responses API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d4308d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_response(message:str, instruction:str, model:str=LLM_MODEL):\n",
    "    response = openai.OpenAI().responses.create(\n",
    "        model=model,\n",
    "        instructions=instruction,\n",
    "        input=message,\n",
    "    )\n",
    "    return response.output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a58fb8",
   "metadata": {},
   "source": [
    "**OpenAI chat completions API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7313d02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_openai_chat_completion(prompt:str, model=LLM_MODEL):\n",
    "    completion = openai.OpenAI().chat.completions.create(\n",
    "        model=model,\n",
    "        messages=[{\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "        }],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a18f0d",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Prompt is a template that structures input to language models. It defines how to format user input, system instructions, and context before sending to the LLM.\n",
    "\n",
    "Prompts ensure consistent, structured communication between your application and the LLM.\n",
    "\n",
    "**Key components:**\n",
    "- `ChatPromptTemplate`:  formats messages for chat models\n",
    "- `SystemMessagePrompt`: sets AI behavior/role\n",
    "- `HumanMessagePrompt`: user input template\n",
    "- `Variables`: placeholders filled at runtime (e.g., {user_input}, {context})\n",
    "\n",
    "\n",
    "**Create prompt from template:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[template] --> C[Prompt]\n",
    "```\n",
    "\n",
    "**Use prompt to get response:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[style] --> C[Prompt]\n",
    "    B[message] --> C\n",
    "\n",
    "    C --> D[LLM]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c164a7d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_email = \"\"\"\\\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\\\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"\\\n",
    "British English \\\n",
    "in a calm and respenctful tone\\\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"\\\n",
    "Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5749cf10",
   "metadata": {},
   "source": [
    "- `\\` at the end of each line of the prompt template:\n",
    "  - `\\` is a line continuation character in Python. It removes `\\n` and makes a string into one line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7d68a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I must say, I am quite upset that my blender lid flew off and splattered my kitchen walls with smoothie! To add to the frustration, the warranty does not cover the cost of cleaning up my kitchen. I would greatly appreciate your assistance at this moment, my friend.\n"
     ]
    }
   ],
   "source": [
    "response = get_openai_chat_completion(prompt=prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0313cd",
   "metadata": {},
   "source": [
    "# LangChain \n",
    "\n",
    "### ChatOpenAI\n",
    "\n",
    "The `ChatOpenAI` helps make prompts reproducible in a controlled manner when used with `ChatPromptTemplate`. It can be thought of as a model adapter to use with `ChatPromptTemplate`.\n",
    "- **Input**: list of messages\n",
    "- **Output**: a message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c16728a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "chat = ChatOpenAI(\n",
    "    model=LLM_MODEL,\n",
    "    temperature=0.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347ce5ad",
   "metadata": {},
   "source": [
    "- temperature:\n",
    "  - Randomness and creativity of generated text can be controlled by temperature.\n",
    "  - A higher value (e.g., 1.0) makes responses more creative\n",
    "  - A lower value (e.g., 0.0) makes them more deterministic and focused"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2056aee9",
   "metadata": {},
   "source": [
    "### ChatPromptTemplate\n",
    "\n",
    "> https://python.langchain.com/api_reference/core/prompts/langchain_core.prompts.chat.ChatPromptTemplate.html\n",
    "\n",
    "Translates user input and parameters into instructions for a model.\n",
    "`ChatPromptTemplate` will be used to format a list of messages.\n",
    "\n",
    "- **Input**: `dict`\n",
    "\t- **key**: a variable in the prompt template to fill in\n",
    "\t- **value**: a value replacing a variable in the prompt\n",
    "- **Output**: `PromptValue`\n",
    "\t- can be passed into `ChatModel`\n",
    "\t- can be cast to a string or list of messages\n",
    "\t- `PromptValue` allows switching between string and message formats\n",
    "\n",
    "**Prompt template 1:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aa2b2926",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d21542e",
   "metadata": {},
   "source": [
    "**Prompt template 2:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b88c7d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w7/8h107ckj52d2fz6fb8wfmfjm0000gn/T/ipykernel_39766/3791515173.py:24: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  customer_response = chat(customer_messages)\n"
     ]
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "template_string = \"\"\"\\\n",
    "Translate the text that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "# Make prompt template\n",
    "prompt_template:ChatPromptTemplate = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "# Create a prompt from template\n",
    "customer_style = \"British English\"\n",
    "customer_email = \"Ah, I be fuming that my blender lid flew off!\"\n",
    "customer_messages = prompt_template.format_messages(\n",
    "\tstyle=customer_style,\n",
    "\ttext=customer_email)\n",
    "type(customer_messages[0]) # <class 'langchain.schema.HumanMessage'>\n",
    "\n",
    "# Get response from the chat model\n",
    "chat = ChatOpenAI(model=LLM_MODEL, temperature=0.0)\n",
    "customer_response = chat(customer_messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5be9f936",
   "metadata": {},
   "source": [
    "- Access prompt:\n",
    "  - `prompt_template.messages[0].prompt`\n",
    "- Access prompt input vars:\n",
    "  - `prompt_template.messages[0].prompt.input_variables`\n",
    "\n",
    "\n",
    "### Overview\n",
    "\n",
    "**Create prompt from template:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[template] --> C[ChatPromptTemplate]\n",
    "\n",
    "    style C fill:green, color:#000\n",
    "```\n",
    "\n",
    "**Use prompt to get response:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[style] --> C[ChatPromptTemplate]\n",
    "    B[message] --> C\n",
    "\n",
    "    C --> D[Messages]\n",
    "    D --> E[ChatOpenAI]\n",
    "\n",
    "    style C fill:green, color:#000\n",
    "```\n",
    "\n",
    "## StructuredOutputParser\n",
    "\n",
    "It's possible to get JSON-formatted strings from LLM by adding format instructions in the prompt. However, the response is always `str` not `dict`; therefore, additional parsing is needed.\n",
    "\n",
    "`StructuredOutputParser` parses the `str` response into desired format.\n",
    "\n",
    "With output parser:\n",
    "- format instructions can be created\n",
    "- output responses can be parsed into `dict`\n",
    "\n",
    "### ResponseSchema\n",
    "\n",
    "`ResponseSchema` is a building block of `StructuredOutputParser` that defines format instructions and the structure of output `dict`.\n",
    "\n",
    "Output format of `StructuredOutputParser.get_format_instructions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b408db64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "The output should be a markdown code snippet formatted in the following schema, including the leading and trailing \"```json\" and \"```\":\n",
    "\n",
    "```json\n",
    "{\n",
    "\t\"gift\": string  // Was the item purchased as a gift for someone else?                              Answer True if yes, False if not or unknown.\n",
    "\t\"price_value\": string  // Extract any sentences about the value or                                     price, and output them as a comma separated Python list.\n",
    "}\n",
    "```\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b47ff5bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema\n",
    "from langchain.output_parsers import StructuredOutputParser\n",
    "\n",
    "gift_schema = ResponseSchema(name=\"gift\",\n",
    "                             description=\"Was the item purchased as a gift for someone else? \\\n",
    "                             Answer True if yes, False if not or unknown.\")\n",
    "price_value_schema = ResponseSchema(name=\"price_value\",\n",
    "                                    description=\"Extract any sentences about the value or \\\n",
    "                                    price, and output them as a comma separated Python list.\")\n",
    "response_schemas:list[ResponseSchema] = [gift_schema,\n",
    "                                         price_value_schema]\n",
    "\n",
    "output_parser:StructuredOutputParser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "format_instruction = output_parser.get_format_instructions()\n",
    "\n",
    "\n",
    "customer_review = \"\"\"\\\n",
    "This leaf blower is pretty amazing.  It has four settings:\\\n",
    "candle blower, gentle breeze, windy city, and tornado. \\\n",
    "It arrived in two days, just in time for my wife's \\\n",
    "anniversary present. \\\n",
    "I think my wife liked it so much she was speechless. \\\n",
    "So far I've been the only one using it, and I've been \\\n",
    "using it every other morning to clear the leaves on our lawn. \\\n",
    "It's slightly more expensive than the other leaf blowers \\\n",
    "out there, but I think it's worth it for the extra features.\n",
    "\"\"\"\n",
    "\n",
    "review_template = \"\"\"\\\n",
    "For the following text, extract the following information:\n",
    "\n",
    "gift: Was the item purchased as a gift for someone else? \\\n",
    "Answer True if yes, False if not or unknown.\n",
    "\n",
    "price_value: Extract any sentences about the value or price,\\\n",
    "and output them as a comma separated Python list.\n",
    "\n",
    "text: {text}\n",
    "\n",
    "{format_instructions}\n",
    "\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template=review_template)\n",
    "messages = prompt.format_messages(text=customer_review, \n",
    "                                format_instructions=format_instruction)\n",
    "response = chat(messages)\n",
    "output_dict = output_parser.parse(response.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0df6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'gift': 'True', 'price_value': [\"It's slightly more expensive than the other leaf blowers out there, but I think it's worth it for the extra features.\"]}\n"
     ]
    }
   ],
   "source": [
    "print(output_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe9a668",
   "metadata": {},
   "source": [
    "### Overview\n",
    "\n",
    "**Create StructuredOutputParser from list of ResponseSchema:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[ResponseSchema] --> D[List]\n",
    "    B[ResponseSchema] --> D\n",
    "    C[ResponseSchema] --> D\n",
    "\n",
    "    D --> E[StructuredOutputParser]\n",
    "    style E fill:green, color:#000\n",
    "```\n",
    "\n",
    "**Use StructuredOutputParser to get output dict**\n",
    "\n",
    "```mermaid\n",
    "graph TD\n",
    "    A[StructuredOutputParser]:::parser --> B[format_instruction]\n",
    "    B --> E[ChatPromptTemplate]\n",
    "    C[style] --> E\n",
    "    D[message] --> E\n",
    "    \n",
    "    E --> F[BaseMessage]\n",
    "    F --> G[ChatOpenAI]\n",
    "    G --> H[response]\n",
    "    H --> I[StructuredOutputParser]:::parser\n",
    "    I --> J[output_dict]\n",
    "\n",
    "    classDef parser fill:green, color:#000\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71055c4f",
   "metadata": {},
   "source": [
    "## Memory\n",
    "\n",
    "LLMs are stateless\n",
    "- Each transaction is independent\n",
    "- Memory can be achieved by providing the full conversation as context\n",
    "- If we keep sending the full conversation, it becomes large fast; hence, expensive.\n",
    "  \n",
    "Memory objects in LangChain handle the following:\n",
    "- Managing conversation history\n",
    "  - Keep only the last n turns of the conversation between the user and the AI.\n",
    "- Extraction of structured information\n",
    "  - Extract structured information from the conversation history, such as a list of facts learned about the user.\n",
    "- Composite memory implementations\n",
    "  - Combine multiple memory sources,\n",
    "  - i.e. a list of known facts about the user along with facts learned during a given conversation.\n",
    "\n",
    "We use the `ConversationChain` instead of `ChatPromptTemplate` to implement memory.\n",
    "\n",
    "### LangChain Objects\n",
    "\n",
    "Note that all of the following are deprecated in favor of LangGraph:\n",
    "- `langchain.chains.ConversationChain`\n",
    "- `langchain.memory.ConversationBufferMemory`\n",
    "- `langchain.memory.ConversationBufferWindowMemory`\n",
    "- `langchain.memory.ConversationTokenBufferMemory`\n",
    "- `langchain.memory.ConversationSummaryMemory`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83aa85dd",
   "metadata": {},
   "source": [
    "## Chain\n",
    "\n",
    "> https://python.langchain.com/docs/how_to/sequence/\n",
    "\n",
    "Any two runnable can be chained into sequence by the pipe operator `|`. In chain, the output of the previous runnable's `.invoke` call is passed as input to the next runnable.\n",
    "\n",
    "### LanghChain Objects\n",
    "\n",
    "- `langchain_core.runnables.base.RunnableSequence`\n",
    "\n",
    "**Basic Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c375b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import RunnableSequence\n",
    "\n",
    "template:str = \"\"\"\\\n",
    "What is the best name to describe \\\n",
    "a company that makes {product}?\\\n",
    "\"\"\"\n",
    "\n",
    "llm:ChatOpenAI = ChatOpenAI(temperature=0.9, model=LLM_MODEL)\n",
    "prompt:ChatPromptTemplate = ChatPromptTemplate.from_template(template=template)\n",
    "chain:RunnableSequence = prompt | llm\n",
    "\n",
    "product = \"Random Size Sheet Set\"\n",
    "\n",
    "#response = chain.invoke({\"product\":product})\n",
    "response = chain.invoke(product)\n",
    "\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e513c1aa",
   "metadata": {},
   "source": [
    "### Sequential Chain\n",
    "\n",
    "Constructed chain, `RunnableSequence`, can be combined with more runnable to create another chain. This involve input/output formatting depending on required input and output of the each chain component.\n",
    "\n",
    "**Single Input/Output Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9ffb1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='\"VariSize Linens offers customizable linens to fit any bed perfectly, providing comfort and style for a restful night\\'s sleep.\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 27, 'prompt_tokens': 244, 'total_tokens': 271, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--b364701c-22a0-4de2-85f8-253fd7e2a90f-0', usage_metadata={'input_tokens': 244, 'output_tokens': 27, 'total_tokens': 271, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables.base import RunnableSequence\n",
    "\n",
    "llm:ChatOpenAI = ChatOpenAI(temperature=0.9, model=LLM_MODEL)\n",
    "product = \"Random Size Sheet Set\"\n",
    "\n",
    "# Chain1 --------------------------------------------------\n",
    "first_template = \"\\\n",
    "What is the best name to describe \\\n",
    "a company that makes {product}?\"\n",
    "first_prompt = ChatPromptTemplate.from_template(first_template)\n",
    "chain_one:RunnableSequence = first_prompt | llm\n",
    "\n",
    "# Chain2 --------------------------------------------------\n",
    "second_template = \"\\\n",
    "Write a 20 words description for the following \\\n",
    "company: {company_name}\"\n",
    "second_prompt = ChatPromptTemplate.from_template(second_template)\n",
    "chain_two:RunnableSequence = second_prompt | llm\n",
    "\n",
    "# Sequential Chain\n",
    "overall_simple_chain:RunnableSequence = chain_one | chain_two\n",
    "overall_simple_chain.invoke({\"product\":product})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463b2425",
   "metadata": {},
   "source": [
    "- Both `first_chain` and `second_chain` have only one variable:\n",
    "  - Input of both chain can be a `str` instead of `dict`\n",
    "  - Output of `first_chain` can be used without parser\n",
    "- How to make it verbose? Following is deprecated version:\n",
    "  ```python\n",
    "  SequentialChain(\n",
    "    chains=[chain_one, chain_two],\n",
    "    verbose=True)\n",
    "  ```\n",
    "\n",
    "## Sequential Chains with LCEL\n",
    "\n",
    "> https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.base.Runnable.html#lcel-and-composition\n",
    "\n",
    "The LangChain Expression Language (LCEL) is a declarative way to compose Runnables into chains.\n",
    "\n",
    "**Multi-Input/Output Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b76329db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'debug': {'Review': \"Je trouve le goût médiocre. La mousse ne tient pas, c'est bizarre. J'achète les mêmes dans le commerce et le goût est bien meilleur...\\nVieux lot ou contrefaçon !?\",\n",
       "  'English_Review': \"I find the taste mediocre. The foam doesn't hold, it's weird. I buy the same ones in stores and the taste is much better... Old batch or counterfeit!?\",\n",
       "  'language': 'French'},\n",
       " 'summary': 'The reviewer believes the taste of the product is subpar and suspects that it may be an old batch or counterfeit compared to the ones purchased in stores.',\n",
       " 'language': 'French'}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableParallel, RunnablePassthrough, RunnableLambda\n",
    "from langchain_core.output_parsers.string import StrOutputParser\n",
    "\n",
    "llm:ChatOpenAI = ChatOpenAI(temperature=0.9, model=LLM_MODEL)\n",
    "\n",
    "# Chain1: translate to english ----------------------------\n",
    "# - Input: Review\n",
    "# - Output: English_Review\n",
    "first_prompt:ChatPromptTemplate = ChatPromptTemplate.from_template(\n",
    "    \"Translate the following review to english:\"\n",
    "    \"\\n\\n{Review}\"\n",
    ")\n",
    "chain_one = first_prompt | llm\n",
    "\n",
    "# Chain2: summarise ---------------------------------------\n",
    "# - Input: English_Review\n",
    "# - Output: summary\n",
    "second_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Can you summarize the following review in 1 sentence:\"\n",
    "    \"\\n\\n{English_Review}\"\n",
    ")\n",
    "chain_two = second_prompt | llm\n",
    "\n",
    "# Chain3: find language ------------------------------------\n",
    "# Input: Review\n",
    "# Output: language\n",
    "third_prompt = ChatPromptTemplate.from_template(\n",
    "    \"What language is the following review:\\n\\n{Review}\"\n",
    ")\n",
    "chain_three = third_prompt | llm\n",
    "\n",
    "# Chain4: write respond -------------------------------------\n",
    "# Input: summary, language\n",
    "# Output: followup_message\n",
    "fourth_prompt = ChatPromptTemplate.from_template(\n",
    "    \"Write a follow up response to the following \"\n",
    "    \"summary in the specified language:\"\n",
    "    \"\\n\\nSummary: {summary}\\n\\nLanguage: {language}\"\n",
    ")\n",
    "chain_four = fourth_prompt | llm\n",
    "\n",
    "def debug_val(value):\n",
    "      return value\n",
    "\n",
    "# overall_chain: input= Review \n",
    "# and output= English_Review,summary, followup_message\n",
    "overall_chain = (\n",
    "    RunnableParallel({\n",
    "        \"Review\": RunnablePassthrough(),\n",
    "        \"English_Review\": chain_one | StrOutputParser(),\n",
    "        \"language\": chain_three | StrOutputParser()\n",
    "    })\n",
    "    | RunnableParallel({\n",
    "        \"debug\": lambda x: debug_val(x),\n",
    "        \"summary\": RunnableLambda(lambda x: x) | chain_two | StrOutputParser(),\n",
    "        \"language\": lambda x: x[\"language\"]\n",
    "    })\n",
    "  #  | chain_four\n",
    ")\n",
    "\n",
    "import pandas as pd\n",
    "df = pd.read_csv('Data.csv')\n",
    "overall_chain.invoke(df.Review[5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e626d36f",
   "metadata": {},
   "source": [
    "- `x` in `lambda x` has a `dict` output from previous `RunnableParallel`\n",
    "- to chain, it must be `RunnableX`. For example, raw lambda below isn't working:\n",
    "  ```python\n",
    "  \"summary\": lambda x: x | chain_two | StrOutputParser()\n",
    "  ```\n",
    "- `RunnableParallel({})` can be shorten to `{}`\n",
    "- `RunnablePassthrough` returns input as is\n",
    "- `StrOutputParser` is required for next chain\n",
    "  - This converts llm respond into `str`\n",
    "\n",
    "## Router Chain\n",
    "\n",
    "The chain can be routed in a few ways. For example, \"router chain\" can be parsed with `RouterOutputParser`, then routed into desired chain. Given an input query, it uses an LLM to select from a list of destination chains, and passes its inputs to the selected chain.\n",
    "\n",
    "(`RunnableBranch`)[https://python.langchain.com/api_reference/core/runnables/langchain_core.runnables.branch.RunnableBranch.html] can be used to achieve routing behaviour. Otherwise, `with_structured_output` can be used.\n",
    "\n",
    "**Router template**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3f054b",
   "metadata": {},
   "outputs": [],
   "source": [
    "MULTI_PROMPT_ROUTER_TEMPLATE = \"\"\"\\\n",
    "Given a raw text input to a \\\n",
    "language model select the model prompt best suited for the input. \\\n",
    "You will be given the names of the available prompts and a \\\n",
    "description of what the prompt is best suited for. \\\n",
    "You may also revise the original input if you think that revising\\\n",
    "it will ultimately lead to a better response from the language model.\n",
    "\n",
    "<< FORMATTING >>\n",
    "Return a markdown code snippet with a JSON object formatted to look like:\n",
    "```json\n",
    "{{{{\n",
    "    \"destination\": string \\ \"DEFAULT\" or name of the prompt to use in {destinations}\n",
    "    \"next_inputs\": string \\ a potentially modified version of the original input\n",
    "}}}}\n",
    "```\n",
    "\n",
    "REMEMBER: The value of “destination” MUST match one of \\\n",
    "the candidate prompts listed below.\\\n",
    "If “destination” does not fit any of the specified prompts, set it to “DEFAULT.”\n",
    "REMEMBER: \"next_inputs\" can just be the original input \\\n",
    "if you don't think any modifications are needed.\n",
    "\n",
    "<< CANDIDATE PROMPTS >>\n",
    "{destinations}\n",
    "\n",
    "<< INPUT >>\n",
    "{{input}}\n",
    "\n",
    "<< OUTPUT (remember to include the ```json)>>\\\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ceadef9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import StrEnum\n",
    "\n",
    "class DstPromptType(StrEnum):\n",
    "    default = \"default\"\n",
    "    physics = \"physics\"\n",
    "    math = \"math\"\n",
    "\n",
    "physics_template = \"\"\"\\\n",
    "You are a very smart physics professor. \\\n",
    "You are great at answering questions about physics in a concise\\\n",
    "and easy to understand manner. \\\n",
    "When you don't know the answer to a question you admit\\\n",
    "that you don't know.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "math_template = \"\"\"\\\n",
    "You are a very good mathematician. \\\n",
    "You are great at answering math questions. \\\n",
    "You are so good because you are able to break down \\\n",
    "hard problems into their component parts, \n",
    "answer the component parts, and then put them together\\\n",
    "to answer the broader question.\n",
    "\n",
    "Here is a question:\n",
    "{input}\"\"\"\n",
    "\n",
    "prompt_infos = [\n",
    "    {\n",
    "        \"name\": DstPromptType.physics.value, \n",
    "        \"description\": \"Good for answering questions about physics\", \n",
    "        \"prompt_template\": physics_template\n",
    "    },\n",
    "    {\n",
    "        \"name\": DstPromptType.math.value, \n",
    "        \"description\": \"Good for answering math questions\", \n",
    "        \"prompt_template\": math_template\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a035a481",
   "metadata": {},
   "source": [
    "**Router Example 1**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "47a5c1d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='1+1 is a basic addition problem. When you add 1 and 1 together, you get 2. This is because when you combine one object with another object, you end up with a total of two objects. So, 1+1 equals 2.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 56, 'prompt_tokens': 71, 'total_tokens': 127, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-3.5-turbo-0125', 'system_fingerprint': None, 'finish_reason': 'stop', 'logprobs': None}, id='run--a85d647b-c8fa-4e2a-98a0-0acb0f54a250-0', usage_metadata={'input_tokens': 71, 'output_tokens': 56, 'total_tokens': 127, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnableLambda\n",
    "from langchain.chains.router.llm_router import RouterOutputParser\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=LLM_MODEL)\n",
    "\n",
    "destination_chains:dict[str, RunnableSequence] = {}\n",
    "for p_info in prompt_infos:\n",
    "    prompt:ChatPromptTemplate = ChatPromptTemplate.from_template(template=p_info[\"prompt_template\"])\n",
    "    chain:RunnableSequence = prompt | llm\n",
    "    destination_chains[p_info[\"name\"]] = chain  \n",
    "    \n",
    "destinations = [f\"{p['name']}: {p['description']}\" for p in prompt_infos]\n",
    "destinations_str = \"\\n\".join(destinations)\n",
    "\n",
    "default_prompt = ChatPromptTemplate.from_template(\"{input}\")\n",
    "default_chain:RunnableSequence = default_prompt | llm\n",
    "\n",
    "router_template = MULTI_PROMPT_ROUTER_TEMPLATE.format(\n",
    "    destinations=destinations_str)\n",
    "router_prompt = ChatPromptTemplate.from_template(router_template)\n",
    "\n",
    "router_chain = router_prompt | llm | RouterOutputParser()\n",
    "\n",
    "# With helper function\n",
    "def route(router_chain_out:dict[str,str]):\n",
    "    return (\n",
    "        RunnableLambda(lambda router_chain_out : router_chain_out[\"next_inputs\"])\n",
    "        | destination_chains.get(router_chain_out[\"destination\"], default_chain)\n",
    "    )\n",
    "\n",
    "chain = (\n",
    "    router_chain\n",
    "    | RunnableParallel({\n",
    "        \"prompt_name\": lambda x : x[\"destination\"],\n",
    "        \"answer\": RunnableLambda(route)\n",
    "    })\n",
    ")\n",
    "chain.invoke(\"Explain 1+1\")\n",
    "\n",
    "# Without helper function\n",
    "chain = (\n",
    "    router_chain\n",
    "    | RunnableLambda(lambda x : destination_chains.get(x[\"destination\"], default_chain).invoke(x[\"next_inputs\"]))\n",
    ")\n",
    "\n",
    "chain.invoke(\"Explain 1+1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6bd79f",
   "metadata": {},
   "source": [
    "**Router Example 2**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "82274bc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Water evaporates through a process where molecules at the surface gain enough energy to break free from the liquid state and enter the gas phase. This process occurs at any temperature, not just at boiling point.\\n\\nHere's a simple breakdown of how it works:\\n\\n1. **Molecular Motion**: Water is made up of molecules that are constantly moving. At any given temperature, some molecules have higher kinetic energy than others.\\n\\n2. **Surface Molecules**: The molecules at the surface of the water are less tightly bound than those deeper in the liquid. When these surface molecules gain enough energy (often from heat), they can overcome the attractive forces of neighboring molecules.\\n\\n3. **Energy Sources**: The energy needed for evaporation can come from various sources, such as heat from the sun, warm air, or even the surrounding environment.\\n\\n4. **Vaporization**: Once a surface molecule gains enough energy, it escapes into the air as water vapor. This process continues as long as there are molecules with sufficient energy at the surface.\\n\\n5. **Factors Affecting Evaporation**: Several factors can influence the rate of evaporation, including temperature, humidity, air movement, and surface area. Higher temperatures and lower humidity generally increase the rate of evaporation.\\n\\nIn summary, evaporation is a natural process driven by the energy of water molecules, allowing them to transition from liquid to gas.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 275, 'prompt_tokens': 58, 'total_tokens': 333, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_560af6e559', 'finish_reason': 'stop', 'logprobs': None}, id='run--f8d81342-dc39-42c8-b01e-11b78c95d255-0', usage_metadata={'input_tokens': 58, 'output_tokens': 275, 'total_tokens': 333, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence, RunnableParallel, RunnableLambda\n",
    "from pydantic import BaseModel\n",
    "\n",
    "llm = ChatOpenAI(temperature=0, model=LLM_MODEL)\n",
    "\n",
    "destination_chains:dict[str, RunnableSequence] = {}\n",
    "for p_info in prompt_infos:\n",
    "    prompt:ChatPromptTemplate = ChatPromptTemplate.from_template(template=p_info[\"prompt_template\"])\n",
    "    chain:RunnableSequence = prompt | llm\n",
    "    destination_chains[p_info[\"name\"]] = chain\n",
    "\n",
    "class RouteQuery(BaseModel):\n",
    "    \"\"\"Name of destination chain\"\"\"\n",
    "    destination: DstPromptType\n",
    "\n",
    "router_template:str = \"\"\"\\\n",
    "Find the text input to a best suited destinations.\n",
    "<input>\n",
    "{input}\n",
    "</input>\n",
    "<destination>\n",
    "{destinations}\n",
    "</destination>\n",
    "\n",
    "default is your default destination when you are not sure.\\\n",
    "\"\"\"\n",
    "\n",
    "router_prompt = ChatPromptTemplate.from_template(router_template)\n",
    "router_chain = (\n",
    "    router_prompt\n",
    "    | llm.with_structured_output(RouteQuery)\n",
    ")\n",
    "overall_chain = (\n",
    "    RunnableParallel({\n",
    "        \"input\": lambda x : x[\"input\"],\n",
    "        \"destination\": router_chain\n",
    "    })\n",
    "    | RunnableLambda(lambda x : destination_chains.get(\n",
    "        x[\"destination\"].destination).invoke(x[\"input\"]))\n",
    ")\n",
    "\n",
    "overall_chain.invoke({\n",
    "    \"input\": \"How water evaporate?\",\n",
    "    \"destinations\": \"\\n\".join([p_info[\"name\"] for p_info in prompt_infos])\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669f756f",
   "metadata": {},
   "source": [
    "**Router Prompt Infos**\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    D[name]:::child --> A[prompt_info]\n",
    "    E[description]:::child --> A\n",
    "    F[prompt_template]:::child --> A\n",
    "\n",
    "    style A fill:green, color:#000\n",
    "    classDef child fill:lightgreen, color:#000\n",
    "```\n",
    "\n",
    "**Prompt Infos, Router Chain and Destination Chain**\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    A[prompt_info] --> D[name]:::child\n",
    "    A --> E[description]:::child\n",
    "\n",
    "    D --> G[router template]\n",
    "    E --> G\n",
    "    G --> H[router chain]\n",
    "\n",
    "    A --> F[promt_template]:::child\n",
    "    F --> I[destination</br>chain]\n",
    "\n",
    "    D --> J[\"destination_chains</br>{name:destination chain}\"]\n",
    "    I --> J\n",
    "\n",
    "    style A fill:green, color:#000\n",
    "    classDef child fill:lightgreen, color:#000\n",
    "    style H fill:pink, color:#000\n",
    "    style I fill:purple, color:#FFF\n",
    "```\n",
    "\n",
    "**Overall Chain Flowchart**\n",
    "\n",
    "```mermaid\n",
    "flowchart TB\n",
    "    A[input] --> B[router_prompt]\n",
    "\n",
    "    subgraph router_chain\n",
    "    B --> C[LLM]\n",
    "    C --> D[RouterOutputParser]\n",
    "    end\n",
    "\n",
    "    D --> E[\"destination_chains<br/>{name:destination chain}\"]\n",
    "\n",
    "    E -->|physics| F[physics prompt]\n",
    "    subgraph physics chain\n",
    "    F --> G[LLM]\n",
    "    end\n",
    "\n",
    "    E -->|math| H[math prompt]\n",
    "    subgraph math chain\n",
    "    H --> I[LLM]\n",
    "    end\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
