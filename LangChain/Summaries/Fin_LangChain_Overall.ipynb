{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfb50cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "def setup_openai_api_key():\n",
    "    from dotenv import load_dotenv\n",
    "    _ = load_dotenv()\n",
    "setup_openai_api_key()\n",
    "\n",
    "LLM_MODEL:str = \"gpt-4o-mini\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3040a3a",
   "metadata": {},
   "source": [
    "# LangChain\n",
    "\n",
    "LangChain is an framework designed to simplify LLM application development. It allows to chain LLM calls, prompts, tools, and other components together into sequential workflows.\n",
    "\n",
    "## Module\n",
    "\n",
    "> https://python.langchain.com/docs/concepts/architecture/\n",
    "\n",
    "- `langchain-core`: Base abstractions for chat models and other components.\n",
    "- `langchain-openai`, `langchain-anthropic`: Important integrations have been split into lightweight packages.\n",
    "- `langchain`: Chains, agents, and retrieval strategies that make up an application's cognitive architecture.\n",
    "- `langchain-community`: Third-party integrations that are community maintained.\n",
    "- `langgraph`: Orchestration framework for combining LangChain components into production-ready applications with persistence, streaming, and other key features.\n",
    "\n",
    "### Installation\n",
    "\n",
    "**With [uv](https://docs.astral.sh/uv/getting-started/installation/)**\n",
    "\n",
    "```\n",
    "uv add langchain\n",
    "```\n",
    "\n",
    "**With pip**\n",
    "\n",
    "```\n",
    "pip install langchain\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3583f878",
   "metadata": {},
   "source": [
    "## Chat Model\n",
    "\n",
    "LLMs commonly takes `messages` as input and returns a `message` as output. \n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    a[/messages/] --> b[LLM]\n",
    "    b --> c[/message/]\n",
    "```\n",
    "\n",
    "Modern LLM offers additional capabilities:\n",
    "\n",
    "|Capability|Description|\n",
    "|----------|-----------|\n",
    "|**Tool calling**    |Ability to find a suitable function along with arguments.\n",
    "|**Structure output**|Ability to return structured format such as json matching a given schema.\n",
    "|**Muldimodality**   |Ability to work with data other than text such as image, audio and video.\n",
    "\n",
    "### Standard parameters\n",
    "\n",
    "|Parameter|Description|\n",
    "|---------|-----------|\n",
    "model     |The identifier of AI model to use (i.e. \"gpt-4\")\n",
    "temperature|Controls the randomness of the model's output. A higher value (e.g., 1.0) makes responses more creative, while a lower value (e.g., 0.0) makes them more deterministic and focused.\n",
    "timeout   |The maximum time (in seconds) to wait for a response from the model before canceling the request.\n",
    "max_tokens|Limits the total number of tokens in the response. This controls how long the output can be.\n",
    "stop      |Specifies stop sequences that indicate when the model should stop generating tokens. For example, you might use specific strings to signal the end of a response.\n",
    "max_retries|The maximum number of attempts the system will make to resend a request if it fails due to issues like network timeouts or rate limits.\n",
    "api_key    |The API key required for authenticating with the model provider.\n",
    "base_url   |The URL of the API endpoint where requests are sent. This is typically provided by the model's provider and is necessary for directing your requests.\n",
    "rate_limiter|An optional BaseRateLimiter to space out requests to avoid exceeding rate limits. See rate-limiting below for more details.\n",
    "\n",
    "## Message\n",
    "\n",
    "> https://python.langchain.com/docs/concepts/messages/\n",
    "\n",
    "Messages are the unit of communication in chat models.\n",
    "\n",
    "**Message represents:**\n",
    "- input and output of a chat model\n",
    "- any additional context\n",
    "- metadata that may be associated with a conversation\n",
    "\n",
    "A message typically consists of the following information:\n",
    "- **Role**: The role of the message (i.e. \"user\", \"assistant\").\n",
    "- **Content**: The content of the message (i.e. text, multimodal data).\n",
    "- **Additional metadata**: id, name, token usage and other model-specific metadata.\n",
    "\n",
    "**Role**\n",
    "\n",
    "Roles are used to distinguish between different types of messages in a conversation and help the chat model understand how to respond to a given sequence of messages.\n",
    "\n",
    "|Role  |Description|\n",
    "|------|-----------|\n",
    "system<br/>`SystemMessage`| Tell the LLM how to behave and provide additional context. Not supported by all chat model providers.\n",
    "user<br/>`HumanMessage`   | Represents input from a user interacting with the model, usually in the form of text or other interactive input.\n",
    "assistant<br/>`AIMessage` | Represents a response from the model, which can include text or a request to invoke tools.\n",
    "tool<br/>`ToolMessage`    | A message used to pass the results of a tool invocation back to the model after external data or processing has been retrieved. Used with chat models that support tool calling.\n",
    "\n",
    "**Content**\n",
    "\n",
    "The content of a message text or a list of dictionaries representing multimodal data. The exact format of the content can vary between different chat model providers.\n",
    "\n",
    "**LangChain Messages**\n",
    "\n",
    "LangChain provides a unified message format that can be used across all chat models, allowing users to work with different chat models without worrying about the specific details of the message format used by each model provider.\n",
    "\n",
    "The five main message types are:\n",
    "|Type|Description|\n",
    "|----|-----------|\n",
    "|`SystemMessage` | corresponds to system role\n",
    "|`HumanMessage`  | corresponds to user role\n",
    "|`AIMessage`     | corresponds to assistant role\n",
    "|`ToolMessage`   | corresponds to tool role\n",
    "\n",
    "**OpenAI Message Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce329625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Response(id='resp_68bfd7fa93708196be44e400856c1bfa02d1f141ae65216b', created_at=1757403130.0, error=None, incomplete_details=None, instructions='You are a helpful assistant', metadata={}, model='gpt-4o-mini-2024-07-18', object='response', output=[ResponseOutputMessage(id='msg_68bfd7fb81d4819683db710cbb2c7c1702d1f141ae65216b', content=[ResponseOutputText(annotations=[], text='Hello! How can I assist you today?', type='output_text', logprobs=[])], role='assistant', status='completed', type='message')], parallel_tool_calls=True, temperature=0.0, tool_choice='auto', tools=[], top_p=1.0, background=False, max_output_tokens=None, max_tool_calls=None, previous_response_id=None, prompt=None, prompt_cache_key=None, reasoning=Reasoning(effort=None, generate_summary=None, summary=None), safety_identifier=None, service_tier='default', status='completed', text=ResponseTextConfig(format=ResponseFormatText(type='text'), verbosity='medium'), top_logprobs=0, truncation='disabled', usage=ResponseUsage(input_tokens=19, input_tokens_details=InputTokensDetails(cached_tokens=0), output_tokens=10, output_tokens_details=OutputTokensDetails(reasoning_tokens=0), total_tokens=29), user=None, store=True)\n",
      "ChatCompletion(id='chatcmpl-CDn7klfDSwXLryA1tkKq6mTwJ7BHL', choices=[Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='Hello! How can I assist you today?', refusal=None, role='assistant', annotations=[], audio=None, function_call=None, tool_calls=None))], created=1757403132, model='gpt-4o-mini-2024-07-18', object='chat.completion', service_tier='default', system_fingerprint='fp_8bda4d3a2c', usage=CompletionUsage(completion_tokens=9, prompt_tokens=19, total_tokens=28, completion_tokens_details=CompletionTokensDetails(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0), prompt_tokens_details=PromptTokensDetails(audio_tokens=0, cached_tokens=0)))\n"
     ]
    }
   ],
   "source": [
    "# OpenAI Message Format \n",
    "import openai\n",
    "from openai.types.chat import (\n",
    "    ChatCompletionSystemMessageParam, ChatCompletionUserMessageParam, ChatCompletionAssistantMessageParam\n",
    ")\n",
    "# response API\n",
    "respond = openai.OpenAI().responses.create(     \n",
    "    model=LLM_MODEL,\n",
    "    instructions=\"You are a helpful assistant\", # system message\n",
    "    input = \"Hello World!\",                     # user message\n",
    "    temperature=0\n",
    ")\n",
    "print(respond)\n",
    "\n",
    "system_msg: ChatCompletionSystemMessageParam = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful assistant\"\n",
    "}\n",
    "human_msg: ChatCompletionUserMessageParam = {\n",
    "    \"role\": \"user\",\n",
    "    \"content\": \"Hello World!\"\n",
    "}\n",
    "ai_msg: ChatCompletionAssistantMessageParam = {\n",
    "    \"role\": \"assistant\",\n",
    "    \"content\": \"This is message from LLM!\"\n",
    "}\n",
    "# chat completion API\n",
    "respond = openai.OpenAI().chat.completions.create(\n",
    "    model=LLM_MODEL,\n",
    "    messages=[system_msg, human_msg],\n",
    "    temperature=0\n",
    ")\n",
    "print(respond)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "401a7b63",
   "metadata": {},
   "source": [
    "**LangChain Message Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ff06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[SystemMessage(content='this is system message', additional_kwargs={}, response_metadata={}), HumanMessage(content='this is human message', additional_kwargs={}, response_metadata={}), AIMessage(content='this is ai message', additional_kwargs={}, response_metadata={})]\n",
      "['system', 'human', 'ai']\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.messages import (\n",
    "    SystemMessage, HumanMessage, AIMessage, ToolMessage\n",
    ")\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "msg_sys = SystemMessage(content=\"this is system message\")\n",
    "msg_human = HumanMessage(content=\"this is human message\")\n",
    "msg_ai = AIMessage(content=\"this is ai message\")\n",
    "print(\"msg :\", [msg_sys, msg_human, msg_ai])\n",
    "print(\"type:\", [msg_sys.type, msg_human.type, msg_ai.type])\n",
    "\n",
    "model = OpenAI(model=LLM_MODEL, temperature=0)\n",
    "respond = model.invoke([msg_sys, msg_human])\n",
    "print(respond)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f3d405",
   "metadata": {},
   "source": [
    "## Prompt\n",
    "\n",
    "Prompt is a template that structures input to language models. It defines how to format user input, system instructions, and context before sending to the LLM.\n",
    "\n",
    "Prompts ensure consistent, structured communication between your application and the LLM.\n",
    "\n",
    "Prompt Templates take as input a dictionary, where each key represents a variable in the prompt template to fill in.\n",
    "\n",
    "**Key components:**\n",
    "- `ChatPromptTemplate`: formats messages for chat models\n",
    "- `SystemMessagePrompt`: sets AI behavior/role\n",
    "- `HumanMessagePrompt`: user input template\n",
    "- `Variables`: placeholders filled at runtime (e.g., {user_input}, {context})\n",
    "\n",
    "### Prompt Templates\n",
    "\n",
    "**String PromptTemplates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "be8e33ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StringPromptValue(text='Tell me a joke about cats')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db356fd",
   "metadata": {},
   "source": [
    "**ChatPromptTemplates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4f602ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content='Tell me a joke about cats', additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    (\"user\", \"Tell me a joke about {topic}\")\n",
    "])\n",
    "prompt_template.invoke({\"topic\": \"cats\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf0b586",
   "metadata": {},
   "source": [
    "**MessagesPlaceholder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "15492133",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "messages=[SystemMessage(content='You are a helpful assistant', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"What's the capital of France?\", additional_kwargs={}, response_metadata={}), AIMessage(content='The capital of France is Paris.', additional_kwargs={}, response_metadata={}), HumanMessage(content='And what about Germany?', additional_kwargs={}, response_metadata={})]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    (\"system\", \"You are a helpful assistant\"),\n",
    "    MessagesPlaceholder(\"msgs\")\n",
    "])\n",
    "# Simple example with one message\n",
    "prompt_template.invoke({\"msgs\": [HumanMessage(content=\"hi!\")]})\n",
    "# More complex example with conversation history\n",
    "messages_to_pass = [\n",
    "    HumanMessage(content=\"What's the capital of France?\"),\n",
    "    AIMessage(content=\"The capital of France is Paris.\"),\n",
    "    HumanMessage(content=\"And what about Germany?\")\n",
    "]\n",
    "formatted_prompt = prompt_template.invoke({\"msgs\": messages_to_pass})\n",
    "print(formatted_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a0d2bbe",
   "metadata": {},
   "source": [
    "### Prompt Pipeline\n",
    "\n",
    "**Create prompt from template:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[template] --> C[Prompt]\n",
    "```\n",
    "\n",
    "**Use prompt to get response:**\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[style] --> C[Prompt]\n",
    "    B[message] --> C\n",
    "\n",
    "    C --> D[LLM]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1576005d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatPromptValue(messages=[SystemMessage(content='Translate the user input into a style that is British English in a calm and respenctful tone.', additional_kwargs={}, response_metadata={}), HumanMessage(content=\"Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\", additional_kwargs={}, response_metadata={})])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "customer_email = \"\"\"\\\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\\\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"\\\n",
    "British English \\\n",
    "in a calm and respenctful tone\\\n",
    "\"\"\"\n",
    "\n",
    "prompt = \"\"\"\\\n",
    "Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "# With PromptTemplate\n",
    "prompt_template = PromptTemplate.from_template(prompt)\n",
    "prompt_template.invoke({\n",
    "    \"style\": style,\n",
    "    \"customer_email\": customer_email\n",
    "})\n",
    "\n",
    "# With ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate([\n",
    "    SystemMessage(content=\"Translate the user input into a style that is {style}.\"),\n",
    "    HumanMessage(content=\"{customer_email}\")\n",
    "])\n",
    "prompt_template.invoke({\n",
    "    \"style\": style,\n",
    "    \"customer_email\": customer_email\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff1527a",
   "metadata": {},
   "source": [
    "## Runnable\n",
    "\n",
    "The Runnable interface is the fundamental component of LangChain, and it's implemented across many of them, such as language models, output parsers, retrievers, compiled LangGraph graphs and more.\n",
    "\n",
    "### Core interface\n",
    "\n",
    "- `invoke`: A single input is transformed into an output.\n",
    "- `batch`: Multiple inputs are efficiently transformed into outputs.\n",
    "- `stream`: Outputs are streamed as they are produced.\n",
    "- `inspect`: Schematic information about Runnable's input, output, and configuration can be accessed.\n",
    "- `compose`: Multiple Runnables can be composed to work together using the LangChain Expression Language (LCEL) to create complex pipelines.\n",
    " \n",
    "### Input and output types\n",
    "\n",
    "Every Runnable is characterized by an input and output type. These input and output types are defined by the Runnable itself.\n",
    "\n",
    "Runnable methods that result in the execution of the Runnable (e.g., invoke, batch, stream, astream_events) work with these input and output types.\n",
    "\n",
    "- `invoke`: Accepts an input and returns an output.\n",
    "- `batch`: Accepts a list of inputs and returns a list of outputs.\n",
    "- `stream`: Accepts an input and returns a generator that yields outputs.\n",
    "\n",
    "**Input and output of each component**\n",
    "\n",
    "|Component|Input Type|Output Type|\n",
    "|---------|----------|-----------|\n",
    "|Prompt   |dictionary|PromptValue|\n",
    "|ChatModel|a string, list of chat messages or a PromptValue|ChatMessage|\n",
    "|LLM      |a string, list of chat messages or a PromptValue|String|\n",
    "|OutputParser|the output of an LLM or ChatModel|Depends on the parser|\n",
    "|Retriever|a string|List of Documents|\n",
    "|Tool     |a string or dictionary, depending on the tool|Depends on the tool|\n",
    "\n",
    "### Inspect Schema\n",
    "\n",
    "Runnable provides methods to get json or Pydantic schema of input, output and config.\n",
    "\n",
    "|Method|Description|\n",
    "|------|-----------|\n",
    "|`get_input_schema`|Gives the Pydantic Schema of the input schema for the Runnable|\n",
    "|`get_output_schema`|Gives the Pydantic Schema of the output schema for the Runnable|\n",
    "|`config_schema`|Gives the Pydantic Schema of the config schema for the Runnable|\n",
    "|`get_input_jsonschema`|Gives the JSONSchema of the input schema for the Runnable|\n",
    "|`get_output_jsonschema`|Gives the JSONSchema of the output schema for the Runnable|\n",
    "|`get_config_jsonschema`|Gives the JSONSchema of the config schema for the Runnable|\n",
    "\n",
    "### RunnableConfig\n",
    "\n",
    "Any of the runnnable execute methods (i.e. `invoke`) accept a second argument called `RunnableConfig`. This argument is a `dict` that contains configuration that will be used at run time during the execution.\n",
    "\n",
    "It is important that the `RunnableConfig` is propagated to all sub-calls made by the `Runnable`. This allows providing run time configuration values to the parent `Runnable` that are inherited by all sub-calls.\n",
    "\n",
    "|Attribute|Description|\n",
    "|---------|-----------|\n",
    "|run_name|Name used for the given Runnable (not inherited)|\n",
    "|run_id  |Unique identifier for this call. sub-calls will get their own unique run ids|\n",
    "|tags    |Tags for this call and any sub-calls|\n",
    "|metadata|Metadata for this call and any sub-calls|\n",
    "|callbacks|Callbacks for this call and any sub-calls|\n",
    "|max_concurrency|Maximum number of parallel calls to make (e.g., used by batch)|\n",
    "|recursion_limit|Maximum number of times a call can recurse (e.g., used by Runnables that return Runnables)|\n",
    "|configurable|Runtime values for configurable attributes of the Runnable|\n",
    "\n",
    "**Create a `Runnable`:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b76e84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello It's me!\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_openai import OpenAI\n",
    "\n",
    "# With LCEL\n",
    "prompt_template = PromptTemplate.from_template(\"My prompt\")\n",
    "model = OpenAI(model=LLM_MODEL, temperature=0)\n",
    "output_parser = StrOutputParser()\n",
    "runnable_chain = prompt_template | model | output_parser\n",
    "\n",
    "# With custom runnable\n",
    "def foo(input):\n",
    "    return f\"{input} It's me!\"\n",
    "foo_runnable = RunnableLambda(foo)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "746f6e19",
   "metadata": {},
   "source": [
    "**Running a runnable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81e46a59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val1, It's me! val2\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.runnables import RunnableLambda\n",
    "\n",
    "def foo(input):\n",
    "    return f\"{input[\"key1\"]}, It's me! {input[\"key2\"]}\"\n",
    "foo_runnable = RunnableLambda(foo)\n",
    "\n",
    "result = foo_runnable.invoke({\"key1\": \"val1\", \"key2\": \"val2\"})\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd4e73a",
   "metadata": {},
   "source": [
    "**Setting a runnable config**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2213a8f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableLambda, RunnableConfig\n",
    "\n",
    "config: RunnableConfig = {\n",
    "    \"run_name\": \"hello_run\",\n",
    "    \"tags\": [\"tag1\", \"tag2\"],\n",
    "    \"metadata\": {\"hello_key\": \"hello_val\"}\n",
    "}\n",
    "\n",
    "def foo(input):\n",
    "    return f\"{input} It's me!\"\n",
    "foo_runnable = RunnableLambda(foo)\n",
    "\n",
    "result = foo_runnable.invoke(\n",
    "    \"hello\",\n",
    "    config = config\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a39d91c",
   "metadata": {},
   "source": [
    "### LCEL Cheatsheet\n",
    "\n",
    "> https://python.langchain.com/docs/how_to/lcel_cheatsheet/\n",
    "\n",
    "**Runnable**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232d0cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "invoke: 5\n",
      "batch: ['7', '8', '9']\n",
      "stream:\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "compose with pipe: [{'foo': 2}, {'foo': 2}]\n",
      "RunnableParallel: {'first': {'foo': 2}, 'second': [2, 2]}\n",
      "RunnableLambda: 7\n",
      "Merge input and output: {'foo': 10, 'bar': 17}\n",
      "Bind: {'bar': 'hello', 'foo': 'bye'}\n",
      "Fallback: 5foo\n",
      "attempt with counter=0\n",
      "attempt with counter=1\n",
      "Retry: 2.0\n"
     ]
    }
   ],
   "source": [
    "# invoke\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "runnable = RunnableLambda(lambda x: str(x))\n",
    "print(\"invoke:\", runnable.invoke(5))\n",
    "\n",
    "# batch\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "runnable = RunnableLambda(lambda x: str(x))\n",
    "print(\"batch:\", runnable.batch([7, 8, 9]))\n",
    "\n",
    "# stream\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "def func1(x):\n",
    "    for y in x:\n",
    "        yield str(y)\n",
    "runnable = RunnableLambda(func1)\n",
    "print(\"stream:\")\n",
    "for chunk in runnable.stream(range(5)):\n",
    "    print(chunk)\n",
    "\n",
    "# Compose runnable with pipe operator\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "chain = runnable1 | runnable2\n",
    "print(\"compose with pipe:\", chain.invoke(2))\n",
    "\n",
    "# Runnable parallel\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "chain = RunnableParallel(first=runnable1, second=runnable2)\n",
    "print(\"RunnableParallel:\", chain.invoke(2))\n",
    "\n",
    "# Runnable lambda\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "def func2(x):\n",
    "    return x + 5\n",
    "runnable = RunnableLambda(func2)\n",
    "print(\"RunnableLambda:\", runnable.invoke(2))\n",
    "\n",
    "# Merge input and output\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "runnable1 = RunnableLambda(lambda x: x[\"foo\"] + 7)\n",
    "chain = RunnablePassthrough.assign(bar=runnable1)\n",
    "print(\"Merge input and output:\", chain.invoke({\"foo\": 10}))\n",
    "\n",
    "# Runnable bind\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "def func3(main_arg: dict, other_arg: str|None = None) -> dict:\n",
    "    if other_arg:\n",
    "        return {**main_arg, **{\"foo\": other_arg}}\n",
    "    return main_arg\n",
    "runnable1 = RunnableLambda(func3)\n",
    "bound_runnable1 = runnable1.bind(other_arg=\"bye\")\n",
    "print(\"Bind:\", bound_runnable1.invoke({\"bar\": \"hello\"}))\n",
    "\n",
    "# Rrunnable fallback\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "runnable1 = RunnableLambda(lambda x: x + \"foo\")\n",
    "runnable2 = RunnableLambda(lambda x: str(x) + \"foo\")\n",
    "chain = runnable1.with_fallbacks([runnable2])\n",
    "print(\"Fallback:\", chain.invoke(5))\n",
    "\n",
    "# Runnable retry\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "counter = -1\n",
    "def func4(x):\n",
    "    global counter\n",
    "    counter += 1\n",
    "    print(f\"attempt with {counter=}\")\n",
    "    return x / counter\n",
    "chain = RunnableLambda(func4).with_retry(stop_after_attempt=2)\n",
    "print(\"Retry:\", chain.invoke(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6fa89a",
   "metadata": {},
   "source": [
    "**RunnableConfig**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39933614",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config: {'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}\n",
      "Default config: {'first': {'foo': 7}, 'second': [7, 7], 'third': '7'}\n",
      "w/ configurable config : {'not bar': 3}\n",
      "w/o configurable config: {'bar': 3}\n",
      "w/ configurable config : {'foo': 7}\n",
      "w/o configurable config: [{'foo': 7}]\n"
     ]
    }
   ],
   "source": [
    "# Runnable config\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "runnable3 = RunnableLambda(lambda x: str(x))\n",
    "chain = RunnableParallel(first=runnable1, second=runnable2, third=runnable3)\n",
    "print(\"Config:\", chain.invoke(7, config={\"max_concurrency\": 2}))\n",
    "\n",
    "# Runnable default config\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "runnable3 = RunnableLambda(lambda x: str(x))\n",
    "chain = RunnableParallel(first=runnable1, second=runnable2, third=runnable3)\n",
    "configured_chain = chain.with_config(max_concurrency=2)\n",
    "print(\"Default config:\", chain.invoke(7))\n",
    "\n",
    "# Runnable attributes configurable\n",
    "from typing import Any\n",
    "from langchain_core.runnables import (\n",
    "    ConfigurableField, RunnableConfig, RunnableSerializable,\n",
    ")\n",
    "class FooRunnable(RunnableSerializable[dict, dict]):\n",
    "    output_key: str\n",
    "    def invoke(self,\n",
    "        input: Any,\n",
    "        config: RunnableConfig|None = None,\n",
    "        **kwargs: Any) -> list:\n",
    "        return self._call_with_config(self.subtract_seven, input, config, **kwargs)\n",
    "    def subtract_seven(self, input: dict) -> dict:\n",
    "        return {self.output_key: input[\"foo\"] - 7}\n",
    "\n",
    "runnable1 = FooRunnable(output_key=\"bar\")\n",
    "configurable_runnable1 = runnable1.configurable_fields(\n",
    "    output_key=ConfigurableField(id=\"output_key\")\n",
    ")\n",
    "result1 = configurable_runnable1.invoke(\n",
    "    {\"foo\": 10}, config={\"configurable\": {\"output_key\": \"not bar\"}}\n",
    ")\n",
    "result2 = configurable_runnable1.invoke({\"foo\": 10})\n",
    "print(\"w/ configurable config :\", result1)\n",
    "print(\"w/o configurable config:\", result2)\n",
    "\n",
    "# Chain components configurable\n",
    "from typing import Any\n",
    "from langchain_core.runnables import (\n",
    "    RunnableConfig, RunnableLambda, RunnableParallel\n",
    ")\n",
    "class ListRunnable(RunnableSerializable[Any, list]):\n",
    "    def invoke(\n",
    "        self, input: Any, config: RunnableConfig|None = None, **kwargs: Any\n",
    "    ) -> list:\n",
    "        return self._call_with_config(self.listify, input, config, **kwargs)\n",
    "    def listify(self, input: Any) -> list:\n",
    "        return [input]\n",
    "\n",
    "class StrRunnable(RunnableSerializable[Any, str]):\n",
    "    def invoke(\n",
    "        self, input: Any, config: RunnableConfig|None = None, **kwargs: Any\n",
    "    ) -> list:\n",
    "        return self._call_with_config(self.strify, input, config, **kwargs)\n",
    "    def strify(self, input: Any) -> str:\n",
    "        return str(input)\n",
    "\n",
    "\"\"\"ListRunnable can be replaced by StrRunnable based on second_step\"\"\"\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "configurable_runnable = ListRunnable().configurable_alternatives(\n",
    "    ConfigurableField(id=\"second_step\"), default_key=\"list\", string=StrRunnable()\n",
    ")\n",
    "chain = runnable1 | configurable_runnable\n",
    "result1 = chain.invoke(7, config={\"configurable\": {\"second_step\": \"string\"}})\n",
    "result2 = chain.invoke(7)\n",
    "print(\"w/ configurable config :\", result1)\n",
    "print(\"w/o configurable config:\", result2)\n",
    "\n",
    "# Build chain dynamicall based on input\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "chain = RunnableLambda(lambda x: runnable1 if x > 6 else runnable2)\n",
    "print(\"Dynamic chain:\", chain.invoke(7))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013596a5",
   "metadata": {},
   "source": [
    "**Others**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a371326b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "astream:\n",
      "event=on_chain_start | name=RunnableSequence | data={'input': 'bar'}\n",
      "event=on_chain_start | name=first | data={}\n",
      "event=on_chain_stream | name=first | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_start | name=second | data={}\n",
      "event=on_chain_end | name=first | data={'output': {'foo': 'bar'}, 'input': 'bar'}\n",
      "event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=second | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_stream | name=RunnableSequence | data={'chunk': {'foo': 'bar'}}\n",
      "event=on_chain_end | name=second | data={'output': {'foo': 'bar'}, 'input': {'foo': 'bar'}}\n",
      "event=on_chain_end | name=RunnableSequence | data={'output': {'foo': 'bar'}}\n",
      "batch as completed:\n",
      "slept 1\n",
      "1 None\n",
      "slept 2\n",
      "0 None\n",
      "subset: {'foo': 7, 'bar': 'hi'}\n",
      "map: [5, 6, 7]\n",
      "get all prompts:\n",
      "**prompt i=0**\n",
      "\n",
      "================================ System Message ================================\n",
      "\n",
      "good ai\n",
      "\n",
      "================================ Human Message =================================\n",
      "\n",
      "{input}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "**prompt i=1**\n",
      "\n",
      "================================ System Message ================================\n",
      "\n",
      "really good ai\n",
      "\n",
      "================================ Human Message =================================\n",
      "\n",
      "{input}\n",
      "\n",
      "================================== AI Message ==================================\n",
      "\n",
      "{ai_output}\n",
      "\n",
      "================================ Human Message =================================\n",
      "\n",
      "{input2}\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "listner:\n",
      "start_time: 2025-09-10 19:58:44.543660+00:00\n",
      "end_time: 2025-09-10 19:58:46.549249+00:00\n"
     ]
    }
   ],
   "source": [
    "# astream\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x}, name=\"first\")\n",
    "async def func(x):\n",
    "    for _ in range(5):\n",
    "        yield x\n",
    "runnable2 = RunnableLambda(func, name=\"second\")\n",
    "chain = runnable1 | runnable2\n",
    "print(\"astream:\")\n",
    "async for event in chain.astream_events(\"bar\", version=\"v2\"):\n",
    "    print(f\"event={event['event']} | name={event['name']} | data={event['data']}\")\n",
    "\n",
    "# batch as completed\n",
    "import time\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "runnable1 = RunnableLambda(lambda x: time.sleep(x) or print(f\"slept {x}\"))\n",
    "print(\"batch as completed:\")\n",
    "for idx, result in runnable1.batch_as_completed([2, 1]):\n",
    "    print(idx, result)\n",
    "\n",
    "# returns subset of output dict\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "runnable1 = RunnableLambda(lambda x: x[\"baz\"] + 5)\n",
    "chain = RunnablePassthrough.assign(foo=runnable1).pick([\"foo\", \"bar\"])\n",
    "print(\"subset:\", chain.invoke({\"bar\": \"hi\", \"baz\": 2}))\n",
    "\n",
    "# Make batch from mutiple output\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "runnable1 = RunnableLambda(lambda x: list(range(x)))\n",
    "runnable2 = RunnableLambda(lambda x: x + 5)\n",
    "chain = runnable1 | runnable2.map()\n",
    "print(\"map:\", chain.invoke(3))\n",
    "\n",
    "# Graph\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel\n",
    "runnable1 = RunnableLambda(lambda x: {\"foo\": x})\n",
    "runnable2 = RunnableLambda(lambda x: [x] * 2)\n",
    "runnable3 = RunnableLambda(lambda x: str(x))\n",
    "chain = runnable1 | RunnableParallel(second=runnable2, third=runnable3)\n",
    "chain.get_graph().draw_mermaid()\n",
    "\n",
    "# Get all prompts\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "prompt1 = ChatPromptTemplate.from_messages(\n",
    "    [(\"system\", \"good ai\"), (\"human\", \"{input}\")]\n",
    ")\n",
    "prompt2 = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"really good ai\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "        (\"ai\", \"{ai_output}\"),\n",
    "        (\"human\", \"{input2}\"),\n",
    "    ]\n",
    ")\n",
    "fake_llm = RunnableLambda(lambda prompt: \"i am good ai\")\n",
    "chain = prompt1.assign(ai_output=fake_llm) | prompt2 | fake_llm\n",
    "print(\"get all prompts:\")\n",
    "for i, prompt in enumerate(chain.get_prompts()):\n",
    "    print(f\"**prompt {i=}**\\n\")\n",
    "    print(prompt.pretty_repr())\n",
    "    print(\"\\n\" * 3)\n",
    "\n",
    "# Listener\n",
    "import time\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_core.tracers.schemas import Run\n",
    "def on_start(run_obj: Run):\n",
    "    print(\"start_time:\", run_obj.start_time)\n",
    "def on_end(run_obj: Run):\n",
    "    print(\"end_time:\", run_obj.end_time)\n",
    "\n",
    "runnable1 = RunnableLambda(lambda x: time.sleep(x))\n",
    "print(\"listner:\")\n",
    "chain = runnable1.with_listeners(on_start=on_start, on_end=on_end)\n",
    "chain.invoke(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7162e77",
   "metadata": {},
   "source": [
    "## Output Parser\n",
    "\n",
    "> https://python.langchain.com/docs/concepts/output_parsers/\n",
    "\n",
    "Output parser takes output of a model and transforming it to structured output  for downstream tasks. Useful when you are using LLMs to generate structured data, or to normalize output from chat models and LLMs.\n",
    "\n",
    "- StrOutputParser\n",
    "- JsonOutputParser\n",
    "- XMLOutputParser\n",
    "- CommaSeparatedListOutputParser\n",
    "- OutputFixingParser\n",
    "- RetryWithErrorOutputParser\n",
    "- PydanticOutputParser\n",
    "- YamlOutputParser\n",
    "- PandasDataFrameOutputParser\n",
    "- EnumOutputParser\n",
    "- DatetimeOutputParser\n",
    "- StructuredOutputParser"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain-study-note",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
