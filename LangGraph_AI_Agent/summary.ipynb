{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48688dd8",
   "metadata": {},
   "source": [
    "# ReAct Agent\n",
    "\n",
    "> https://arxiv.org/abs/2210.03629\n",
    "\n",
    "ReAct stands for reasoning and acting.\n",
    "\n",
    "the **ReAct agent combines reasoning and acting**: it thinks about the problem, chooses actions using tools, observes the results, and then produces a final answer.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Reasoning</br>Traces] --> |thought| B[LLM]\n",
    "    B --> A\n",
    "\n",
    "    B --> |Action| C[Env]\n",
    "    C --> |Observation| B\n",
    "```\n",
    "\n",
    "## ReAct Agent Workflow\n",
    "\n",
    "1. **System Message Setup**\n",
    "    - The agent requires a very specific system prompt to define how it should operate.\n",
    "2. **Loop Execution**\n",
    "    - The agent cycles through a structured loop of:\n",
    "        - **Thought** → internal reasoning about the question.\n",
    "        - **Action** → selecting and running an available tool/function.\n",
    "        - **Pause** → marking the completion of the action.\n",
    "        - **Observation** → capturing the result of the action.\n",
    "3. **Answer Production**\n",
    "    - Once the loop finishes, the agent outputs an answer.\n",
    "4. **Tools Provided**\n",
    "    - The agent is given access to specific tools/functions.\n",
    "    - Example tools in the transcript:\n",
    "        - `calculate` → evaluates string expressions.\n",
    "        - `average_dog_weight` → returns mock weights for certain breeds.\n",
    "5. **Traceability**\n",
    "    - The workflow shows step-by-step traces:\n",
    "      - Incoming question → Thought → Action → Pause → Observation → Answer.\n",
    "6. **Example Run**\n",
    "   * Input: *“How much does a toy poodle weigh?”*\n",
    "   * Thought: *“I should look up the dog’s weight using average dog weight for a toy poodle.”*\n",
    "   * Action: *`average_dog_weight, toy poodle`*\n",
    "   * Observation: returns result.\n",
    "   * Final Answer is then produced from the observation.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start / Incoming question] --> B[System message defines ReAct protocol]\n",
    "    B --> C{Answer ready?}\n",
    "\n",
    "    C -- \"No\" --> D[Thought: reason about the question]\n",
    "    D --> E[Action: run a chosen tool/function]\n",
    "    E --> F[Pause: mark action completion]\n",
    "    F --> G[Observation: record tool result]\n",
    "    G --> C\n",
    "\n",
    "    C -- \"Yes\" --> H[Compose final answer]\n",
    "    H --> I[Output answer]\n",
    "\n",
    "    %% Tooling context\n",
    "    subgraph Tools available to the agent\n",
    "        T1[\"calculate(expr)\"]\n",
    "        T2[\"average_dog_weight(breed)\"]\n",
    "    end\n",
    "\n",
    "    %% Action may call any tool\n",
    "    E -. may call .-> T1\n",
    "    E -. may call .-> T2\n",
    "    T1 --> G\n",
    "    T2 --> G\n",
    "```\n",
    "\n",
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c77c0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Tavily API key\n",
    "def setup_openai_api_key():\n",
    "    from dotenv import load_dotenv\n",
    "    _ = load_dotenv()\n",
    "setup_openai_api_key()\n",
    "\n",
    "LLM_MODEL:str = \"gpt-4o-mini\"\n",
    "# LLM_MODEL:str = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c9917",
   "metadata": {},
   "source": [
    "**Agent class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "8c60e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "client = openai.OpenAI()\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, system:str=\"\"):\n",
    "        self.system = system\n",
    "        self.messages:list[dict[str,str]] = []\n",
    "        if self.system:\n",
    "            self.messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system\n",
    "            })\n",
    "    def __call__(self, message:str):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "        result = self.execute()\n",
    "        self.messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": result\n",
    "        })\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        completion:ChatCompletion = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=0,\n",
    "            messages=self.messages)\n",
    "        return completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19827526",
   "metadata": {},
   "source": [
    "- `Agent.__call__`\n",
    "  - add user message into agent's message array\n",
    "  - add LLM response to agent's message array\n",
    "- `OpenAI.chat.completions.create`\n",
    "  - `temperature=0`: for the deterministic response\n",
    "  - `messages=self.messages`: list of messages comprising the conversation so far.\n",
    "\n",
    "**Agent prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "bb59c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "average_dog_weight:\n",
    "e.g. average_dog_weight: Collie\n",
    "returns average weight of a dog when given the breed\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: How much does a Bulldog weigh?\n",
    "Thought: I should look the dogs weight using average_dog_weight\n",
    "Action: average_dog_weight: Bulldog\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: A Bulldog weights 51 lbs\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: A bulldog weights 51 lbs\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49129a",
   "metadata": {},
   "source": [
    "**Agent actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61ae204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(what):\n",
    "    return eval(what)\n",
    "\n",
    "def average_dog_weight(name):\n",
    "    if name in \"Scottish Terrier\": \n",
    "        return(\"Scottish Terriers average 20 lbs\")\n",
    "    elif name in \"Border Collie\":\n",
    "        return(\"a Border Collies average weight is 37 lbs\")\n",
    "    elif name in \"Toy Poodle\":\n",
    "        return(\"a toy poodles average weight is 7 lbs\")\n",
    "    else:\n",
    "        return(\"An average dog weights 50 lbs\")\n",
    "\n",
    "known_actions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"average_dog_weight\": average_dog_weight\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71992856",
   "metadata": {},
   "source": [
    "**Manual Agent Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "16d09ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'system', 'content': 'You run in a loop of Thought, Action, PAUSE, Observation.\\nAt the end of the loop you output an Answer\\nUse Thought to describe your thoughts about the question you have been asked.\\nUse Action to run one of the actions available to you - then return PAUSE.\\nObservation will be the result of running those actions.\\n\\nYour available actions are:\\n\\ncalculate:\\ne.g. calculate: 4 * 7 / 3\\nRuns a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\\n\\naverage_dog_weight:\\ne.g. average_dog_weight: Collie\\nreturns average weight of a dog when given the breed\\n\\nExample session:\\n\\nQuestion: How much does a Bulldog weigh?\\nThought: I should look the dogs weight using average_dog_weight\\nAction: average_dog_weight: Bulldog\\nPAUSE\\n\\nYou will be called again with this:\\n\\nObservation: A Bulldog weights 51 lbs\\n\\nYou then output:\\n\\nAnswer: A bulldog weights 51 lbs'}\n",
      "{'role': 'user', 'content': 'How much does a toy poodle weigh?'}\n",
      "{'role': 'assistant', 'content': 'Thought: I need to find the average weight of a Toy Poodle using the average_dog_weight action. \\nAction: average_dog_weight: Toy Poodle\\nPAUSE'}\n",
      "{'role': 'user', 'content': 'Observation: a toy poodles average weight is 7 lbs'}\n",
      "{'role': 'assistant', 'content': 'Answer: A Toy Poodle weighs 7 lbs.'}\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(prompt)\n",
    "\n",
    "agent_result = agent(\"How much does a toy poodle weigh?\")\n",
    "# link between thought and action is missing.\n",
    "tool_result = average_dog_weight(\"Toy Poodle\")\n",
    "# link between tool result and agent input is missing.\n",
    "agent_result = agent(f\"Observation: {tool_result}\")\n",
    "\n",
    "for msg in agent.messages:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288f2d6",
   "metadata": {},
   "source": [
    "**Agent with look example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "action_regex = re.compile(\"^Action: (\\w+): (.*)$\")\n",
    "\n",
    "def query(question:str, max_iterations:int=5):\n",
    "    agent = Agent(prompt)\n",
    "    next_prompt:str = question\n",
    "    for i in range(max_iterations):\n",
    "        result = agent(next_prompt)\n",
    "        print(result)\n",
    "        actions = [\n",
    "            action_regex.match(action)\n",
    "            for action in result.split('\\n')\n",
    "            if action_regex.match(action)\n",
    "        ]\n",
    "        if len(actions) == 0:\n",
    "            return\n",
    "        \n",
    "        action, action_input = actions[0].groups()\n",
    "        if action not in known_actions:\n",
    "            raise Exception(\n",
    "                f\"Unknown action: {action}: {action_input}\")\n",
    "        print(f\" -- running {action} {action_input}\")\n",
    "        observation = known_actions[action](action_input)\n",
    "        next_prompt = f\"Observation: {observation}\"\n",
    "        print(next_prompt)\n",
    "\n",
    "question = \"\"\"\\\n",
    "I have 2 dogs, a border collie and a scottish terrier. \\\n",
    "What is their combined weight\\\n",
    "\"\"\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d60b84",
   "metadata": {},
   "source": [
    "- `^Action: (\\w+): (.*)$`\n",
    "  - `^`\n",
    "    - Start of string\n",
    "  - `Action: `\n",
    "    - Literal \"Action: \"\n",
    "  - `(\\w+)` \n",
    "    - Capture group 1\n",
    "    - One or more word characters (letters, digits, underscore)\n",
    "    - Action type\n",
    "  - `: `\n",
    "    - Literal \": \"\n",
    "  - `(.*)`\n",
    "    - Capture group 2 - match().group(1)\n",
    "    - Zero or more of any character (except newline)\n",
    "    - Action input\n",
    "  - `$`\n",
    "    - End of string\n",
    "- `action_regex.match`\n",
    "  - `.group(0)` : The entire matched string\n",
    "  - `.group(1)` : The action name (first capture group)\n",
    "  - `.group(2)` : The action parameters (second capture group)\n",
    "  - `.groups()` : `Tuple(.group(1), .group(2))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c8cc8",
   "metadata": {},
   "source": [
    "# Agent in LangGraph\n",
    "\n",
    "---\n",
    "**Sequence Chart**\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant LLM\n",
    "    participant Tools\n",
    "\n",
    "    User ->> LLM: Initial input\n",
    "    \n",
    "    Note over LLM: Prompt + LLM\n",
    "\n",
    "    loop [while tool_calls present]\n",
    "        LLM ->> Tools: Execute tools\n",
    "        Tools -->> LLM: ToolMessage for each tool_calls\n",
    "    end\n",
    "\n",
    "    LLM ->> User: Return final state\n",
    "```\n",
    "---\n",
    "**Flow Chart**\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    llm --> B{exists_action}\n",
    "    B --> D([END])\n",
    "    B --> C[take_action]\n",
    "    C --> llm\n",
    "```\n",
    "---\n",
    "\n",
    "### AgentState and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=4) #increased number of results\n",
    "print(type(tool))\n",
    "print(tool.name)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system:str=\"\"):\n",
    "        self.system:str = system\n",
    "        graph:StateGraph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    \n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "    \n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results: list[ToolMessage] = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t[\"name\"] in self.tools:\n",
    "                print(\"...bad tool name...\")\n",
    "                result = \"bad tool name, retry\"\n",
    "            else:\n",
    "                result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(ToolMessage(\n",
    "                tool_call_id=t[\"id\"],\n",
    "                name=t[\"name\"],\n",
    "                content=str(result)))\n",
    "        print(\"Back to the model\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176be19b",
   "metadata": {},
   "source": [
    "- `AgentState`\n",
    "  - accessible to all parts of the graph\n",
    "  - local to the graph\n",
    "  - can be stored in a persistence layer\n",
    "    - can resume with the state at any point in time\n",
    "  - `Annotated`\n",
    "    - `list[AnyMessage]` is the actual type of `messages`\n",
    "    - `operator.add` is used as message reducer. as a result, message will be concatenated rather than replaced on update\n",
    "      - first callable metatdata\n",
    "    - without `Annotated`, value will be replaced on update\n",
    "    - when return value doesn't contain the field, it'll stay not changed\n",
    "- `StateGraph`\n",
    "  - `add_node`: add node with name-runnable pair\n",
    "  - `add_edge`: add connection between nodes in \"from\", \"to\" order\n",
    "  - `add_conditional_edges`: `add_edge` but with condition\n",
    "- each node returns `{\"messages\", result}`, which then gets added into `AgentState[\"messages\"]`\n",
    "\n",
    "#### State graph\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/low_level/?h=stategraph#schema\n",
    "\n",
    "The first thing to do when you defining a graph is define the `State` of the graph.\n",
    "\n",
    "The `State` consists of:\n",
    "- schema of the graph\n",
    "- reducer functions which specify how to apply updates to the state\n",
    "\n",
    "The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a `TypedDict` or a `Pydantic` model. All Nodes will emit updates to the `State` which are then applied using the specified reducer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=LLM_MODEL)\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "\n",
    "# from IPython.display import Image\n",
    "# Image(abot.graph.get_graph().draw_png())\n",
    "\n",
    "messages = [HumanMessage(content=\"What is the weather in london?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "\n",
    "print(result)\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a7b5c",
   "metadata": {},
   "source": [
    "## Agentic search tool\n",
    "\n",
    "Traditional zero-shot learning model struggles with dynamic data and sourcing information (live information). Agentic search addresses this by involving a search tool.\n",
    "\n",
    "**The process involves:**\n",
    "- Receiving a query and breaking it into sub-questions if needed.\n",
    "- Selecting the best source (e.g. weather API for weather queries).\n",
    "- Extracting and chunking relevant information.\n",
    "- Running a vector search to retrieve top results.\n",
    "- Scoring and filtering the output for relevance.\n",
    "\n",
    "---\n",
    "**Search flowchart**\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "    Query[Query] --> B@{ shape: processes, label: \"Sub-Query\" }\n",
    "    B --> Retrieve[Retrieve]\n",
    "    Retrieve -->A@{ shape: processes, label: \"Source\" }\n",
    "    A --> Scoring[Scoring & Filtering]\n",
    "    Scoring --> Return[Return Top-K Docs]\n",
    "\n",
    "    classDef highlight fill:#a3e3b0,color:#000,stroke:#000\n",
    "    class B,A highlight;\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Source\n",
    "        direction LR\n",
    "        A[Sub-Query] --> B@{ shape: docs, label: \"Chunked Source\" }\n",
    "        B --> C@{ shape: docs, label: \"Top-K Chunks\" }\n",
    "    end\n",
    "\n",
    "    classDef highlight fill:#a3e3b0,color:#000,stroke:#000\n",
    "    class B,C highlight;\n",
    "```\n",
    "---\n",
    "\n",
    "**tavily search example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
    "result = client.search(\"What is in Nvidia's new Blackwell GPU?\",\n",
    "                       include_answer=True)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f343d8",
   "metadata": {},
   "source": [
    "## Persistence in LangGraph\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/persistence/\n",
    "\n",
    "LangGraph has a built-in persistence layer via **checkpointers**, which enable agents to save and restore state.\n",
    "\n",
    "**Checkpointers** snapshot the state of the graph at each \"super-step\" during execution. **Checkpoints** capture:\n",
    "- `config`: Config associated with this checkpoint.\n",
    "- `metadata`: Metadata associated with this checkpoint.\n",
    "- `values`: Values of the state channels at this point in time.\n",
    "- `next`: A tuple of the node names to execute next in the graph.\n",
    "- `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed.\n",
    "  - If the step was previously attempted, it will include error information. \n",
    "  - If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.\n",
    "\n",
    "Checkpoints are saved to a thread, which can be accessed after graph execution.\n",
    "- Saved checkpoints belong to a **thread**, identified by `thread_id`. \n",
    "- Threads and checkpoints unlock powerful capabilities like:\n",
    "    - **Human-in-the-loop** interactions\n",
    "    - **Time travel** (past-state recovery)\n",
    "    - **Fault-tolerance** (resuming after failure)\n",
    "    - **Memory** across runs or conversations\n",
    "- Checkpointers support various backends:\n",
    "  - **In-memory** (for simple scenarios)\n",
    "  - **SQLite** (via `SqliteSaver`)\n",
    "  - Persistent stores like **Redis** and **PostgreSQL**, including those with vector similarity search\n",
    "\n",
    "Core functionality of checkpoints are:\n",
    "- Get state\n",
    "  - `get_state`\n",
    "  - get a state snapshot for a specific `checkpoint_id`\n",
    "- Get state history\n",
    "  - `get_state_history`\n",
    "  - get the full history of the graph execution for a given thread\n",
    "- Replay\n",
    "  - `invoke`\n",
    "  - re-play the previously executed steps before a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps after the checkpoint\n",
    "- Update state\n",
    "  - `update_state`\n",
    "\n",
    "**Available libraries:**\n",
    "- `langgraph-checkpoint`\n",
    "- `langgraph-checkpoint-sqlite`\n",
    "- `langgraph-checkpoint-postgres`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e94cd",
   "metadata": {},
   "source": [
    "\n",
    "## Streaming in LangGraph\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/streaming/\n",
    "\n",
    "LangGraph also offers a streaming system to provide real-time feedbacks.\n",
    "`stream` replaces `invoke`.\n",
    "\n",
    "**Categories:**\n",
    "- **Workflow progress:** get state updates after each graph node is executed\n",
    "- **LLM tokens:** stream language model tokens as they’re generated\n",
    "- **Custom updates:** emit user-defined signals\n",
    "\n",
    "**Supported stream modes**\n",
    "\n",
    "Data can be streamed during graph excution using `stream` or `astream`\n",
    "\n",
    "| Mode | Description |\n",
    "| ---- | ----------- |\n",
    "|values|Streams the full value of the state after each step of the graph.\n",
    "|updates|Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\n",
    "|custom|Streams custom data from inside your graph nodes.\n",
    "|messages|Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.\n",
    "debug|Streams as much information as possible throughout the execution of the graph.\n",
    "\n",
    "## Overview of Persistence and Streaming\n",
    "\n",
    "| Concept | Purpose | Benefits |\n",
    "| ------- | ------- | -------- |\n",
    "| **Persistence** | Saves and restores agent state across execution steps using checkpointers | Enables resilience, time travel, memory retention |\n",
    "| **Streaming**   | Streams state changes, tokens, messages, and custom updates in real time  | Improves transparency, interactivity, and debugging capability |\n",
    "\n",
    "**package**\n",
    "\n",
    "```bash\n",
    "uv add langgraph-checkpoint-sqlite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system:str=\"\"):\n",
    "        self.system:str = system\n",
    "        graph:StateGraph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    \n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "    \n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results: list[ToolMessage] = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t[\"name\"] in self.tools:\n",
    "                print(\"...bad tool name...\")\n",
    "                result = \"bad tool name, retry\"\n",
    "            else:\n",
    "                result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(ToolMessage(\n",
    "                tool_call_id=t[\"id\"],\n",
    "                name=t[\"name\"],\n",
    "                content=str(result)))\n",
    "        print(\"Back to the model\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea94b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "tool = TavilySearchResults(max_results=4)\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    model = ChatOpenAI(model=LLM_MODEL)\n",
    "    agent = Agent(model, [tool], system=prompt, checkpointer=checkpointer)\n",
    "\n",
    "    print(\"First iteration ============\")\n",
    "    messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v['messages'])\n",
    "\n",
    "    print(\"Second iteration ============\")\n",
    "    messages = [HumanMessage(content=\"What about in la?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    tool = TavilySearchResults(max_results=4)\n",
    "    model = ChatOpenAI(model=LLM_MODEL)\n",
    "    agent = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "    messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    async for event in agent.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Empty content in the context of OpenAI means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "                print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f0d0e",
   "metadata": {},
   "source": [
    "- `AsyncSqliteSaver`\n",
    "  - Package has been changed from `aiosqlite` to `sqlite.aio`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3beae",
   "metadata": {},
   "source": [
    "## Human in the Loop\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#key-capabilities\n",
    "\n",
    "In an agent workflow, with the **human-in-the-loop**, user can:\n",
    "- review\n",
    "- edit\n",
    "- approve tool call\n",
    "\n",
    "Additional state information is stored to memory and displayed when using `get_state` or `get_state_history`.\n",
    "- `State` is additionally stored every state transition while previously it was stored at an interrupt or at the end.\n",
    "- These change the command output slightly, but are a useful addtion to the information available.\n",
    "\n",
    "**Key Capability**\n",
    "- Persisent execution state\n",
    "  - Interrupts use LangGraph's persistence layer, which saves the graph state\n",
    "  - `interrupt` (dynamic) and `interrupt_before`/`interrupt_after` (static) can be used to pause a graph\n",
    "- Flexible integration point\n",
    "  - intturpt can be done at any point in a workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ba423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "\"\"\"\n",
    "In previous examples we've annotated the `messages` state key\n",
    "with the default `operator.add` or `+` reducer, which always\n",
    "appends new messages to the end of the existing messages array.\n",
    "\n",
    "Now, to support replacing existing messages, we annotate the\n",
    "`messages` key with a customer reducer function, which replaces\n",
    "messages with the same `id`, and appends them otherwise.\n",
    "\"\"\"\n",
    "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
    "    # assign ids to messages that don't have them\n",
    "    for message in right:\n",
    "        if not message.id:\n",
    "            message.id = str(uuid4())\n",
    "    # merge the new messages with the existing messages\n",
    "    merged = left.copy()\n",
    "    for message in right:\n",
    "        for i, existing in enumerate(merged):\n",
    "            # replace any existing messages with the same id\n",
    "            if existing.id == message.id:\n",
    "                merged[i] = message\n",
    "                break\n",
    "        else:\n",
    "            # append any new messages to the end\n",
    "            merged.append(message)\n",
    "    return merged\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], reduce_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0232b360",
   "metadata": {},
   "source": [
    "- `left`: existing snappshot state that human-in-the-loop is about to added in\n",
    "- `right`: new message to merge into the `left`\n",
    "  - if same id message exists, it'll replace the message with the same id\n",
    "  - if same id message does not exist in `left`, it'll get appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            interrupt_before=[\"action\"]\n",
    "        )\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        print(state)\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98723dbf",
   "metadata": {},
   "source": [
    "- `__init__`\n",
    "  - `interrupt_before` has been added when `graph.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.types import StateSnapshot\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=LLM_MODEL)\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    agent = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "    # 1. Normal stream\n",
    "    if False:\n",
    "        messages = [HumanMessage(content=\"Whats the weather in SF?\")]\n",
    "        thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "        print(\"get state\")\n",
    "        print(agent.graph.get_state(thread))\n",
    "        print(agent.graph.get_state(thread).next)\n",
    "\n",
    "    # 2. Approve tool call\n",
    "    if False:\n",
    "        messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "        thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "        for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "\n",
    "        while agent.graph.get_state(thread).next:\n",
    "            print(\"\\n\", agent.graph.get_state(thread),\"\\n\")\n",
    "            print(\"about to input\")\n",
    "            # This will cause hang in vscode jupyter notebook\n",
    "            _input = input(\"proceed?\")\n",
    "            if _input != \"y\":\n",
    "                print(\"aborting\")\n",
    "                break\n",
    "            for event in agent.graph.stream(None, thread):\n",
    "                for v in event.values():\n",
    "                    print(v)\n",
    "    \n",
    "    # 3. Modify State\n",
    "    if True:\n",
    "        messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "        thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "        for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "        print(\"State before:\", agent.graph.get_state(thread))\n",
    "        current_values = agent.graph.get_state(thread)\n",
    "        print(current_values.values['messages'][-1])\n",
    "        print(current_values.values['messages'][-1].tool_calls)\n",
    "        _id = current_values.values['messages'][-1].tool_calls[0]['id']\n",
    "        current_values.values['messages'][-1].tool_calls = [{\n",
    "            'name': 'tavily_search_results_json',\n",
    "            'args': {'query': 'current weather in Louisiana'},\n",
    "            'id': _id\n",
    "        }]\n",
    "        agent.graph.update_state(thread, current_values.values)\n",
    "        print(\"State after:\", agent.graph.get_state(thread))\n",
    "        for event in agent.graph.stream(None, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "\n",
    "    # Print all current states\n",
    "    states: list[StateSnapshot] = []\n",
    "    for state in agent.graph.get_state_history(thread):\n",
    "        print(state)\n",
    "        print('--')\n",
    "        states.append(state)\n",
    "\n",
    "    # 4. Time Travel\n",
    "    if False:\n",
    "        to_replay = states[-3]\n",
    "        print(\"replay\", to_replay)\n",
    "        for event in agent.graph.stream(None, to_replay.config):\n",
    "            for k, v in event.items():\n",
    "                print(v)\n",
    "\n",
    "    if False:\n",
    "        to_replay = states[-3]\n",
    "        print(\"replay\", to_replay)\n",
    "        _id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
    "        to_replay.values['messages'][-1].tool_calls = [{\n",
    "            'name': 'tavily_search_results_json',\n",
    "            'args': {'query': 'current weather in LA, accuweather'},\n",
    "            'id': _id\n",
    "        }]\n",
    "        branch_state = agent.graph.update_state(to_replay.config, to_replay.values)\n",
    "        for event in agent.graph.stream(None, branch_state):\n",
    "            for k, v in event.items():\n",
    "                if k != \"__end__\":\n",
    "                    print(v)\n",
    "\n",
    "    # 5. Add message to a state at a given time\n",
    "    to_replay = states[-3]\n",
    "    print(\"replay\", to_replay)\n",
    "    _id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
    "    state_update = {\"messages\": [ToolMessage(\n",
    "        tool_call_id=_id,\n",
    "        name=\"tavily_search_results_json\",\n",
    "        content=\"54 degree celcius\",\n",
    "    )]}\n",
    "    branch_and_add = agent.graph.update_state(\n",
    "        to_replay.config, \n",
    "        state_update, \n",
    "        as_node=\"action\"\n",
    "    )\n",
    "    for event in agent.graph.stream(None, branch_and_add):\n",
    "        for k, v in event.items():\n",
    "            print(v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ef138",
   "metadata": {},
   "source": [
    "- each returned state `StateSnapshot` contains full history until the snapshot point\n",
    "- `get_state`\n",
    "  - Get the current state of the graph\n",
    "- `get_state_history`\n",
    "  - Get the history of the state of the graph\n",
    "  \n",
    "**Small Graph Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "dba9e79a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node1, count:0\n",
      "node2, count:1\n",
      "node1, count:2\n",
      "node2, count:3\n",
      "State: StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 4}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab96-6170-8004-1ba9a87b0f9d'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '1', 'step': 4, 'parents': {}}, created_at='2025-08-30T21:04:28.554061+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab95-62fc-8003-f7c0a392ed42'}}, tasks=())\n",
      "State history:\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 4}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab96-6170-8004-1ba9a87b0f9d'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '1', 'step': 4, 'parents': {}}, created_at='2025-08-30T21:04:28.554061+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab95-62fc-8003-f7c0a392ed42'}}, tasks=())\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 3}, next=('Node2',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab95-62fc-8003-f7c0a392ed42'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '1', 'step': 3, 'parents': {}}, created_at='2025-08-30T21:04:28.553690+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab93-6cc2-8002-770d0cf157f3'}}, tasks=(PregelTask(id='cb3eedfc-a5b1-9684-91ee-3f1ac68c5305', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 2}, next=('Node1',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab93-6cc2-8002-770d0cf157f3'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '1', 'step': 2, 'parents': {}}, created_at='2025-08-30T21:04:28.553114+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}}, tasks=(PregelTask(id='9a53951c-5d8a-1243-43cb-48e1cc068039', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 1}, next=('Node2',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '1', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.551538+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8d-65a2-8000-41027d971571'}}, tasks=(PregelTask(id='0e12c30c-47df-1dfb-14cb-ef046b603fda', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'scratch': 'hi', 'count': 0}, next=('Node1',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8d-65a2-8000-41027d971571'}}, metadata={'source': 'loop', 'writes': None, 'thread_id': '1', 'step': 0, 'parents': {}}, created_at='2025-08-30T21:04:28.550482+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab86-6c48-bfff-8552ca52d79e'}}, tasks=(PregelTask(id='3b801c7b-2d6f-a8b6-2074-843c75cd42c8', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'count': 0}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab86-6c48-bfff-8552ca52d79e'}}, metadata={'source': 'input', 'writes': {'__start__': {'count': 0, 'scratch': 'hi'}}, 'thread_id': '1', 'step': -1, 'parents': {}}, created_at='2025-08-30T21:04:28.547789+00:00', parent_config=None, tasks=(PregelTask(id='f33863a9-4832-2c41-a9b4-c2b90a1a5091', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'count': 0, 'scratch': 'hi'}),))\n",
      "State history end\n",
      "Config history:\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab96-6170-8004-1ba9a87b0f9d'}} 4\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab95-62fc-8003-f7c0a392ed42'}} 3\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab93-6cc2-8002-770d0cf157f3'}} 2\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}} 1\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8d-65a2-8000-41027d971571'}} 0\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab86-6c48-bfff-8552ca52d79e'}} 0\n",
      "Config history end\n",
      "states[-3]\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}}\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 1}, next=('Node2',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '1', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.551538+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8d-65a2-8000-41027d971571'}}, tasks=(PregelTask(id='0e12c30c-47df-1dfb-14cb-ef046b603fda', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "node2, count:1\n",
      "node1, count:2\n",
      "node2, count:3\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba1-65ac-8004-20a567487afa'}} 4\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba0-610c-8003-4bb0e701bed5'}} 3\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab9f-60b8-8002-bb53b47547d1'}} 2\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab96-6170-8004-1ba9a87b0f9d'}} 4\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab95-62fc-8003-f7c0a392ed42'}} 3\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab93-6cc2-8002-770d0cf157f3'}} 2\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}} 1\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8d-65a2-8000-41027d971571'}} 0\n",
      "{'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab86-6c48-bfff-8552ca52d79e'}} 0\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 4}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba1-65ac-8004-20a567487afa'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f', 'step': 4, 'parents': {}}, created_at='2025-08-30T21:04:28.558643+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba0-610c-8003-4bb0e701bed5'}}, tasks=()) \n",
      "\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 3}, next=('Node2',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba0-610c-8003-4bb0e701bed5'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f', 'step': 3, 'parents': {}}, created_at='2025-08-30T21:04:28.558149+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab9f-60b8-8002-bb53b47547d1'}}, tasks=(PregelTask(id='8827d5fe-5827-2db4-68a5-bc6ffdf31481', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),)) \n",
      "\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 2}, next=('Node1',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab9f-60b8-8002-bb53b47547d1'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f', 'step': 2, 'parents': {}}, created_at='2025-08-30T21:04:28.557727+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}}, tasks=(PregelTask(id='0554c859-e8cf-7b02-ae29-ed983ecbcd66', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),)) \n",
      "\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 4}, next=(), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab96-6170-8004-1ba9a87b0f9d'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '1', 'step': 4, 'parents': {}}, created_at='2025-08-30T21:04:28.554061+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab95-62fc-8003-f7c0a392ed42'}}, tasks=()) \n",
      "\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 3}, next=('Node2',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab95-62fc-8003-f7c0a392ed42'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '1', 'step': 3, 'parents': {}}, created_at='2025-08-30T21:04:28.553690+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab93-6cc2-8002-770d0cf157f3'}}, tasks=(PregelTask(id='cb3eedfc-a5b1-9684-91ee-3f1ac68c5305', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),)) \n",
      "\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 2}, next=('Node1',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab93-6cc2-8002-770d0cf157f3'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '1', 'step': 2, 'parents': {}}, created_at='2025-08-30T21:04:28.553114+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}}, tasks=(PregelTask(id='9a53951c-5d8a-1243-43cb-48e1cc068039', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),)) \n",
      "\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 1}, next=('Node2',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8f-6f32-8001-4f7b07b3c24f'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '1', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.551538+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8d-65a2-8000-41027d971571'}}, tasks=(PregelTask(id='0e12c30c-47df-1dfb-14cb-ef046b603fda', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),)) \n",
      "\n",
      "StateSnapshot(values={'scratch': 'hi', 'count': 0}, next=('Node1',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab8d-65a2-8000-41027d971571'}}, metadata={'source': 'loop', 'writes': None, 'thread_id': '1', 'step': 0, 'parents': {}}, created_at='2025-08-30T21:04:28.550482+00:00', parent_config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab86-6c48-bfff-8552ca52d79e'}}, tasks=(PregelTask(id='3b801c7b-2d6f-a8b6-2074-843c75cd42c8', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),)) \n",
      "\n",
      "StateSnapshot(values={'count': 0}, next=('__start__',), config={'configurable': {'thread_id': '1', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-ab86-6c48-bfff-8552ca52d79e'}}, metadata={'source': 'input', 'writes': {'__start__': {'count': 0, 'scratch': 'hi'}}, 'thread_id': '1', 'step': -1, 'parents': {}}, created_at='2025-08-30T21:04:28.547789+00:00', parent_config=None, tasks=(PregelTask(id='f33863a9-4832-2c41-a9b4-c2b90a1a5091', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'count': 0, 'scratch': 'hi'}),)) \n",
      "\n",
      "node1, count:0\n",
      "node2, count:1\n",
      "node1, count:2\n",
      "node2, count:3\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abac-6682-8004-1ecccfcf30df'}} 4\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abab-673c-8003-01bcd7653e4a'}} 3\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abaa-6c60-8002-b83938b2f708'}} 2\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}} 1\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}} 0\n",
      "{'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba7-64fc-bfff-2d2207b99337'}} 0\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 1}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.561897+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, tasks=(PregelTask(id='adebb291-239b-610f-1f16-cd9755aabca1', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': -3}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.561897+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, tasks=(PregelTask(id='adebb291-239b-610f-1f16-cd9755aabca1', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "after update state\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': 1}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb2-6ca8-8005-fb50eec49bc8'}}, metadata={'source': 'update', 'writes': {'Node2': {'lnode': 'node_1', 'scratch': 'hello', 'count': -3}}, 'thread_id': '2', 'step': 5, 'parents': {}}, created_at='2025-08-30T21:04:28.565814+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abac-6682-8004-1ecccfcf30df'}}, tasks=(PregelTask(id='0190a9a5-d88d-4a88-e4e0-db9782bcdcf9', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result=None),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 4}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abac-6682-8004-1ecccfcf30df'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 4, 'parents': {}}, created_at='2025-08-30T21:04:28.563205+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abab-673c-8003-01bcd7653e4a'}}, tasks=())\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 3}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abab-673c-8003-01bcd7653e4a'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 3, 'parents': {}}, created_at='2025-08-30T21:04:28.562811+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abaa-6c60-8002-b83938b2f708'}}, tasks=(PregelTask(id='2896914a-bc02-ed39-16a4-293af599d6dd', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 2}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abaa-6c60-8002-b83938b2f708'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 2, 'parents': {}}, created_at='2025-08-30T21:04:28.562536+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, tasks=(PregelTask(id='395eb242-88cd-3280-61bb-d64242e9298a', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 1}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.561897+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, tasks=(PregelTask(id='adebb291-239b-610f-1f16-cd9755aabca1', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'scratch': 'hi', 'count': 0}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, metadata={'source': 'loop', 'writes': None, 'thread_id': '2', 'step': 0, 'parents': {}}, created_at='2025-08-30T21:04:28.561583+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba7-64fc-bfff-2d2207b99337'}}, tasks=(PregelTask(id='018e1a16-5c1f-4d87-d414-dd627858a58c', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'count': 0}, next=('__start__',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba7-64fc-bfff-2d2207b99337'}}, metadata={'source': 'input', 'writes': {'__start__': {'count': 0, 'scratch': 'hi'}}, 'thread_id': '2', 'step': -1, 'parents': {}}, created_at='2025-08-30T21:04:28.561122+00:00', parent_config=None, tasks=(PregelTask(id='b3676ccc-7a60-747e-de21-b438b32c84e7', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'count': 0, 'scratch': 'hi'}),))\n",
      "after update state as node\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': -2}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb5-6f2a-8006-bb50065e82f8'}}, metadata={'source': 'update', 'writes': {'Node1': {'lnode': 'node_1', 'scratch': 'hello', 'count': -3}}, 'thread_id': '2', 'step': 6, 'parents': {}}, created_at='2025-08-30T21:04:28.567108+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb2-6ca8-8005-fb50eec49bc8'}}, tasks=(PregelTask(id='8a36a20a-ca0a-e318-1b4e-3435789147b8', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result=None),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': 1}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb2-6ca8-8005-fb50eec49bc8'}}, metadata={'source': 'update', 'writes': {'Node2': {'lnode': 'node_1', 'scratch': 'hello', 'count': -3}}, 'thread_id': '2', 'step': 5, 'parents': {}}, created_at='2025-08-30T21:04:28.565814+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abac-6682-8004-1ecccfcf30df'}}, tasks=(PregelTask(id='0190a9a5-d88d-4a88-e4e0-db9782bcdcf9', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result=None),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 4}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abac-6682-8004-1ecccfcf30df'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 4, 'parents': {}}, created_at='2025-08-30T21:04:28.563205+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abab-673c-8003-01bcd7653e4a'}}, tasks=())\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 3}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abab-673c-8003-01bcd7653e4a'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 3, 'parents': {}}, created_at='2025-08-30T21:04:28.562811+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abaa-6c60-8002-b83938b2f708'}}, tasks=(PregelTask(id='2896914a-bc02-ed39-16a4-293af599d6dd', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 2}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abaa-6c60-8002-b83938b2f708'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 2, 'parents': {}}, created_at='2025-08-30T21:04:28.562536+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, tasks=(PregelTask(id='395eb242-88cd-3280-61bb-d64242e9298a', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 1}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.561897+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, tasks=(PregelTask(id='adebb291-239b-610f-1f16-cd9755aabca1', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'scratch': 'hi', 'count': 0}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, metadata={'source': 'loop', 'writes': None, 'thread_id': '2', 'step': 0, 'parents': {}}, created_at='2025-08-30T21:04:28.561583+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba7-64fc-bfff-2d2207b99337'}}, tasks=(PregelTask(id='018e1a16-5c1f-4d87-d414-dd627858a58c', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'count': 0}, next=('__start__',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba7-64fc-bfff-2d2207b99337'}}, metadata={'source': 'input', 'writes': {'__start__': {'count': 0, 'scratch': 'hi'}}, 'thread_id': '2', 'step': -1, 'parents': {}}, created_at='2025-08-30T21:04:28.561122+00:00', parent_config=None, tasks=(PregelTask(id='b3676ccc-7a60-747e-de21-b438b32c84e7', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'count': 0, 'scratch': 'hi'}),))\n",
      "Invoke\n",
      "node2, count:-2\n",
      "node1, count:-1\n",
      "node2, count:0\n",
      "node1, count:1\n",
      "node2, count:2\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hello', 'count': 3}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abbd-69b4-800b-09f4cee9995f'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 11, 'parents': {}}, created_at='2025-08-30T21:04:28.570249+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abbc-69c4-800a-dcb5e998a983'}}, tasks=())\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': 2}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abbc-69c4-800a-dcb5e998a983'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 10, 'parents': {}}, created_at='2025-08-30T21:04:28.569841+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abbb-6ec0-8009-b88f3e84d9a1'}}, tasks=(PregelTask(id='d001a65a-0c03-62d1-8795-a019a137f327', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hello', 'count': 1}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abbb-6ec0-8009-b88f3e84d9a1'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 9, 'parents': {}}, created_at='2025-08-30T21:04:28.569561+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abba-6d90-8008-478a4dbc08ea'}}, tasks=(PregelTask(id='c3bb63c3-ecd0-1743-39e6-c05c5d851076', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': 0}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abba-6d90-8008-478a4dbc08ea'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 8, 'parents': {}}, created_at='2025-08-30T21:04:28.569118+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb9-6e5e-8007-6ac1b0b09208'}}, tasks=(PregelTask(id='029963d4-c3ca-4364-b8d2-5ecb6501d63d', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hello', 'count': -1}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb9-6e5e-8007-6ac1b0b09208'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 7, 'parents': {}}, created_at='2025-08-30T21:04:28.568729+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb5-6f2a-8006-bb50065e82f8'}}, tasks=(PregelTask(id='89998002-4ffb-c2fb-f84b-2cf665c582d3', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': -2}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb5-6f2a-8006-bb50065e82f8'}}, metadata={'source': 'update', 'writes': {'Node1': {'lnode': 'node_1', 'scratch': 'hello', 'count': -3}}, 'thread_id': '2', 'step': 6, 'parents': {}}, created_at='2025-08-30T21:04:28.567108+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb2-6ca8-8005-fb50eec49bc8'}}, tasks=(PregelTask(id='8a36a20a-ca0a-e318-1b4e-3435789147b8', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hello', 'count': 1}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abb2-6ca8-8005-fb50eec49bc8'}}, metadata={'source': 'update', 'writes': {'Node2': {'lnode': 'node_1', 'scratch': 'hello', 'count': -3}}, 'thread_id': '2', 'step': 5, 'parents': {}}, created_at='2025-08-30T21:04:28.565814+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abac-6682-8004-1ecccfcf30df'}}, tasks=(PregelTask(id='0190a9a5-d88d-4a88-e4e0-db9782bcdcf9', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result=None),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 4}, next=(), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abac-6682-8004-1ecccfcf30df'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 4, 'parents': {}}, created_at='2025-08-30T21:04:28.563205+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abab-673c-8003-01bcd7653e4a'}}, tasks=())\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 3}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abab-673c-8003-01bcd7653e4a'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 3, 'parents': {}}, created_at='2025-08-30T21:04:28.562811+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abaa-6c60-8002-b83938b2f708'}}, tasks=(PregelTask(id='2896914a-bc02-ed39-16a4-293af599d6dd', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_2', 'scratch': 'hi', 'count': 2}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-abaa-6c60-8002-b83938b2f708'}}, metadata={'source': 'loop', 'writes': {'Node2': {'lnode': 'node_2', 'count': 1}}, 'thread_id': '2', 'step': 2, 'parents': {}}, created_at='2025-08-30T21:04:28.562536+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, tasks=(PregelTask(id='395eb242-88cd-3280-61bb-d64242e9298a', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'lnode': 'node_1', 'scratch': 'hi', 'count': 1}, next=('Node2',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba9-6374-8001-3245b6965815'}}, metadata={'source': 'loop', 'writes': {'Node1': {'lnode': 'node_1', 'count': 1}}, 'thread_id': '2', 'step': 1, 'parents': {}}, created_at='2025-08-30T21:04:28.561897+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, tasks=(PregelTask(id='adebb291-239b-610f-1f16-cd9755aabca1', name='Node2', path=('__pregel_pull', 'Node2'), error=None, interrupts=(), state=None, result={'lnode': 'node_2', 'count': 1}),))\n",
      "StateSnapshot(values={'scratch': 'hi', 'count': 0}, next=('Node1',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba8-6712-8000-cb510060f194'}}, metadata={'source': 'loop', 'writes': None, 'thread_id': '2', 'step': 0, 'parents': {}}, created_at='2025-08-30T21:04:28.561583+00:00', parent_config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba7-64fc-bfff-2d2207b99337'}}, tasks=(PregelTask(id='018e1a16-5c1f-4d87-d414-dd627858a58c', name='Node1', path=('__pregel_pull', 'Node1'), error=None, interrupts=(), state=None, result={'lnode': 'node_1', 'count': 1}),))\n",
      "StateSnapshot(values={'count': 0}, next=('__start__',), config={'configurable': {'thread_id': '2', 'checkpoint_ns': '', 'checkpoint_id': '1f085e4e-aba7-64fc-bfff-2d2207b99337'}}, metadata={'source': 'input', 'writes': {'__start__': {'count': 0, 'scratch': 'hi'}}, 'thread_id': '2', 'step': -1, 'parents': {}}, created_at='2025-08-30T21:04:28.561122+00:00', parent_config=None, tasks=(PregelTask(id='b3676ccc-7a60-747e-de21-b438b32c84e7', name='__start__', path=('__pregel_pull', '__start__'), error=None, interrupts=(), state=None, result={'count': 0, 'scratch': 'hi'}),))\n"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.runnables.config import RunnableConfig\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    lnode: str\n",
    "    scratch: str\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "def node1(state: AgentState):\n",
    "    print(f\"node1, count:{state['count']}\")\n",
    "    return {\n",
    "        \"lnode\": \"node_1\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "\n",
    "def node2(state: AgentState):\n",
    "    print(f\"node2, count:{state['count']}\")\n",
    "    return {\n",
    "        \"lnode\": \"node_2\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "\n",
    "def should_continue(state):\n",
    "    return state[\"count\"] < 3\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"Node1\", node1)\n",
    "builder.add_node(\"Node2\", node2)\n",
    "\n",
    "builder.add_edge(\"Node1\", \"Node2\")\n",
    "builder.add_conditional_edges(\"Node2\", \n",
    "                              should_continue, \n",
    "                              {True: \"Node1\", False: END})\n",
    "builder.set_entry_point(\"Node1\")\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "    # Run\n",
    "    thread: RunnableConfig = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "    graph.invoke(input={\"count\":0, \"scratch\":\"hi\"},\n",
    "                 config=thread)\n",
    "\n",
    "    # State\n",
    "    print(\"State:\", graph.get_state(thread))\n",
    "\n",
    "    # State history\n",
    "    print(\"State history:\")\n",
    "    for state in graph.get_state_history(thread):\n",
    "        print(state)\n",
    "    print(\"State history end\")\n",
    "\n",
    "    # Config history\n",
    "    print(\"Config history:\")\n",
    "    states = []\n",
    "    for state in graph.get_state_history(thread):\n",
    "        states.append(state.config)\n",
    "        print(state.config, state.values['count'])\n",
    "    print(\"Config history end\")\n",
    "    print(\"states[-3]\")\n",
    "    print(states[-3])\n",
    "    print(graph.get_state(states[-3]))\n",
    "\n",
    "    # Go back in time\n",
    "    graph.invoke(None, states[-3])\n",
    "    thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "    for state in graph.get_state_history(thread):\n",
    "        print(state.config, state.values['count'])\n",
    "\n",
    "    thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "    for state in graph.get_state_history(thread):\n",
    "        print(state,\"\\n\")\n",
    "\n",
    "    # Modify state\n",
    "    thread2 = {\"configurable\": {\"thread_id\": str(2)}}\n",
    "    graph.invoke({\"count\":0, \"scratch\":\"hi\"},thread2)\n",
    "\n",
    "    states2 = []\n",
    "    for state in graph.get_state_history(thread2):\n",
    "        states2.append(state.config)\n",
    "        print(state.config, state.values['count'])   \n",
    "\n",
    "    save_state = graph.get_state(states2[-3])\n",
    "    print(save_state)\n",
    "\n",
    "    save_state.values[\"count\"] = -3\n",
    "    save_state.values[\"scratch\"] = \"hello\"\n",
    "    print(save_state)\n",
    "\n",
    "    graph.update_state(thread2,save_state.values)\n",
    "\n",
    "    print(\"after update state\")\n",
    "    for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "        print(state)\n",
    "\n",
    "    # Modify state as node\n",
    "    print(\"after update state as node\")\n",
    "    graph.update_state(thread2,save_state.values, as_node=\"Node1\")\n",
    "    for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "        print(state)\n",
    "\n",
    "    print(\"Invoke\")\n",
    "    graph.invoke(None,thread2)\n",
    "    for state in graph.get_state_history(thread2):\n",
    "        print(state)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4b8c",
   "metadata": {},
   "source": [
    "## Essay writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f981a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': \"### Essay Outline: The Difference Between LangChain and LangSmith\\n\\n#### I. Introduction\\n   A. Definition of LangChain and LangSmith\\n      1. Brief overview of what each tool is designed for\\n      2. Importance of understanding the differences in the context of AI and language processing\\n   B. Purpose of the essay\\n      1. To compare and contrast LangChain and LangSmith\\n      2. To provide insights into their respective use cases and functionalities\\n\\n#### II. Overview of LangChain\\n   A. Description of LangChain\\n      1. Purpose and primary functionalities\\n      2. Key features (e.g., modularity, integration with language models)\\n   B. Use cases\\n      1. Applications in AI development\\n      2. Examples of projects or industries utilizing LangChain\\n\\n#### III. Overview of LangSmith\\n   A. Description of LangSmith\\n      1. Purpose and primary functionalities\\n      2. Key features (e.g., user interface, analytics capabilities)\\n   B. Use cases\\n      1. Applications in AI development\\n      2. Examples of projects or industries utilizing LangSmith\\n\\n#### IV. Comparative Analysis\\n   A. Core Differences\\n      1. Functionality and design philosophy\\n         a. LangChain's focus on modularity vs. LangSmith's user-centric design\\n      2. Target audience and user experience\\n         a. Developers vs. non-technical users\\n   B. Performance and Scalability\\n      1. How each tool handles large datasets and complex tasks\\n      2. Speed and efficiency comparisons\\n   C. Integration and Compatibility\\n      1. Compatibility with other tools and platforms\\n      2. Ease of integration into existing workflows\\n\\n#### V. Strengths and Weaknesses\\n   A. Strengths of LangChain\\n      1. Flexibility and customization options\\n      2. Strong community support and resources\\n   B. Weaknesses of LangChain\\n      1. Steeper learning curve for new users\\n   C. Strengths of LangSmith\\n      1. User-friendly interface and accessibility\\n      2. Robust analytics and reporting features\\n   D. Weaknesses of LangSmith\\n      1. Limited customization compared to LangChain\\n\\n#### VI. Conclusion\\n   A. Summary of key points discussed\\n   B. Final thoughts on choosing between LangChain and LangSmith\\n      1. Considerations based on user needs and project requirements\\n   C. Future outlook for both tools in the evolving landscape of AI and language processing\\n\\n### Notes/Instructions:\\n- Ensure that each section is well-researched and supported by relevant examples and data.\\n- Use clear and concise language to explain technical concepts, making them accessible to a broader audience.\\n- Include citations and references where necessary to support claims and provide further reading.\\n- Consider adding visuals or diagrams in the comparative analysis section to enhance understanding.\"}}\n",
      "{'research_plan': {'content': ['In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'In summary, Langchain is easier to adopt with a strong ecosystem for beginners and intermediate users, while Langsmith offers deeper', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', '# Get started with LangSmith **LangSmith** is a platform for building production-grade LLM applications. Evaluate your application over production traffic — score application performance and get human feedback on your data. This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith’s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production. The LangSmith SDK and UI make building and running high-quality evaluations easy. * Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.', 'LangSmith is essential for debugging, testing, and monitoring LangChain applications through a unified platform for inspecting chains, tracking', 'LangChain excels in scenarios where you need to quickly prototype LLM applications, integrate multiple data sources, or create applications that']}}\n",
      "{'generate': {'draft': \"### The Difference Between LangChain and LangSmith\\n\\n#### I. Introduction\\nIn the rapidly evolving field of artificial intelligence (AI) and language processing, tools like LangChain and LangSmith have emerged as pivotal resources for developers. LangChain is an open-source framework designed to facilitate the creation of large language model (LLM) applications, while LangSmith provides a comprehensive platform for debugging, monitoring, and enhancing LLM-powered agents. Understanding the differences between these two tools is crucial for developers looking to leverage their capabilities effectively. This essay aims to compare and contrast LangChain and LangSmith, shedding light on their respective functionalities and use cases.\\n\\n#### II. Overview of LangChain\\nLangChain serves as a robust framework that empowers developers to build LLM applications with efficiency and flexibility. Its primary functionalities include modular chains and sequences, prompt templates, and an agent framework that allows for seamless integration with various data sources. The modularity of LangChain enables developers to create customized applications tailored to specific needs, making it particularly valuable in AI development. For instance, industries such as finance and healthcare have utilized LangChain to develop applications that require complex data processing and natural language understanding, showcasing its versatility in real-world scenarios.\\n\\n#### III. Overview of LangSmith\\nIn contrast, LangSmith is designed to enhance the development and management of LLM applications through its user-friendly interface and powerful analytics capabilities. It provides tools for debugging, monitoring, and evaluating LLM-powered agents, making it an essential resource for developers seeking to optimize their applications. LangSmith's observability features allow users to gain insights throughout all stages of application development, from prototyping to production. For example, companies in the customer service sector have leveraged LangSmith to improve the performance of chatbots, utilizing its analytics to refine prompts and enhance user interactions.\\n\\n#### IV. Comparative Analysis\\nThe core differences between LangChain and LangSmith lie in their functionality and design philosophy. LangChain emphasizes modularity and flexibility, catering primarily to developers who seek to create highly customized applications. In contrast, LangSmith adopts a user-centric design, making it accessible to both technical and non-technical users. When it comes to performance and scalability, LangChain excels in handling large datasets and complex tasks, while LangSmith focuses on providing robust analytics and monitoring capabilities. Integration and compatibility also differ; LangChain offers extensive community support and resources for integration with various tools, whereas LangSmith provides a managed cloud service that simplifies the integration process for users.\\n\\n#### V. Strengths and Weaknesses\\nLangChain's strengths lie in its flexibility and customization options, supported by a strong community that fosters collaboration and resource sharing. However, it presents a steeper learning curve for new users, which may deter those unfamiliar with programming. On the other hand, LangSmith's user-friendly interface and robust analytics make it an attractive option for users seeking ease of use. Nevertheless, its limited customization compared to LangChain may be a drawback for developers looking for more tailored solutions.\\n\\n#### VI. Conclusion\\nIn summary, both LangChain and LangSmith offer unique advantages and functionalities that cater to different user needs in the realm of LLM application development. LangChain is ideal for developers seeking flexibility and customization, while LangSmith excels in providing user-friendly tools for monitoring and evaluation. Ultimately, the choice between LangChain and LangSmith should be guided by the specific requirements of the project and the technical expertise of the user. As the landscape of AI and language processing continues to evolve, both tools are likely to play significant roles in shaping the future of LLM applications.\", 'revision_number': 2}}\n",
      "{'reflect': {'critique': '### Critique and Recommendations\\n\\nYour essay provides a clear and structured comparison between LangChain and LangSmith, effectively outlining their functionalities, strengths, and weaknesses. However, there are several areas where you could enhance the depth, clarity, and overall impact of your submission. Below are detailed recommendations:\\n\\n#### 1. **Length and Depth**\\n- **Recommendation**: Aim for a more in-depth analysis, particularly in the comparative analysis section. Consider expanding this section to include specific examples or case studies that illustrate how each tool has been applied in real-world scenarios. This could help to ground your arguments in practical applications and provide a richer context for your comparisons.\\n- **Target Length**: Consider extending the essay to around 1,500-2,000 words to allow for a more comprehensive exploration of each tool, including user testimonials or expert opinions if available.\\n\\n#### 2. **Clarity and Precision**\\n- **Recommendation**: While your writing is generally clear, some sentences could be more concise. For example, the phrase \"tools like LangChain and LangSmith have emerged as pivotal resources for developers\" could be simplified to \"LangChain and LangSmith are pivotal tools for developers.\" This would enhance readability.\\n- **Action**: Review each paragraph for opportunities to tighten your language and eliminate redundancy.\\n\\n#### 3. **Style and Tone**\\n- **Recommendation**: The tone is mostly formal and appropriate for an academic essay. However, consider incorporating a more engaging introduction that captures the reader\\'s interest. You might start with a compelling statistic or a brief anecdote about the impact of LLMs in industry.\\n- **Action**: Revise the introduction to include a hook that draws the reader in, perhaps by discussing the rapid growth of AI applications in everyday life.\\n\\n#### 4. **Comparative Analysis**\\n- **Recommendation**: The comparative analysis section could benefit from a more structured approach. Consider using a table or bullet points to clearly delineate the differences in functionality, user experience, and target audience between LangChain and LangSmith. This visual aid can help readers quickly grasp the distinctions.\\n- **Action**: Create a comparison table that highlights key features, strengths, and weaknesses side by side.\\n\\n#### 5. **Examples and Case Studies**\\n- **Recommendation**: While you mention industries that use LangChain and LangSmith, providing specific examples or case studies would strengthen your arguments. For instance, you could detail a particular application developed using LangChain or a success story from a company that improved its chatbot performance with LangSmith.\\n- **Action**: Research and include at least one detailed case study for each tool to illustrate their practical applications.\\n\\n#### 6. **Conclusion**\\n- **Recommendation**: The conclusion summarizes the main points well, but it could be more impactful. Consider discussing future trends in AI and how these tools might evolve or adapt to meet changing demands. This would provide a forward-looking perspective that could resonate with readers.\\n- **Action**: Expand the conclusion to include insights on the future of LLM applications and the potential developments in both LangChain and LangSmith.\\n\\n#### 7. **Citations and References**\\n- **Recommendation**: If you reference specific tools, case studies, or statistics, ensure that you provide citations. This adds credibility to your essay and allows readers to explore the sources further.\\n- **Action**: Include a references section at the end of your essay, citing any sources you used for information or examples.\\n\\nBy addressing these recommendations, you can enhance the clarity, depth, and overall effectiveness of your essay. Good luck with your revisions!'}}\n",
      "{'research_critique': {'content': ['In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'In summary, Langchain is easier to adopt with a strong ecosystem for beginners and intermediate users, while Langsmith offers deeper', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', '# Get started with LangSmith **LangSmith** is a platform for building production-grade LLM applications. Evaluate your application over production traffic — score application performance and get human feedback on your data. This is where LangSmith can help! LangSmith has LLM-native observability, allowing you to get meaningful insights from your application. LangSmith’s observability features have you covered throughout all stages of application development - from prototyping, to beta testing, to production. The LangSmith SDK and UI make building and running high-quality evaluations easy. * Quickly assess the performance of your application using our off-the-shelf evaluators as a starting point. LangSmith provides a set of tools designed to enable and facilitate prompt engineering to help you find the perfect prompt for your application.', 'LangSmith is essential for debugging, testing, and monitoring LangChain applications through a unified platform for inspecting chains, tracking', 'LangChain excels in scenarios where you need to quickly prototype LLM applications, integrate multiple data sources, or create applications that', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'In summary, Langchain is easier to adopt with a strong ecosystem for beginners and intermediate users, while Langsmith offers deeper', 'LangSmith is essential for debugging, testing ... LangChain in real-world applications to streamline operations and optimize performance.', 'LangChain is an open-source framework designed to simplify the development of applications powered by large language models (LLMs).', '# The Near Future of AI in Software Development: Trends to Watch in 2025 and Beyond From AI-native development platforms to the next generation of cybersecurity, the landscape of software engineering is rapidly changing. In 2025, AI-native development platforms are set to redefine how software is built. These platforms are designed from the ground up to leverage AI, integrating machine learning models directly into the development environment. AI-native platforms are not just tools, they are co-developers. AI-powered low-code/no-code platforms democratize software development, allowing non-technical stakeholders to participate directly in the prototyping process. As concerns about data privacy and latency grow, local AI is emerging as a powerful trend in software development. From autonomous coding agents to AI-native platforms, the possibilities are limitless.', 'The Top 5 AI Trends for Developers - DevOps.com Powered by advancements in AI models and available tools, today’s AI assistants are excelling in agent mode and actually doing things as a developer would, instead of merely suggesting. AI models’ training is often time-boxed, and for newer or updated content, developers are now easily able to direct AI to look up the latest before acting on prompts. Filed Under: AI, Blogs, Business of DevOps, Contributed Content, Social - Facebook, Social - LinkedIn, Social - XTagged With: Agentic Workflows, AI Customization, generative AI, Model Context Protocol Image 6: DevSecOps: Cracking the Code - The Agentic AI Imperative for Cloud-Native AppSec']}}\n",
      "{'generate': {'draft': \"### The Difference Between LangChain and LangSmith\\n\\n#### I. Introduction\\nIn the rapidly evolving landscape of artificial intelligence (AI) and language processing, tools like LangChain and LangSmith have emerged as pivotal resources for developers. LangChain is an open-source framework designed to facilitate the creation of applications powered by large language models (LLMs), while LangSmith provides a platform for debugging, monitoring, and enhancing LLM-powered agents. Understanding the differences between these two tools is crucial for developers looking to leverage AI effectively in their projects. This essay aims to compare and contrast LangChain and LangSmith, providing insights into their respective functionalities and use cases.\\n\\n#### II. Overview of LangChain\\nLangChain is primarily focused on simplifying the development of LLM applications. Its modular design allows developers to create applications efficiently by utilizing various components such as prompt templates, agent frameworks, and data connectors. Key features of LangChain include its support for multiple language models and community integrations, which enhance its versatility. LangChain is particularly beneficial in scenarios where rapid prototyping is essential, making it a popular choice among developers in industries such as finance, healthcare, and education. For instance, a financial institution might use LangChain to develop a chatbot that assists customers with inquiries, leveraging its modular capabilities to integrate various data sources seamlessly.\\n\\n#### III. Overview of LangSmith\\nIn contrast, LangSmith is designed to provide a comprehensive platform for building production-grade LLM applications. It offers tools for debugging, monitoring, and evaluating LLM-powered agents, along with a user-friendly web interface. LangSmith's observability features allow developers to gain meaningful insights throughout the application development lifecycle, from prototyping to production. The platform's analytics capabilities enable users to assess application performance and gather human feedback effectively. LangSmith is particularly valuable in industries where application reliability and performance are critical, such as e-commerce and customer service. For example, an e-commerce company might utilize LangSmith to monitor the performance of its virtual shopping assistant, ensuring it meets customer expectations.\\n\\n#### IV. Comparative Analysis\\nThe core differences between LangChain and LangSmith lie in their functionality and design philosophy. LangChain emphasizes modularity and flexibility, catering primarily to developers who seek customization options. In contrast, LangSmith adopts a user-centric design, making it accessible to both technical and non-technical users. This distinction influences the target audience for each tool, with LangChain appealing more to developers and LangSmith attracting a broader range of users, including project managers and business analysts.\\n\\nWhen it comes to performance and scalability, LangChain excels in handling large datasets and complex tasks due to its modular architecture. However, LangSmith's robust analytics and monitoring capabilities provide valuable insights that can enhance performance optimization. In terms of integration and compatibility, LangChain offers extensive support for various tools and platforms, while LangSmith focuses on providing a seamless user experience within its ecosystem.\\n\\n#### V. Strengths and Weaknesses\\nLangChain's strengths lie in its flexibility and strong community support, which provides developers with ample resources for troubleshooting and innovation. However, its steeper learning curve may pose challenges for new users. On the other hand, LangSmith's user-friendly interface and powerful analytics features make it an attractive option for those seeking ease of use. Nevertheless, its limited customization options compared to LangChain may deter developers looking for more tailored solutions.\\n\\n#### VI. Conclusion\\nIn summary, both LangChain and LangSmith offer unique advantages and cater to different user needs within the AI and language processing landscape. LangChain is ideal for developers seeking flexibility and customization, while LangSmith excels in providing a user-friendly experience with robust monitoring capabilities. Ultimately, the choice between LangChain and LangSmith should be guided by the specific requirements of the project and the technical expertise of the users involved. As the field of AI continues to evolve, both tools are likely to play significant roles in shaping the future of language model applications.\", 'revision_number': 3}}\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gradio'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[52]\u001b[39m\u001b[32m, line 153\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m    152\u001b[39m warnings.filterwarnings(\u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mhelper\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ewriter, writer_gui\n\u001b[32m    155\u001b[39m MultiAgent = ewriter()\n\u001b[32m    156\u001b[39m app = writer_gui(MultiAgent.graph)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/dev/repo/study_group/LangGraph_AI_Agent/helper.py:152\u001b[39m\n\u001b[32m    149\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m END\n\u001b[32m    150\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mreflect\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgradio\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgr\u001b[39;00m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtime\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwriter_gui\u001b[39;00m( ):\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'gradio'"
     ]
    }
   ],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "\n",
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}\n",
    "\n",
    "def research_plan_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state.get(\"content\", [])\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}\n",
    "\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}\n",
    "\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    # from IPython.display import Image\n",
    "    # Image(graph.get_graph().draw_png())\n",
    "\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for s in graph.stream({\n",
    "        \"task\": \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)\n",
    "\n",
    "    ## Essay Writer Interface\n",
    "    # import warnings\n",
    "    # warnings.filterwarnings(\"ignore\")\n",
    "    # from helper import ewriter, writer_gui\n",
    "\n",
    "    # MultiAgent = ewriter()\n",
    "    # app = writer_gui(MultiAgent.graph)\n",
    "    # app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8640c211",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
