{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48688dd8",
   "metadata": {},
   "source": [
    "# ReAct Agent\n",
    "\n",
    "> https://arxiv.org/abs/2210.03629\n",
    "\n",
    "ReAct stands for reasoning and acting.\n",
    "\n",
    "the **ReAct agent combines reasoning and acting**: it thinks about the problem, chooses actions using tools, observes the results, and then produces a final answer.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    A[Reasoning</br>Traces] --> |thought| B[LLM]\n",
    "    B --> A\n",
    "\n",
    "    B --> |Action| C[Env]\n",
    "    C --> |Observation| B\n",
    "```\n",
    "\n",
    "## ReAct Agent Workflow\n",
    "\n",
    "1. **System Message Setup**\n",
    "    - The agent requires a very specific system prompt to define how it should operate.\n",
    "2. **Loop Execution**\n",
    "    - The agent cycles through a structured loop of:\n",
    "        - **Thought** → internal reasoning about the question.\n",
    "        - **Action** → selecting and running an available tool/function.\n",
    "        - **Pause** → marking the completion of the action.\n",
    "        - **Observation** → capturing the result of the action.\n",
    "3. **Answer Production**\n",
    "    - Once the loop finishes, the agent outputs an answer.\n",
    "4. **Tools Provided**\n",
    "    - The agent is given access to specific tools/functions.\n",
    "    - Example tools in the transcript:\n",
    "        - `calculate` → evaluates string expressions.\n",
    "        - `average_dog_weight` → returns mock weights for certain breeds.\n",
    "5. **Traceability**\n",
    "    - The workflow shows step-by-step traces:\n",
    "      - Incoming question → Thought → Action → Pause → Observation → Answer.\n",
    "6. **Example Run**\n",
    "   * Input: *“How much does a toy poodle weigh?”*\n",
    "   * Thought: *“I should look up the dog’s weight using average dog weight for a toy poodle.”*\n",
    "   * Action: *`average_dog_weight, toy poodle`*\n",
    "   * Observation: returns result.\n",
    "   * Final Answer is then produced from the observation.\n",
    "\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    A[Start / Incoming question] --> B[System message defines ReAct protocol]\n",
    "    B --> C{Answer ready?}\n",
    "\n",
    "    C -- \"No\" --> D[Thought: reason about the question]\n",
    "    D --> E[Action: run a chosen tool/function]\n",
    "    E --> F[Pause: mark action completion]\n",
    "    F --> G[Observation: record tool result]\n",
    "    G --> C\n",
    "\n",
    "    C -- \"Yes\" --> H[Compose final answer]\n",
    "    H --> I[Output answer]\n",
    "\n",
    "    %% Tooling context\n",
    "    subgraph Tools available to the agent\n",
    "        T1[\"calculate(expr)\"]\n",
    "        T2[\"average_dog_weight(breed)\"]\n",
    "    end\n",
    "\n",
    "    %% Action may call any tool\n",
    "    E -. may call .-> T1\n",
    "    E -. may call .-> T2\n",
    "    T1 --> G\n",
    "    T2 --> G\n",
    "```\n",
    "\n",
    "## Setup OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c77c0f49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup OpenAI Tavily API key\n",
    "def setup_openai_api_key():\n",
    "    from dotenv import load_dotenv\n",
    "    _ = load_dotenv()\n",
    "setup_openai_api_key()\n",
    "\n",
    "LLM_MODEL:str = \"gpt-4o-mini\"\n",
    "# LLM_MODEL:str = \"gpt-3.5-turbo\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40c9917",
   "metadata": {},
   "source": [
    "**Agent class**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c60e649",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "from openai.types.chat.chat_completion import ChatCompletion\n",
    "client = openai.OpenAI()\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, system:str=\"\"):\n",
    "        self.system = system\n",
    "        self.messages:list[dict[str,str]] = []\n",
    "        if self.system:\n",
    "            self.messages.append({\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system\n",
    "            })\n",
    "    def __call__(self, message:str):\n",
    "        self.messages.append({\n",
    "            \"role\": \"user\",\n",
    "            \"content\": message\n",
    "        })\n",
    "        result = self.execute()\n",
    "        self.messages.append({\n",
    "            \"role\": \"assistant\",\n",
    "            \"content\": result\n",
    "        })\n",
    "        return result\n",
    "\n",
    "    def execute(self):\n",
    "        completion:ChatCompletion = client.chat.completions.create(\n",
    "            model=LLM_MODEL,\n",
    "            temperature=0,\n",
    "            messages=self.messages)\n",
    "        return completion.choices[0].message.content\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19827526",
   "metadata": {},
   "source": [
    "- `Agent.__call__`\n",
    "  - add user message into agent's message array\n",
    "  - add LLM response to agent's message array\n",
    "- `OpenAI.chat.completions.create`\n",
    "  - `temperature=0`: for the deterministic response\n",
    "  - `messages=self.messages`: list of messages comprising the conversation so far.\n",
    "\n",
    "**Agent prompt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb59c7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "You run in a loop of Thought, Action, PAUSE, Observation.\n",
    "At the end of the loop you output an Answer\n",
    "Use Thought to describe your thoughts about the question you have been asked.\n",
    "Use Action to run one of the actions available to you - then return PAUSE.\n",
    "Observation will be the result of running those actions.\n",
    "\n",
    "Your available actions are:\n",
    "\n",
    "calculate:\n",
    "e.g. calculate: 4 * 7 / 3\n",
    "Runs a calculation and returns the number - uses Python so be sure to use floating point syntax if necessary\n",
    "\n",
    "average_dog_weight:\n",
    "e.g. average_dog_weight: Collie\n",
    "returns average weight of a dog when given the breed\n",
    "\n",
    "Example session:\n",
    "\n",
    "Question: How much does a Bulldog weigh?\n",
    "Thought: I should look the dogs weight using average_dog_weight\n",
    "Action: average_dog_weight: Bulldog\n",
    "PAUSE\n",
    "\n",
    "You will be called again with this:\n",
    "\n",
    "Observation: A Bulldog weights 51 lbs\n",
    "\n",
    "You then output:\n",
    "\n",
    "Answer: A bulldog weights 51 lbs\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e49129a",
   "metadata": {},
   "source": [
    "**Agent actions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ae204d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate(what):\n",
    "    return eval(what)\n",
    "\n",
    "def average_dog_weight(name):\n",
    "    if name in \"Scottish Terrier\": \n",
    "        return(\"Scottish Terriers average 20 lbs\")\n",
    "    elif name in \"Border Collie\":\n",
    "        return(\"a Border Collies average weight is 37 lbs\")\n",
    "    elif name in \"Toy Poodle\":\n",
    "        return(\"a toy poodles average weight is 7 lbs\")\n",
    "    else:\n",
    "        return(\"An average dog weights 50 lbs\")\n",
    "\n",
    "known_actions = {\n",
    "    \"calculate\": calculate,\n",
    "    \"average_dog_weight\": average_dog_weight\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71992856",
   "metadata": {},
   "source": [
    "**Manual Agent Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d09ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(prompt)\n",
    "\n",
    "agent_result = agent(\"How much does a toy poodle weigh?\")\n",
    "# link between thought and action is missing.\n",
    "tool_result = average_dog_weight(\"Toy Poodle\")\n",
    "# link between tool result and agent input is missing.\n",
    "agent_result = agent(f\"Observation: {tool_result}\")\n",
    "\n",
    "for msg in agent.messages:\n",
    "    print(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d288f2d6",
   "metadata": {},
   "source": [
    "**Agent with look example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ec9311b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "action_regex = re.compile(\"^Action: (\\w+): (.*)$\")\n",
    "\n",
    "def query(question:str, max_iterations:int=5):\n",
    "    agent = Agent(prompt)\n",
    "    next_prompt:str = question\n",
    "    for i in range(max_iterations):\n",
    "        result = agent(next_prompt)\n",
    "        print(result)\n",
    "        actions = [\n",
    "            action_regex.match(action)\n",
    "            for action in result.split('\\n')\n",
    "            if action_regex.match(action)\n",
    "        ]\n",
    "        if len(actions) == 0:\n",
    "            return\n",
    "        \n",
    "        action, action_input = actions[0].groups()\n",
    "        if action not in known_actions:\n",
    "            raise Exception(\n",
    "                f\"Unknown action: {action}: {action_input}\")\n",
    "        print(f\" -- running {action} {action_input}\")\n",
    "        observation = known_actions[action](action_input)\n",
    "        next_prompt = f\"Observation: {observation}\"\n",
    "        print(next_prompt)\n",
    "\n",
    "question = \"\"\"\\\n",
    "I have 2 dogs, a border collie and a scottish terrier. \\\n",
    "What is their combined weight\\\n",
    "\"\"\"\n",
    "query(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d60b84",
   "metadata": {},
   "source": [
    "- `^Action: (\\w+): (.*)$`\n",
    "  - `^`\n",
    "    - Start of string\n",
    "  - `Action: `\n",
    "    - Literal \"Action: \"\n",
    "  - `(\\w+)` \n",
    "    - Capture group 1\n",
    "    - One or more word characters (letters, digits, underscore)\n",
    "    - Action type\n",
    "  - `: `\n",
    "    - Literal \": \"\n",
    "  - `(.*)`\n",
    "    - Capture group 2 - match().group(1)\n",
    "    - Zero or more of any character (except newline)\n",
    "    - Action input\n",
    "  - `$`\n",
    "    - End of string\n",
    "- `action_regex.match`\n",
    "  - `.group(0)` : The entire matched string\n",
    "  - `.group(1)` : The action name (first capture group)\n",
    "  - `.group(2)` : The action parameters (second capture group)\n",
    "  - `.groups()` : `Tuple(.group(1), .group(2))`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a26c8cc8",
   "metadata": {},
   "source": [
    "# Agent in LangGraph\n",
    "\n",
    "---\n",
    "**Sequence Chart**\n",
    "```mermaid\n",
    "sequenceDiagram\n",
    "    participant User\n",
    "    participant LLM\n",
    "    participant Tools\n",
    "\n",
    "    User ->> LLM: Initial input\n",
    "    \n",
    "    Note over LLM: Prompt + LLM\n",
    "\n",
    "    loop [while tool_calls present]\n",
    "        LLM ->> Tools: Execute tools\n",
    "        Tools -->> LLM: ToolMessage for each tool_calls\n",
    "    end\n",
    "\n",
    "    LLM ->> User: Return final state\n",
    "```\n",
    "---\n",
    "**Flow Chart**\n",
    "```mermaid\n",
    "flowchart TD\n",
    "    llm --> B{exists_action}\n",
    "    B --> D([END])\n",
    "    B --> C[take_action]\n",
    "    C --> llm\n",
    "```\n",
    "---\n",
    "\n",
    "### AgentState and Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaac0091",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "tool = TavilySearchResults(max_results=4) #increased number of results\n",
    "print(type(tool))\n",
    "print(tool.name)\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system:str=\"\"):\n",
    "        self.system:str = system\n",
    "        graph:StateGraph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile()\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    \n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "    \n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results: list[ToolMessage] = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t[\"name\"] in self.tools:\n",
    "                print(\"...bad tool name...\")\n",
    "                result = \"bad tool name, retry\"\n",
    "            else:\n",
    "                result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(ToolMessage(\n",
    "                tool_call_id=t[\"id\"],\n",
    "                name=t[\"name\"],\n",
    "                content=str(result)))\n",
    "        print(\"Back to the model\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "176be19b",
   "metadata": {},
   "source": [
    "- `AgentState`\n",
    "  - accessible to all parts of the graph\n",
    "  - local to the graph\n",
    "  - can be stored in a persistence layer\n",
    "    - can resume with the state at any point in time\n",
    "  - `Annotated`\n",
    "    - `list[AnyMessage]` is the actual type of `messages`\n",
    "    - `operator.add` is used as message reducer. as a result, message will be concatenated rather than replaced on update\n",
    "      - first callable metatdata\n",
    "    - without `Annotated`, value will be replaced on update\n",
    "- `StateGraph`\n",
    "  - `add_node`: add node with name-runnable pair\n",
    "  - `add_edge`: add connection between nodes in \"from\", \"to\" order\n",
    "  - `add_conditional_edges`: `add_edge` but with condition\n",
    "- each node returns `{\"messages\", result}`, which then gets added into `AgentState[\"messages\"]`\n",
    "\n",
    "#### State graph\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/low_level/?h=stategraph#schema\n",
    "\n",
    "The first thing to do when you defining a graph is define the `State` of the graph.\n",
    "\n",
    "The `State` consists of:\n",
    "- schema of the graph\n",
    "- reducer functions which specify how to apply updates to the state\n",
    "\n",
    "The schema of the State will be the input schema to all Nodes and Edges in the graph, and can be either a `TypedDict` or a `Pydantic` model. All Nodes will emit updates to the `State` which are then applied using the specified reducer function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59edf3db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=LLM_MODEL)\n",
    "abot = Agent(model, [tool], system=prompt)\n",
    "\n",
    "# from IPython.display import Image\n",
    "# Image(abot.graph.get_graph().draw_png())\n",
    "\n",
    "messages = [HumanMessage(content=\"What is the weather in london?\")]\n",
    "result = abot.graph.invoke({\"messages\": messages})\n",
    "\n",
    "print(result)\n",
    "print(result['messages'][-1].content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1a7b5c",
   "metadata": {},
   "source": [
    "## Agentic search tool\n",
    "\n",
    "Traditional zero-shot learning model struggles with dynamic data and sourcing information (live information). Agentic search addresses this by involving a search tool.\n",
    "\n",
    "**The process involves:**\n",
    "- Receiving a query and breaking it into sub-questions if needed.\n",
    "- Selecting the best source (e.g. weather API for weather queries).\n",
    "- Extracting and chunking relevant information.\n",
    "- Running a vector search to retrieve top results.\n",
    "- Scoring and filtering the output for relevance.\n",
    "\n",
    "---\n",
    "**Search flowchart**\n",
    "```mermaid\n",
    "flowchart LR\n",
    "\n",
    "    Query[Query] --> B@{ shape: processes, label: \"Sub-Query\" }\n",
    "    B --> Retrieve[Retrieve]\n",
    "    Retrieve -->A@{ shape: processes, label: \"Source\" }\n",
    "    A --> Scoring[Scoring & Filtering]\n",
    "    Scoring --> Return[Return Top-K Docs]\n",
    "\n",
    "    classDef highlight fill:#a3e3b0,color:#000,stroke:#000\n",
    "    class B,A highlight;\n",
    "```\n",
    "\n",
    "```mermaid\n",
    "flowchart LR\n",
    "    subgraph Source\n",
    "        direction LR\n",
    "        A[Sub-Query] --> B@{ shape: docs, label: \"Chunked Source\" }\n",
    "        B --> C@{ shape: docs, label: \"Top-K Chunks\" }\n",
    "    end\n",
    "\n",
    "    classDef highlight fill:#a3e3b0,color:#000,stroke:#000\n",
    "    class B,C highlight;\n",
    "```\n",
    "---\n",
    "\n",
    "**tavily search example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be57e3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tavily import TavilyClient\n",
    "\n",
    "client = TavilyClient(api_key=os.environ.get(\"TAVILY_API_KEY\"))\n",
    "result = client.search(\"What is in Nvidia's new Blackwell GPU?\",\n",
    "                       include_answer=True)\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f343d8",
   "metadata": {},
   "source": [
    "## Persistence in LangGraph\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/persistence/\n",
    "\n",
    "LangGraph has a built-in persistence layer via **checkpointers**, which enable agents to save and restore state.\n",
    "\n",
    "**Checkpointers** snapshot the state of the graph at each \"super-step\" during execution. **Checkpoints** capture:\n",
    "- `config`: Config associated with this checkpoint.\n",
    "- `metadata`: Metadata associated with this checkpoint.\n",
    "- `values`: Values of the state channels at this point in time.\n",
    "- `next`: A tuple of the node names to execute next in the graph.\n",
    "- `tasks`: A tuple of `PregelTask` objects that contain information about next tasks to be executed.\n",
    "  - If the step was previously attempted, it will include error information. \n",
    "  - If a graph was interrupted dynamically from within a node, tasks will contain additional data associated with interrupts.\n",
    "\n",
    "Checkpoints are saved to a thread, which can be accessed after graph execution.\n",
    "- Saved checkpoints belong to a **thread**, identified by `thread_id`. \n",
    "- Threads and checkpoints unlock powerful capabilities like:\n",
    "    - **Human-in-the-loop** interactions\n",
    "    - **Time travel** (past-state recovery)\n",
    "    - **Fault-tolerance** (resuming after failure)\n",
    "    - **Memory** across runs or conversations\n",
    "- Checkpointers support various backends:\n",
    "  - **In-memory** (for simple scenarios)\n",
    "  - **SQLite** (via `SqliteSaver`)\n",
    "  - Persistent stores like **Redis** and **PostgreSQL**, including those with vector similarity search\n",
    "\n",
    "Core functionality of checkpoints are:\n",
    "- Get state\n",
    "  - `get_state`\n",
    "  - get a state snapshot for a specific `checkpoint_id`\n",
    "- Get state history\n",
    "  - `get_state_history`\n",
    "  - get the full history of the graph execution for a given thread\n",
    "- Replay\n",
    "  - `invoke`\n",
    "  - re-play the previously executed steps before a checkpoint that corresponds to the `checkpoint_id`, and only execute the steps after the checkpoint\n",
    "- Update state\n",
    "  - `update_state`\n",
    "\n",
    "**Available libraries:**\n",
    "- `langgraph-checkpoint`\n",
    "- `langgraph-checkpoint-sqlite`\n",
    "- `langgraph-checkpoint-postgres`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4e94cd",
   "metadata": {},
   "source": [
    "\n",
    "## Streaming in LangGraph\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/streaming/\n",
    "\n",
    "LangGraph also offers a streaming system to provide real-time feedbacks.\n",
    "`stream` replaces `invoke`.\n",
    "\n",
    "**Categories:**\n",
    "- **Workflow progress:** get state updates after each graph node is executed\n",
    "- **LLM tokens:** stream language model tokens as they’re generated\n",
    "- **Custom updates:** emit user-defined signals\n",
    "\n",
    "**Supported stream modes**\n",
    "\n",
    "Data can be streamed during graph excution using `stream` or `astream`\n",
    "\n",
    "| Mode | Description |\n",
    "| ---- | ----------- |\n",
    "|values|Streams the full value of the state after each step of the graph.\n",
    "|updates|Streams the updates to the state after each step of the graph. If multiple updates are made in the same step (e.g., multiple nodes are run), those updates are streamed separately.\n",
    "|custom|Streams custom data from inside your graph nodes.\n",
    "|messages|Streams 2-tuples (LLM token, metadata) from any graph nodes where an LLM is invoked.\n",
    "debug|Streams as much information as possible throughout the execution of the graph.\n",
    "\n",
    "## Overview of Persistence and Streaming\n",
    "\n",
    "| Concept | Purpose | Benefits |\n",
    "| ------- | ------- | -------- |\n",
    "| **Persistence** | Saves and restores agent state across execution steps using checkpointers | Enables resilience, time travel, memory retention |\n",
    "| **Streaming**   | Streams state changes, tokens, messages, and custom updates in real time  | Improves transparency, interactivity, and debugging capability |\n",
    "\n",
    "**package**\n",
    "\n",
    "```bash\n",
    "uv add langgraph-checkpoint-sqlite\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c737b985",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], operator.add]\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, checkpointer, system:str=\"\"):\n",
    "        self.system:str = system\n",
    "        graph:StateGraph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\n",
    "            \"llm\",\n",
    "            self.exists_action,\n",
    "            {True: \"action\", False: END}\n",
    "        )\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(checkpointer=checkpointer)\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        result = state[\"messages\"][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "    \n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state[\"messages\"]\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "    \n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state[\"messages\"][-1].tool_calls\n",
    "        results: list[ToolMessage] = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            if not t[\"name\"] in self.tools:\n",
    "                print(\"...bad tool name...\")\n",
    "                result = \"bad tool name, retry\"\n",
    "            else:\n",
    "                result = self.tools[t[\"name\"]].invoke(t[\"args\"])\n",
    "            results.append(ToolMessage(\n",
    "                tool_call_id=t[\"id\"],\n",
    "                name=t[\"name\"],\n",
    "                content=str(result)))\n",
    "        print(\"Back to the model\")\n",
    "        return {\"messages\": results}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea94b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "tool = TavilySearchResults(max_results=4)\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    model = ChatOpenAI(model=LLM_MODEL)\n",
    "    agent = Agent(model, [tool], system=prompt, checkpointer=checkpointer)\n",
    "\n",
    "    print(\"First iteration ============\")\n",
    "    messages = [HumanMessage(content=\"What is the weather in sf?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v['messages'])\n",
    "\n",
    "    print(\"Second iteration ============\")\n",
    "    messages = [HumanMessage(content=\"What about in la?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "        for v in event.values():\n",
    "            print(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3fba6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langgraph.checkpoint.sqlite.aio import AsyncSqliteSaver\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "async with AsyncSqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    tool = TavilySearchResults(max_results=4)\n",
    "    model = ChatOpenAI(model=LLM_MODEL)\n",
    "    agent = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "    messages = [HumanMessage(content=\"What is the weather in SF?\")]\n",
    "    thread = {\"configurable\": {\"thread_id\": \"4\"}}\n",
    "    async for event in agent.graph.astream_events({\"messages\": messages}, thread, version=\"v1\"):\n",
    "        kind = event[\"event\"]\n",
    "        if kind == \"on_chat_model_stream\":\n",
    "            content = event[\"data\"][\"chunk\"].content\n",
    "            if content:\n",
    "                # Empty content in the context of OpenAI means\n",
    "                # that the model is asking for a tool to be invoked.\n",
    "                # So we only print non-empty content\n",
    "                print(content, end=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42f0d0e",
   "metadata": {},
   "source": [
    "- `AsyncSqliteSaver`\n",
    "  - Package has been changed from `aiosqlite` to `sqlite.aio`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce3beae",
   "metadata": {},
   "source": [
    "## Human in the Loop\n",
    "\n",
    "> https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#key-capabilities\n",
    "\n",
    "In an agent workflow, with the **human-in-the-loop**, user can:\n",
    "- review\n",
    "- edit\n",
    "- approve tool call\n",
    "\n",
    "Additional state information is stored to memory and displayed when using `get_state` or `get_state_history`.\n",
    "- `State` is additionally stored every state transition while previously it was stored at an interrupt or at the end.\n",
    "- These change the command output slightly, but are a useful addtion to the information available.\n",
    "\n",
    "**Key Capability**\n",
    "- Persisent execution state\n",
    "  - Interrupts use LangGraph's persistence layer, which saves the graph state\n",
    "  - `interrupt` (dynamic) and `interrupt_before`/`interrupt_after` (static) can be used to pause a graph\n",
    "- Flexible integration point\n",
    "  - intturpt can be done at any point in a workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702ba423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "from typing import TypedDict, Annotated\n",
    "from langchain_core.messages import AnyMessage\n",
    "\n",
    "\"\"\"\n",
    "In previous examples we've annotated the `messages` state key\n",
    "with the default `operator.add` or `+` reducer, which always\n",
    "appends new messages to the end of the existing messages array.\n",
    "\n",
    "Now, to support replacing existing messages, we annotate the\n",
    "`messages` key with a customer reducer function, which replaces\n",
    "messages with the same `id`, and appends them otherwise.\n",
    "\"\"\"\n",
    "def reduce_messages(left: list[AnyMessage], right: list[AnyMessage]) -> list[AnyMessage]:\n",
    "    # assign ids to messages that don't have them\n",
    "    for message in right:\n",
    "        if not message.id:\n",
    "            message.id = str(uuid4())\n",
    "    # merge the new messages with the existing messages\n",
    "    merged = left.copy()\n",
    "    for message in right:\n",
    "        for i, existing in enumerate(merged):\n",
    "            # replace any existing messages with the same id\n",
    "            if existing.id == message.id:\n",
    "                merged[i] = message\n",
    "                break\n",
    "        else:\n",
    "            # append any new messages to the end\n",
    "            merged.append(message)\n",
    "    return merged\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    messages: Annotated[list[AnyMessage], reduce_messages]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0232b360",
   "metadata": {},
   "source": [
    "- `left`: existing snappshot state that human-in-the-loop is about to added in\n",
    "- `right`: new message to merge into the `left`\n",
    "  - if same id message exists, it'll replace the message with the same id\n",
    "  - if same id message does not exist in `left`, it'll get appended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d50b162",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from langchain_core.messages import SystemMessage, ToolMessage\n",
    "\n",
    "class Agent:\n",
    "    def __init__(self, model, tools, system=\"\", checkpointer=None):\n",
    "        self.system = system\n",
    "        graph = StateGraph(AgentState)\n",
    "        graph.add_node(\"llm\", self.call_openai)\n",
    "        graph.add_node(\"action\", self.take_action)\n",
    "        graph.add_conditional_edges(\"llm\", self.exists_action, {True: \"action\", False: END})\n",
    "        graph.add_edge(\"action\", \"llm\")\n",
    "        graph.set_entry_point(\"llm\")\n",
    "        self.graph = graph.compile(\n",
    "            checkpointer=checkpointer,\n",
    "            interrupt_before=[\"action\"]\n",
    "        )\n",
    "        self.tools = {t.name: t for t in tools}\n",
    "        self.model = model.bind_tools(tools)\n",
    "\n",
    "    def call_openai(self, state: AgentState):\n",
    "        messages = state['messages']\n",
    "        if self.system:\n",
    "            messages = [SystemMessage(content=self.system)] + messages\n",
    "        message = self.model.invoke(messages)\n",
    "        return {'messages': [message]}\n",
    "\n",
    "    def exists_action(self, state: AgentState):\n",
    "        print(state)\n",
    "        result = state['messages'][-1]\n",
    "        return len(result.tool_calls) > 0\n",
    "\n",
    "    def take_action(self, state: AgentState):\n",
    "        tool_calls = state['messages'][-1].tool_calls\n",
    "        results = []\n",
    "        for t in tool_calls:\n",
    "            print(f\"Calling: {t}\")\n",
    "            result = self.tools[t['name']].invoke(t['args'])\n",
    "            results.append(ToolMessage(tool_call_id=t['id'], name=t['name'], content=str(result)))\n",
    "        print(\"Back to the model!\")\n",
    "        return {'messages': results}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98723dbf",
   "metadata": {},
   "source": [
    "- `__init__`\n",
    "  - `interrupt_before` has been added when `graph.compile`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5868ce74",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langgraph.types import StateSnapshot\n",
    "\n",
    "tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "prompt = \"\"\"You are a smart research assistant. Use the search engine to look up information. \\\n",
    "You are allowed to make multiple calls (either together or in sequence). \\\n",
    "Only look up information when you are sure of what you want. \\\n",
    "If you need to look up some information before asking a follow up question, you are allowed to do that!\n",
    "\"\"\"\n",
    "\n",
    "model = ChatOpenAI(model=LLM_MODEL)\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    agent = Agent(model, [tool], system=prompt, checkpointer=memory)\n",
    "\n",
    "    # 1. Normal stream\n",
    "    if False:\n",
    "        messages = [HumanMessage(content=\"Whats the weather in SF?\")]\n",
    "        thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "        for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "        print(\"get state\")\n",
    "        print(agent.graph.get_state(thread))\n",
    "        print(agent.graph.get_state(thread).next)\n",
    "\n",
    "    # 2. Approve tool call\n",
    "    if False:\n",
    "        messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "        thread = {\"configurable\": {\"thread_id\": \"2\"}}\n",
    "        for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "\n",
    "        while agent.graph.get_state(thread).next:\n",
    "            print(\"\\n\", agent.graph.get_state(thread),\"\\n\")\n",
    "            print(\"about to input\")\n",
    "            # This will cause hang in vscode jupyter notebook\n",
    "            _input = input(\"proceed?\")\n",
    "            if _input != \"y\":\n",
    "                print(\"aborting\")\n",
    "                break\n",
    "            for event in agent.graph.stream(None, thread):\n",
    "                for v in event.values():\n",
    "                    print(v)\n",
    "    \n",
    "    # 3. Modify State\n",
    "    if True:\n",
    "        messages = [HumanMessage(\"Whats the weather in LA?\")]\n",
    "        thread = {\"configurable\": {\"thread_id\": \"3\"}}\n",
    "        for event in agent.graph.stream({\"messages\": messages}, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "        print(\"State before:\", agent.graph.get_state(thread))\n",
    "        current_values = agent.graph.get_state(thread)\n",
    "        print(current_values.values['messages'][-1])\n",
    "        print(current_values.values['messages'][-1].tool_calls)\n",
    "        _id = current_values.values['messages'][-1].tool_calls[0]['id']\n",
    "        current_values.values['messages'][-1].tool_calls = [{\n",
    "            'name': 'tavily_search_results_json',\n",
    "            'args': {'query': 'current weather in Louisiana'},\n",
    "            'id': _id\n",
    "        }]\n",
    "        agent.graph.update_state(thread, current_values.values)\n",
    "        print(\"State after:\", agent.graph.get_state(thread))\n",
    "        for event in agent.graph.stream(None, thread):\n",
    "            for v in event.values():\n",
    "                print(v)\n",
    "\n",
    "    # Print all current states\n",
    "    states: list[StateSnapshot] = []\n",
    "    for state in agent.graph.get_state_history(thread):\n",
    "        print(state)\n",
    "        print('--')\n",
    "        states.append(state)\n",
    "\n",
    "    # 4. Time Travel\n",
    "    if False:\n",
    "        to_replay = states[-3]\n",
    "        print(\"replay\", to_replay)\n",
    "        for event in agent.graph.stream(None, to_replay.config):\n",
    "            for k, v in event.items():\n",
    "                print(v)\n",
    "\n",
    "    if False:\n",
    "        to_replay = states[-3]\n",
    "        print(\"replay\", to_replay)\n",
    "        _id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
    "        to_replay.values['messages'][-1].tool_calls = [{\n",
    "            'name': 'tavily_search_results_json',\n",
    "            'args': {'query': 'current weather in LA, accuweather'},\n",
    "            'id': _id\n",
    "        }]\n",
    "        branch_state = agent.graph.update_state(to_replay.config, to_replay.values)\n",
    "        for event in agent.graph.stream(None, branch_state):\n",
    "            for k, v in event.items():\n",
    "                if k != \"__end__\":\n",
    "                    print(v)\n",
    "\n",
    "    # 5. Add message to a state at a given time\n",
    "    to_replay = states[-3]\n",
    "    print(\"replay\", to_replay)\n",
    "    _id = to_replay.values['messages'][-1].tool_calls[0]['id']\n",
    "    state_update = {\"messages\": [ToolMessage(\n",
    "        tool_call_id=_id,\n",
    "        name=\"tavily_search_results_json\",\n",
    "        content=\"54 degree celcius\",\n",
    "    )]}\n",
    "    branch_and_add = agent.graph.update_state(\n",
    "        to_replay.config, \n",
    "        state_update, \n",
    "        as_node=\"action\"\n",
    "    )\n",
    "    for event in agent.graph.stream(None, branch_and_add):\n",
    "        for k, v in event.items():\n",
    "            print(v)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e7ef138",
   "metadata": {},
   "source": [
    "- each returned state `StateSnapshot` contains full history until the snapshot point\n",
    "- `get_state`\n",
    "  - Get the current state of the graph\n",
    "- `get_state_history`\n",
    "  - Get the history of the state of the graph\n",
    "  \n",
    "**Small Graph Example**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba9e79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    lnode: str\n",
    "    scratch: str\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "def node1(state: AgentState):\n",
    "    print(f\"node1, count:{state['count']}\")\n",
    "    return {\n",
    "        \"lnode\": \"node_1\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "\n",
    "def node2(state: AgentState):\n",
    "    print(f\"node2, count:{state['count']}\")\n",
    "    return {\n",
    "        \"lnode\": \"node_2\",\n",
    "        \"count\": 1,\n",
    "    }\n",
    "\n",
    "def should_continue(state):\n",
    "    return state[\"count\"] < 3\n",
    "\n",
    "# Build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"Node1\", node1)\n",
    "builder.add_node(\"Node2\", node2)\n",
    "\n",
    "builder.add_edge(\"Node1\", \"Node2\")\n",
    "builder.add_conditional_edges(\"Node2\", \n",
    "                              should_continue, \n",
    "                              {True: \"Node1\", False: END})\n",
    "builder.set_entry_point(\"Node1\")\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "\n",
    "    # Run\n",
    "    thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "    graph.invoke({\"count\":0, \"scratch\":\"hi\"},thread)\n",
    "\n",
    "    # State\n",
    "    print(graph.get_state(thread))\n",
    "\n",
    "    # State history\n",
    "    for state in graph.get_state_history(thread):\n",
    "        print(state, \"\\n\")\n",
    "\n",
    "    # Config history\n",
    "    states = []\n",
    "    for state in graph.get_state_history(thread):\n",
    "        states.append(state.config)\n",
    "        print(state.config, state.values['count'])\n",
    "\n",
    "    print(states[-3])\n",
    "    print(graph.get_state(states[-3]))\n",
    "\n",
    "    # Go back in time\n",
    "    graph.invoke(None, states[-3])\n",
    "    thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "    for state in graph.get_state_history(thread):\n",
    "        print(state.config, state.values['count'])\n",
    "\n",
    "    thread = {\"configurable\": {\"thread_id\": str(1)}}\n",
    "    for state in graph.get_state_history(thread):\n",
    "        print(state,\"\\n\")\n",
    "\n",
    "    # Modify state\n",
    "    thread2 = {\"configurable\": {\"thread_id\": str(2)}}\n",
    "    graph.invoke({\"count\":0, \"scratch\":\"hi\"},thread2)\n",
    "\n",
    "    states2 = []\n",
    "    for state in graph.get_state_history(thread2):\n",
    "        states2.append(state.config)\n",
    "        print(state.config, state.values['count'])   \n",
    "\n",
    "    save_state = graph.get_state(states2[-3])\n",
    "    print(save_state)\n",
    "\n",
    "    save_state.values[\"count\"] = -3\n",
    "    save_state.values[\"scratch\"] = \"hello\"\n",
    "    print(save_state)\n",
    "\n",
    "    graph.update_state(thread2,save_state.values)\n",
    "\n",
    "    for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "        if i >= 3:  #print latest 3\n",
    "            break\n",
    "        print(state, '\\n')\n",
    "\n",
    "    # Modify state as node\n",
    "    graph.update_state(thread2,save_state.values, as_node=\"Node1\")\n",
    "    for i, state in enumerate(graph.get_state_history(thread2)):\n",
    "        if i >= 3:  #print latest 3\n",
    "            break\n",
    "        print(state, '\\n')\n",
    "\n",
    "    graph.invoke(None,thread2)\n",
    "    for state in graph.get_state_history(thread2):\n",
    "        print(state,\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4b8c",
   "metadata": {},
   "source": [
    "## Essay writer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17f981a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List\n",
    "import operator\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, AIMessage, ChatMessage\n",
    "\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "\n",
    "from langchain_openai import ChatOpenAI\n",
    "model = ChatOpenAI(model=LLM_MODEL, temperature=0)\n",
    "\n",
    "PLAN_PROMPT = \"\"\"You are an expert writer tasked with writing a high level outline of an essay. \\\n",
    "Write such an outline for the user provided topic. Give an outline of the essay along with any relevant notes \\\n",
    "or instructions for the sections.\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"You are an essay assistant tasked with writing excellent 5-paragraph essays.\\\n",
    "Generate the best essay possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of your previous attempts. \\\n",
    "Utilize all the information below as needed: \n",
    "\n",
    "------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"You are a teacher grading an essay submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when writing the following essay. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max.\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as outlined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. Only generate 3 queries max.\"\"\"\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "from tavily import TavilyClient\n",
    "import os\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT), \n",
    "        HumanMessage(content=state['task'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"plan\": response.content}\n",
    "\n",
    "def research_plan_node(state: AgentState):\n",
    "    print(state)\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "        HumanMessage(content=state['task'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}\n",
    "\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join(state['content'] or [])\n",
    "    user_message = HumanMessage(\n",
    "        content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(\n",
    "            content=WRITER_PROMPT.format(content=content)\n",
    "        ),\n",
    "        user_message\n",
    "        ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\n",
    "        \"draft\": response.content, \n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1\n",
    "    }\n",
    "\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT), \n",
    "        HumanMessage(content=state['draft'])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    return {\"critique\": response.content}\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    queries = model.with_structured_output(Queries).invoke([\n",
    "        SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "        HumanMessage(content=state['critique'])\n",
    "    ])\n",
    "    content = state['content'] or []\n",
    "    for q in queries.queries:\n",
    "        response = tavily.search(query=q, max_results=2)\n",
    "        for r in response['results']:\n",
    "            content.append(r['content'])\n",
    "    return {\"content\": content}\n",
    "\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "builder = StateGraph(AgentState)\n",
    "\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as memory:\n",
    "    graph = builder.compile(checkpointer=memory)\n",
    "    # from IPython.display import Image\n",
    "    # Image(graph.get_graph().draw_png())\n",
    "\n",
    "    thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "    for s in graph.stream({\n",
    "        \"task\": \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)\n",
    "\n",
    "    ## Essay Writer Interface\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    from helper import ewriter, writer_gui\n",
    "\n",
    "    MultiAgent = ewriter()\n",
    "    app = writer_gui(MultiAgent.graph)\n",
    "    app.launch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b1d24a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
